{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-1. Áí∞Â¢ÉË®≠ÂÆö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LANGSMITH_TRACING_V2: None\n",
      "LANGSMITH_ENDPOINT: https://api.smith.langchain.com\n",
      "LANGSMITH_PROJECT: agent-book\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# .env „Éï„Ç°„Ç§„É´„ÇíË™≠„ÅøËæº„ÇÄ\n",
    "path_env=\"C:\\\\Users\\\\ktgu1\\\\.env\"\n",
    "load_dotenv(path_env)\n",
    "\n",
    "# Áí∞Â¢ÉÂ§âÊï∞„ÇíÂèñÂæó\n",
    "langsmith_tracing_v2 = os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
    "langsmith_endpoint = os.getenv(\"LANGSMITH_ENDPOINT\")\n",
    "langsmith_api_key = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "langsmith_project = os.getenv(\"LANGSMITH_PROJECT\")\n",
    "\n",
    "# Langsmith„Åå„Åü„Å†„Åó„ÅèË™≠„ÅøËæº„ÇÅ„Å¶„ÅÑ„Çã„ÅãÁ¢∫Ë™ç„ÄÇ„Å™„Åú„Åãtracing„ÅåNone„Å†„Åå„Åß„Åç„Å¶„Çã\n",
    "print(f\"LANGSMITH_TRACING_V2: {langsmith_tracing_v2}\")\n",
    "print(f\"LANGSMITH_ENDPOINT: {langsmith_endpoint}\")\n",
    "print(f\"LANGSMITH_PROJECT: {langsmith_project}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install langchain-core==0.3.0 langchain-openai==0.2.0 \\ \n",
    "#%langchain-community==0.3.0 GitPython==3.1.43 \\ \n",
    "#%langchain-chroma==0.1.4 tavily-python==0.5.0 pydantic==2.10.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-2. RAGÂÆüË°å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "417\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import GitLoader\n",
    "\n",
    "\n",
    "def file_filter(file_path: str) -> bool:\n",
    "    return file_path.endswith(\".mdx\")\n",
    "\n",
    "\n",
    "loader = GitLoader(\n",
    "    clone_url=\"https://github.com/langchain-ai/langchain\",\n",
    "    repo_path=\"./langchain\",\n",
    "    branch=\"master\",\n",
    "    file_filter=file_filter,\n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "db = Chroma.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain„ÅØ„ÄÅÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÔºàLLMÔºâ„ÇíÊ¥ªÁî®„Åó„Åü„Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥„ÇíÈñãÁô∫„Åô„Çã„Åü„ÇÅ„ÅÆ„Éï„É¨„Éº„É†„ÉØ„Éº„ÇØ„Åß„Åô„ÄÇ„Åì„ÅÆ„Éï„É¨„Éº„É†„ÉØ„Éº„ÇØ„ÅØ„ÄÅLLM„Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥„ÅÆ„É©„Ç§„Éï„Çµ„Ç§„ÇØ„É´„ÅÆÂêÑÊÆµÈöé„ÇíÁ∞°Á¥†Âåñ„Åó„Åæ„Åô„ÄÇÂÖ∑‰ΩìÁöÑ„Å´„ÅØ„ÄÅ‰ª•‰∏ã„ÅÆ„Çà„ÅÜ„Å™Ê©üËÉΩ„ÇíÊèê‰æõ„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ\\n\\n1. **ÈñãÁô∫**: LangChain„ÅÆ„Ç™„Éº„Éó„É≥„ÇΩ„Éº„Çπ„Ç≥„É≥„Éù„Éº„Éç„É≥„Éà„ÇÑ„Çµ„Éº„Éâ„Éë„Éº„ÉÜ„Ç£„ÅÆÁµ±Âêà„Çí‰ΩøÁî®„Åó„Å¶„Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥„ÇíÊßãÁØâ„Åß„Åç„Åæ„Åô„ÄÇLangGraph„ÇíÂà©Áî®„Åô„Çã„Åì„Å®„Åß„ÄÅÁä∂ÊÖã„ÇíÊåÅ„Å§„Ç®„Éº„Ç∏„Çß„É≥„Éà„ÇíÊßãÁØâ„Åó„ÄÅ„Çπ„Éà„É™„Éº„Éü„É≥„Ç∞„ÇÑ‰∫∫Èñì„ÅÆ‰ªãÂÖ•„Çí„Çµ„Éù„Éº„Éà„Åó„Åæ„Åô„ÄÇ\\n\\n2. **ÁîüÁî£Âåñ**: LangSmith„Çí‰ΩøÁî®„Åó„Å¶„Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥„ÇíÊ§úÊüª„ÄÅÁõ£Ë¶ñ„ÄÅË©ï‰æ°„Åó„ÄÅÁ∂ôÁ∂öÁöÑ„Å´ÊúÄÈÅ©Âåñ„Åó„Å¶Ëá™‰ø°„ÇíÊåÅ„Å£„Å¶„Éá„Éó„É≠„Ç§„Åß„Åç„Åæ„Åô„ÄÇ\\n\\n3. **„Éá„Éó„É≠„Ç§**: LangGraph„Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥„ÇíÁîüÁî£Ê∫ñÂÇô„ÅåÊï¥„Å£„ÅüAPI„ÇÑ„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„Å´Â§âÊèõ„Åß„Åç„Åæ„Åô„ÄÇ\\n\\nLangChain„ÅØ„ÄÅLLM„ÇÑÈñ¢ÈÄ£ÊäÄË°ìÔºàÂüã„ÇÅËæº„Åø„É¢„Éá„É´„ÇÑ„Éô„ÇØ„Éà„É´„Çπ„Éà„Ç¢„Å™„Å©Ôºâ„Å´ÂØæ„Åô„ÇãÊ®ôÊ∫ñ„Ç§„É≥„Çø„Éº„Éï„Çß„Éº„Çπ„ÇíÂÆüË£Ö„Åó„ÄÅÊï∞Áôæ„ÅÆ„Éó„É≠„Éê„Ç§„ÉÄ„Éº„Å®Áµ±Âêà„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Åæ„Åü„ÄÅË§áÊï∞„ÅÆ„Ç™„Éº„Éó„É≥„ÇΩ„Éº„Çπ„É©„Ç§„Éñ„É©„É™„ÅßÊßãÊàê„Åï„Çå„Å¶„Åä„Çä„ÄÅ„É¶„Éº„Ç∂„Éº„ÅØÂøÖË¶Å„Å´Âøú„Åò„Å¶„Ç≥„É≥„Éù„Éº„Éç„É≥„Éà„ÇíÈÅ∏Êäû„Åó„Å¶‰ΩøÁî®„Åß„Åç„Åæ„Åô„ÄÇ'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template('''\\\n",
    "‰ª•‰∏ã„ÅÆÊñáËÑà„Å†„Åë„ÇíË∏è„Åæ„Åà„Å¶Ë≥™Âïè„Å´ÂõûÁ≠î„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n",
    "\n",
    "ÊñáËÑà: \"\"\"\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "Ë≥™Âïè: {question}\n",
    "''')\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "chain = {\n",
    "    \"question\": RunnablePassthrough(),\n",
    "    \"context\": retriever,\n",
    "} | prompt | model | StrOutputParser()\n",
    "\n",
    "chain.invoke(\"LangChain„ÅÆÊ¶ÇË¶Å„ÇíÊïô„Åà„Å¶\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-3. Ê§úÁ¥¢„ÇØ„Ç®„É™„ÅÆÂ∑•Â§´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothetical_prompt = ChatPromptTemplate.from_template(\"\"\"\\\n",
    "Ê¨°„ÅÆË≥™Âïè„Å´ÂõûÁ≠î„Åô„Çã‰∏ÄÊñá„ÇíÊõ∏„ÅÑ„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n",
    "\n",
    "Ë≥™Âïè: {question}\n",
    "\"\"\")\n",
    "\n",
    "hypothetical_chain = hypothetical_prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain„ÅØ„ÄÅÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÔºàLLMÔºâ„ÇíÊ¥ªÁî®„Åó„Åü„Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥„ÇíÈñãÁô∫„Åô„Çã„Åü„ÇÅ„ÅÆ„Éï„É¨„Éº„É†„ÉØ„Éº„ÇØ„Åß„Åô„ÄÇLangChain„ÅØ„ÄÅÈñãÁô∫„ÄÅÈÅãÁî®„ÄÅ„Éá„Éó„É≠„Ç§„ÅÆÂêÑÊÆµÈöé„ÇíÁ∞°Á¥†Âåñ„Åô„Çã„Åì„Å®„ÇíÁõÆÁöÑ„Å®„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇÂÖ∑‰ΩìÁöÑ„Å´„ÅØ„ÄÅ‰ª•‰∏ã„ÅÆ„Çà„ÅÜ„Å™ÁâπÂæ¥„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ\\n\\n1. **ÈñãÁô∫**: LangChain„ÅÆ„Ç™„Éº„Éó„É≥„ÇΩ„Éº„Çπ„Ç≥„É≥„Éù„Éº„Éç„É≥„Éà„ÇÑ„Çµ„Éº„Éâ„Éë„Éº„ÉÜ„Ç£„ÅÆÁµ±Âêà„Çí‰ΩøÁî®„Åó„Å¶„Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥„ÇíÊßãÁØâ„Åß„Åç„Åæ„Åô„ÄÇ„Åæ„Åü„ÄÅLangGraph„ÇíÂà©Áî®„Åó„Å¶„ÄÅÁä∂ÊÖã„ÇíÊåÅ„Å§„Ç®„Éº„Ç∏„Çß„É≥„Éà„ÇíÊßãÁØâ„Åô„Çã„Åì„Å®„Åå„Åß„Åç„Åæ„Åô„ÄÇ\\n\\n2. **ÈÅãÁî®**: LangSmith„Çí‰ΩøÁî®„Åó„Å¶„Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥„ÇíÁõ£Ë¶ñ„ÄÅË©ï‰æ°„Åó„ÄÅÁ∂ôÁ∂öÁöÑ„Å´ÊúÄÈÅ©Âåñ„Åó„Å¶Ëá™‰ø°„ÇíÊåÅ„Å£„Å¶„Éá„Éó„É≠„Ç§„Åß„Åç„Åæ„Åô„ÄÇ\\n\\n3. **„Éá„Éó„É≠„Ç§**: LangGraph„Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥„ÇíÊú¨Áï™Áí∞Â¢ÉÂêë„Åë„ÅÆAPI„ÇÑ„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„Å´Â§âÊèõ„Åô„Çã„Åì„Å®„Åå„Åß„Åç„Åæ„Åô„ÄÇ\\n\\nLangChain„ÅØ„ÄÅ„Åï„Åæ„Åñ„Åæ„Å™„Éó„É≠„Éê„Ç§„ÉÄ„Éº„Å®Áµ±Âêà„Åó„ÄÅÊ®ôÊ∫ñ„Ç§„É≥„Çø„Éº„Éï„Çß„Éº„Çπ„ÇíÊèê‰æõ„Åô„Çã„Åì„Å®„Åß„ÄÅÈñãÁô∫ËÄÖ„ÅåÁï∞„Å™„Çã„Ç≥„É≥„Éù„Éº„Éç„É≥„Éà„ÇíÁ∞°Âçò„Å´Âàá„ÇäÊõø„Åà„Åü„Çä„ÄÅÁµÑ„ÅøÂêà„Çè„Åõ„Åü„Çä„Åß„Åç„Çã„Çà„ÅÜ„Å´„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Åæ„Åü„ÄÅË§áÈõë„Å™„Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥„ÅÆ„Ç™„Éº„Ç±„Çπ„Éà„É¨„Éº„Ç∑„Éß„É≥„Çí„Çµ„Éù„Éº„Éà„Åô„Çã„Åü„ÇÅ„ÅÆLangGraph„ÇÑ„ÄÅ„Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥„ÅÆÂèØË¶ñÂåñ„Å®Ë©ï‰æ°„ÇíË°å„ÅÜLangSmith„Å®„ÅÑ„Å£„Åü„ÉÑ„Éº„É´„ÇÇÊèê‰æõ„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyde_rag_chain = {\n",
    "    \"question\": RunnablePassthrough(),\n",
    "    \"context\": hypothetical_chain | retriever,\n",
    "} | prompt | model | StrOutputParser()\n",
    "\n",
    "hyde_rag_chain.invoke(\"LangChain„ÅÆÊ¶ÇË¶Å„ÇíÊïô„Åà„Å¶\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ë§áÊï∞„ÅÆÊ§úÁ¥¢„ÇØ„Ç®„É™„ÅÆÁîüÊàê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class QueryGenerationOutput(BaseModel):\n",
    "    queries: list[str] = Field(..., description=\"Ê§úÁ¥¢„ÇØ„Ç®„É™„ÅÆ„É™„Çπ„Éà\")\n",
    "\n",
    "\n",
    "query_generation_prompt = ChatPromptTemplate.from_template(\"\"\"\\\n",
    "Ë≥™Âïè„Å´ÂØæ„Åó„Å¶„Éô„ÇØ„Çø„Éº„Éá„Éº„Çø„Éô„Éº„Çπ„Åã„ÇâÈñ¢ÈÄ£ÊñáÊõ∏„ÇíÊ§úÁ¥¢„Åô„Çã„Åü„ÇÅ„Å´„ÄÅ\n",
    "3„Å§„ÅÆÁï∞„Å™„ÇãÊ§úÁ¥¢„ÇØ„Ç®„É™„ÇíÁîüÊàê„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n",
    "Ë∑ùÈõ¢„Éô„Éº„Çπ„ÅÆÈ°û‰ººÊÄßÊ§úÁ¥¢„ÅÆÈôêÁïå„ÇíÂÖãÊúç„Åô„Çã„Åü„ÇÅ„Å´„ÄÅ\n",
    "„É¶„Éº„Ç∂„Éº„ÅÆË≥™Âïè„Å´ÂØæ„Åó„Å¶Ë§áÊï∞„ÅÆË¶ñÁÇπ„ÇíÊèê‰æõ„Åô„Çã„Åì„Å®„ÅåÁõÆÊ®ô„Åß„Åô„ÄÇ\n",
    "\n",
    "Ë≥™Âïè: {question}\n",
    "\"\"\")\n",
    "\n",
    "query_generation_chain = (\n",
    "    query_generation_prompt\n",
    "    | model.with_structured_output(QueryGenerationOutput)\n",
    "    | (lambda x: x.queries)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain„ÅØ„ÄÅÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÔºàLLMÔºâ„ÇíÊ¥ªÁî®„Åó„Åü„Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥„ÇíÈñãÁô∫„Åô„Çã„Åü„ÇÅ„ÅÆ„Éï„É¨„Éº„É†„ÉØ„Éº„ÇØ„Åß„Åô„ÄÇLangChain„ÅØ„ÄÅÈñãÁô∫„ÄÅÈÅãÁî®„ÄÅ„Éá„Éó„É≠„Ç§„ÅÆÂêÑÊÆµÈöé„ÇíÁ∞°Á¥†Âåñ„Åô„Çã„Åì„Å®„ÇíÁõÆÁöÑ„Å®„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇÂÖ∑‰ΩìÁöÑ„Å´„ÅØ„ÄÅ‰ª•‰∏ã„ÅÆ„Çà„ÅÜ„Å™Ê©üËÉΩ„ÇíÊèê‰æõ„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ\\n\\n1. **ÈñãÁô∫**: LangChain„ÅÆ„Ç™„Éº„Éó„É≥„ÇΩ„Éº„Çπ„Ç≥„É≥„Éù„Éº„Éç„É≥„Éà„ÇÑ„Çµ„Éº„Éâ„Éë„Éº„ÉÜ„Ç£„ÅÆÁµ±Âêà„Çí‰ΩøÁî®„Åó„Å¶„Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥„ÇíÊßãÁØâ„Åß„Åç„Åæ„Åô„ÄÇ„Åæ„Åü„ÄÅLangGraph„ÇíÂà©Áî®„Åó„Å¶„ÄÅÁä∂ÊÖã„ÇíÊåÅ„Å§„Ç®„Éº„Ç∏„Çß„É≥„Éà„ÇíÊßãÁØâ„Åó„ÄÅ„Çπ„Éà„É™„Éº„Éü„É≥„Ç∞„ÇÑ‰∫∫Èñì„ÅÆ‰ªãÂÖ•„Çí„Çµ„Éù„Éº„Éà„Åó„Åæ„Åô„ÄÇ\\n\\n2. **ÈÅãÁî®**: LangSmith„Çí‰ΩøÁî®„Åó„Å¶„Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥„ÇíÊ§úÊüª„ÄÅÁõ£Ë¶ñ„ÄÅË©ï‰æ°„Åó„ÄÅÁ∂ôÁ∂öÁöÑ„Å´ÊúÄÈÅ©Âåñ„Åó„Å¶Ëá™‰ø°„ÇíÊåÅ„Å£„Å¶„Éá„Éó„É≠„Ç§„Åß„Åç„Åæ„Åô„ÄÇ\\n\\n3. **„Éá„Éó„É≠„Ç§**: LangGraph„Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥„ÇíÁîüÁî£Ê∫ñÂÇô„ÅåÊï¥„Å£„ÅüAPI„ÇÑ„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„Å´Â§âÊèõ„Åß„Åç„Åæ„Åô„ÄÇ\\n\\nLangChain„ÅØ„ÄÅ„Åï„Åæ„Åñ„Åæ„Å™„É¢„Éá„É´„ÇÑÈñ¢ÈÄ£„Ç≥„É≥„Éù„Éº„Éç„É≥„Éà„Å´ÂØæ„Åó„Å¶Ê®ôÊ∫ñÂåñ„Åï„Çå„Åü„Ç§„É≥„Çø„Éº„Éï„Çß„Éº„Çπ„ÇíÊèê‰æõ„Åó„ÄÅÈñãÁô∫ËÄÖ„ÅåÁï∞„Å™„Çã„Éó„É≠„Éê„Ç§„ÉÄ„ÉºÈñì„ÅßÁ∞°Âçò„Å´Âàá„ÇäÊõø„Åà„Åü„Çä„ÄÅ„Ç≥„É≥„Éù„Éº„Éç„É≥„Éà„ÇíÁµÑ„ÅøÂêà„Çè„Åõ„Åü„Çä„Åß„Åç„Çã„Çà„ÅÜ„Å´„Åó„Åæ„Åô„ÄÇ„Åæ„Åü„ÄÅ„Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥„ÅÆË§áÈõë„Åï„ÅåÂ¢ó„Åô‰∏≠„Åß„ÄÅ„Ç™„Éº„Ç±„Çπ„Éà„É¨„Éº„Ç∑„Éß„É≥„ÇÑÂèØË¶ñÂåñ„ÄÅË©ï‰æ°„ÅÆ„Éã„Éº„Ç∫„Å´„ÇÇÂØæÂøú„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ\\n\\nÂÖ®‰Ωì„Å®„Åó„Å¶„ÄÅLangChain„ÅØ„ÄÅAI„Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥„ÅÆÈñãÁô∫„ÇíÂÆπÊòì„Å´„Åó„ÄÅÈñãÁô∫ËÄÖ„ÅåËá™ÂàÜ„ÅÆ„É¶„Éº„Çπ„Ç±„Éº„Çπ„Å´ÊúÄÈÅ©„Å™„Ç≥„É≥„Éù„Éº„Éç„É≥„Éà„ÇíÈÅ∏Êäû„Åß„Åç„ÇãÊüîËªüÊÄß„ÇíÊèê‰æõ„Åó„Åæ„Åô„ÄÇ'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_query_rag_chain = {\n",
    "    \"question\": RunnablePassthrough(),\n",
    "    \"context\": query_generation_chain | retriever.map(),\n",
    "} | prompt | model | StrOutputParser()\n",
    "\n",
    "multi_query_rag_chain.invoke(\"LangChain„ÅÆÊ¶ÇË¶Å„ÇíÊïô„Åà„Å¶\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-4. Ê§úÁ¥¢Âæå„ÅÆÂ∑•Â§´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "def reciprocal_rank_fusion(\n",
    "    retriever_outputs: list[list[Document]],\n",
    "    k: int = 60,\n",
    ") -> list[str]:\n",
    "    # ÂêÑ„Éâ„Ç≠„É•„É°„É≥„Éà„ÅÆ„Ç≥„É≥„ÉÜ„É≥„ÉÑ (ÊñáÂ≠óÂàó) „Å®„Åù„ÅÆ„Çπ„Ç≥„Ç¢„ÅÆÂØæÂøú„Çí‰øùÊåÅ„Åô„ÇãËæûÊõ∏„ÇíÊ∫ñÂÇô\n",
    "    content_score_mapping = {}\n",
    "\n",
    "    # Ê§úÁ¥¢„ÇØ„Ç®„É™„Åî„Å®„Å´„É´„Éº„ÉóÔºàRFF„Ç¢„É´„Ç¥„É™„Ç∫„É†„Çí‰ΩøÁî®Ôºâ\n",
    "    for docs in retriever_outputs:\n",
    "        print(f\"docs:{docs}\")\n",
    "        # Ê§úÁ¥¢ÁµêÊûú„ÅÆ„Éâ„Ç≠„É•„É°„É≥„Éà„Åî„Å®„Å´„É´„Éº„Éó\n",
    "        for rank, doc in enumerate(docs):\n",
    "            content = doc.page_content\n",
    "            print(f\"rank:{rank}\")\n",
    "            # Âàù„ÇÅ„Å¶ÁôªÂ†¥„Åó„Åü„Ç≥„É≥„ÉÜ„É≥„ÉÑ„ÅÆÂ†¥Âêà„ÅØ„Çπ„Ç≥„Ç¢„Çí0„ÅßÂàùÊúüÂåñ\n",
    "            if content not in content_score_mapping:\n",
    "                content_score_mapping[content] = 0\n",
    "\n",
    "            # (1 / (È†Ü‰Ωç + k)) „ÅÆ„Çπ„Ç≥„Ç¢„ÇíÂä†ÁÆó\n",
    "            content_score_mapping[content] += 1 / (rank + k)\n",
    "            #print(f\"content_score_mapping:{content_score_mapping}\")\n",
    "\n",
    "    # „Çπ„Ç≥„Ç¢„ÅÆÂ§ß„Åç„ÅÑÈ†Ü„Å´„ÇΩ„Éº„Éà\n",
    "    ranked = sorted(content_score_mapping.items(), key=lambda x: x[1], reverse=True)  # noqa\n",
    "    return [content for content, _ in ranked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs:[Document(metadata={'file_name': 'why_langchain.mdx', 'file_path': 'docs\\\\docs\\\\concepts\\\\why_langchain.mdx', 'file_type': '.mdx', 'source': 'docs\\\\docs\\\\concepts\\\\why_langchain.mdx'}, page_content='# Why LangChain?\\n\\nThe goal of `langchain` the Python package and LangChain the company is to make it as easy as possible for developers to build applications that reason.\\nWhile LangChain originally started as a single open source package, it has evolved into a company and a whole ecosystem.\\nThis page will talk about the LangChain ecosystem as a whole.\\nMost of the components within the LangChain ecosystem can be used by themselves - so if you feel particularly drawn to certain components but not others, that is totally fine! Pick and choose whichever components you like best for your own use case!\\n\\n## Features\\n\\nThere are several primary needs that LangChain aims to address:\\n\\n1. **Standardized component interfaces:** The growing number of [models](/docs/integrations/chat/) and [related components](/docs/integrations/vectorstores/) for AI applications has resulted in a wide variety of different APIs that developers need to learn and use.\\nThis diversity can make it challenging for developers to switch between providers or combine components when building applications.\\nLangChain exposes a standard interface for key components, making it easy to switch between providers.\\n\\n2. **Orchestration:** As applications become more complex, combining multiple components and models, there\\'s [a growing need to efficiently connect these elements into control flows](https://lilianweng.github.io/posts/2023-06-23-agent/) that can [accomplish diverse tasks](https://www.sequoiacap.com/article/generative-ais-act-o1/).\\n[Orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)) is crucial for building such applications.\\n\\n3. **Observability and evaluation:** As applications become more complex, it becomes increasingly difficult to understand what is happening within them.\\nFurthermore, the pace of development can become rate-limited by the [paradox of choice](https://en.wikipedia.org/wiki/Paradox_of_choice).\\nFor example, developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\n[Observability](https://en.wikipedia.org/wiki/Observability) and evaluations can help developers monitor their applications and rapidly answer these types of questions with confidence.\\n\\n\\n## Standardized component interfaces\\n\\nLangChain provides common interfaces for components that are central to many AI applications.\\nAs an example, all [chat models](/docs/concepts/chat_models/) implement the [BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) interface.\\nThis provides a standard way to interact with chat models, supporting important but often provider-specific features like [tool calling](/docs/concepts/tool_calling/) and [structured outputs](/docs/concepts/structured_outputs/).\\n\\n\\n### Example: chat models \\n\\nMany [model providers](/docs/concepts/chat_models/) support [tool calling](/docs/concepts/tool_calling/), a critical feature for many applications (e.g., [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/)), that allows a developer to request model responses that match a particular schema.\\nThe APIs for each provider differ. \\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to bind [tools](/docs/concepts/tools) to a model in order to support [tool calling](/docs/concepts/tool_calling/):\\n\\n```python\\n# Tool creation\\ntools = [my_tool]\\n# Tool binding\\nmodel_with_tools = model.bind_tools(tools)\\n```\\n\\nSimilarly, getting models to produce [structured outputs](/docs/concepts/structured_outputs/) is an extremely common use case. \\nProviders support different approaches for this, including [JSON mode or tool calling](https://platform.openai.com/docs/guides/structured-outputs), with different APIs.\\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to produce structured outputs using the `with_structured_output()` method:\\n\\n```python\\n# Define schema\\nschema = ...\\n# Bind schema to model\\nmodel_with_structure = model.with_structured_output(schema)\\n```\\n\\n### Example: retrievers\\n\\nIn the context of [RAG](/docs/concepts/rag/) and LLM application components, LangChain\\'s [retriever](/docs/concepts/retrievers/) interface provides a standard way to connect to many different types of data services or databases (e.g., [vector stores](/docs/concepts/vectorstores) or databases).\\nThe underlying implementation of the retriever depends on the type of data store or database you are connecting to, but all retrievers implement the [runnable interface](/docs/concepts/runnables/), meaning they can be invoked in a common manner.\\n\\n```python\\ndocuments = my_retriever.invoke(\"What is the meaning of life?\")\\n```\\n\\n## Orchestration \\n\\nWhile standardization for individual components is useful, we\\'ve increasingly seen that developers want to *combine* components into more complex applications. \\nThis motivates the need for [orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)).\\nThere are several common characteristics of LLM applications that this orchestration layer should support:\\n\\n* **Complex control flow:** The application requires complex patterns such as cycles (e.g., a loop that reiterates until a condition is met).\\n* **[Persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/):** The application needs to maintain [short-term and / or long-term memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n* **[Human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/):** The application needs human interaction, e.g., pausing, reviewing, editing, approving certain steps.\\n\\nThe recommended way to orchestrate components for complex applications is [LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/).\\nLangGraph is a library that gives developers a high degree of control by expressing the flow of the application as a set of nodes and edges.\\nLangGraph comes with built-in support for [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), [memory](https://langchain-ai.github.io/langgraph/concepts/memory/), and other features.\\nIt\\'s particularly well suited for building [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/) or [multi-agent](https://langchain-ai.github.io/langgraph/concepts/multi_agent/) applications. \\nImportantly, individual LangChain components can be used as LangGraph nodes, but you can also use LangGraph **without** using LangChain components.\\n\\n:::info[Further reading]\\n\\nHave a look at our free course, [Introduction to LangGraph](https://academy.langchain.com/courses/intro-to-langgraph), to learn more about how to use LangGraph to build complex applications.\\n\\n:::\\n\\n## Observability and evaluation\\n\\nThe pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. \\nDevelopers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\nHigh quality tracing and evaluations can help you rapidly answer these types of questions with confidence.\\n[LangSmith](https://docs.smith.langchain.com/) is our platform that supports observability and evaluation for AI applications.\\nSee our conceptual guides on [evaluations](https://docs.smith.langchain.com/concepts/evaluation) and [tracing](https://docs.smith.langchain.com/concepts/tracing) for more details.\\n\\n:::info[Further reading]\\n\\nSee our video playlist on [LangSmith tracing and evaluations](https://youtube.com/playlist?list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S&feature=shared) for more details.\\n\\n:::\\n\\n## Conclusion\\n\\nLangChain offers standard interfaces for components that are central to many AI applications, which offers a few specific advantages:\\n- **Ease of swapping providers:** It allows you to swap out different component providers without having to change the underlying code.\\n- **Advanced features:** It provides common methods for more advanced features, such as [streaming](/docs/concepts/streaming) and [tool calling](/docs/concepts/tool_calling/).\\n\\n[LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/) makes it possible to orchestrate complex applications (e.g., [agents](/docs/concepts/agents/)) and provide features like including [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), or [memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n\\n[LangSmith](https://docs.smith.langchain.com/) makes it possible to iterate with confidence on your applications, by providing LLM-specific observability and framework for testing and evaluating your application.\\n'), Document(metadata={'file_name': 'introduction.mdx', 'file_path': 'docs\\\\docs\\\\introduction.mdx', 'file_type': '.mdx', 'source': 'docs\\\\docs\\\\introduction.mdx'}, page_content='---\\nsidebar_position: 0\\nsidebar_class_name: hidden\\n---\\n\\n# Introduction\\n\\n**LangChain** is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:\\n- **Development**: Build your applications using LangChain\\'s open-source [components](/docs/concepts) and [third-party integrations](/docs/integrations/providers/).\\nUse [LangGraph](/docs/concepts/architecture/#langgraph) to build stateful agents with first-class streaming and human-in-the-loop support.\\n- **Productionization**: Use [LangSmith](https://docs.smith.langchain.com/) to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\n- **Deployment**: Turn your LangGraph applications into production-ready APIs and Assistants with [LangGraph Platform](https://langchain-ai.github.io/langgraph/cloud/).\\n\\nimport ThemedImage from \\'@theme/ThemedImage\\';\\nimport useBaseUrl from \\'@docusaurus/useBaseUrl\\';\\n\\n<ThemedImage\\n  alt=\"Diagram outlining the hierarchical organization of the LangChain framework, displaying the interconnected parts across multiple layers.\"\\n  sources={{\\n    light: useBaseUrl(\\'/svg/langchain_stack_112024.svg\\'),\\n    dark: useBaseUrl(\\'/svg/langchain_stack_112024_dark.svg\\'),\\n  }}\\n  style={{ width: \"100%\" }}\\n  title=\"LangChain Framework Overview\"\\n/>\\n\\nLangChain implements a standard interface for large language models and related\\ntechnologies, such as embedding models and vector stores, and integrates with\\nhundreds of providers. See the [integrations](/docs/integrations/providers/) page for\\nmore.\\n\\nimport ChatModelTabs from \"@theme/ChatModelTabs\";\\n\\n<ChatModelTabs/>\\n\\n```python\\nmodel.invoke(\"Hello, world!\")\\n```\\n\\n:::note\\n\\nThese docs focus on the Python LangChain library. [Head here](https://js.langchain.com) for docs on the JavaScript LangChain library.\\n\\n:::\\n\\n## Architecture\\n\\nThe LangChain framework consists of multiple open-source libraries. Read more in the\\n[Architecture](/docs/concepts/architecture/) page.\\n\\n- **`langchain-core`**: Base abstractions for chat models and other components.\\n- **Integration packages** (e.g. `langchain-openai`, `langchain-anthropic`, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\\n- **`langchain`**: Chains, agents, and retrieval strategies that make up an application\\'s cognitive architecture.\\n- **`langchain-community`**: Third-party integrations that are community maintained.\\n- **`langgraph`**: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See [LangGraph documentation](https://langchain-ai.github.io/langgraph/).\\n\\n## Guides\\n\\n### [Tutorials](/docs/tutorials)\\n\\nIf you\\'re looking to build something specific or are more of a hands-on learner, check out our [tutorials section](/docs/tutorials).\\nThis is the best place to get started.\\n\\nThese are the best ones to get started with:\\n\\n- [Build a Simple LLM Application](/docs/tutorials/llm_chain)\\n- [Build a Chatbot](/docs/tutorials/chatbot)\\n- [Build an Agent](/docs/tutorials/agents)\\n- [Introduction to LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/)\\n\\nExplore the full list of LangChain tutorials [here](/docs/tutorials), and check out other [LangGraph tutorials here](https://langchain-ai.github.io/langgraph/tutorials/). To learn more about LangGraph, check out our first LangChain Academy course, *Introduction to LangGraph*, available [here](https://academy.langchain.com/courses/intro-to-langgraph).\\n\\n\\n### [How-to guides](/docs/how_to)\\n\\n[Here](/docs/how_to) you‚Äôll find short answers to ‚ÄúHow do I‚Ä¶.?‚Äù types of questions.\\nThese how-to guides don‚Äôt cover topics in depth ‚Äì you‚Äôll find that material in the [Tutorials](/docs/tutorials) and the [API Reference](https://python.langchain.com/api_reference/).\\nHowever, these guides will help you quickly accomplish common tasks using [chat models](/docs/how_to/#chat-models),\\n[vector stores](/docs/how_to/#vector-stores), and other common LangChain components.\\n\\nCheck out [LangGraph-specific how-tos here](https://langchain-ai.github.io/langgraph/how-tos/).\\n\\n### [Conceptual guide](/docs/concepts)\\n\\nIntroductions to all the key parts of LangChain you‚Äôll need to know! [Here](/docs/concepts) you\\'ll find high level explanations of all LangChain concepts.\\n\\nFor a deeper dive into LangGraph concepts, check out [this page](https://langchain-ai.github.io/langgraph/concepts/).\\n\\n### [Integrations](integrations/providers/index.mdx)\\n\\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\\nIf you\\'re looking to get up and running quickly with [chat models](/docs/integrations/chat/), [vector stores](/docs/integrations/vectorstores/),\\nor other LangChain components from a specific provider, check out our growing list of [integrations](/docs/integrations/providers/).\\n\\n\\n### [API reference](https://python.langchain.com/api_reference/)\\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\\n\\n## Ecosystem\\n\\n### [ü¶úüõ†Ô∏è LangSmith](https://docs.smith.langchain.com)\\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\\n\\n### [ü¶úüï∏Ô∏è LangGraph](https://langchain-ai.github.io/langgraph)\\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by Linkedin, Uber, Klarna, GitLab, and many more.\\n\\n## Additional resources\\n\\n### [Versions](/docs/versions/v0_3/)\\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\\n\\n### [Security](/docs/security)\\nRead up on [security](/docs/security) best practices to make sure you\\'re developing safely with LangChain.\\n\\n### [Contributing](contributing/index.mdx)\\nCheck out the developer\\'s guide for guidelines on contributing and help getting your dev environment set up.\\n'), Document(metadata={'file_name': 'lcel.mdx', 'file_path': 'docs\\\\docs\\\\concepts\\\\lcel.mdx', 'file_type': '.mdx', 'source': 'docs\\\\docs\\\\concepts\\\\lcel.mdx'}, page_content='# LangChain Expression Language (LCEL)\\n\\n:::info Prerequisites\\n* [Runnable Interface](/docs/concepts/runnables)\\n:::\\n\\nThe **L**ang**C**hain **E**xpression **L**anguage (LCEL) takes a [declarative](https://en.wikipedia.org/wiki/Declarative_programming) approach to building new [Runnables](/docs/concepts/runnables) from existing Runnables.\\n\\nThis means that you describe what *should* happen, rather than *how* it should happen, allowing LangChain to optimize the run-time execution of the chains.\\n\\nWe often refer to a `Runnable` created using LCEL as a \"chain\". It\\'s important to remember that a \"chain\" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).\\n\\n:::note\\n* The [LCEL cheatsheet](/docs/how_to/lcel_cheatsheet/) shows common patterns that involve the Runnable interface and LCEL expressions.\\n* Please see the following list of [how-to guides](/docs/how_to/#langchain-expression-language-lcel) that cover common tasks with LCEL.\\n* A list of built-in `Runnables` can be found in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html). Many of these Runnables are useful when composing custom \"chains\" in LangChain using LCEL.\\n:::\\n\\n## Benefits of LCEL\\n\\nLangChain optimizes the run-time execution of chains built with LCEL in a number of ways:\\n\\n- **Optimized parallel execution**: Run Runnables in parallel using [RunnableParallel](#runnableparallel) or run multiple inputs through a given chain in parallel using the [Runnable Batch API](/docs/concepts/runnables/#optimized-parallel-execution-batch). Parallel execution can significantly reduce the latency as processing can be done in parallel instead of sequentially.\\n- **Guaranteed Async support**: Any chain built with LCEL can be run asynchronously using the [Runnable Async API](/docs/concepts/runnables/#asynchronous-support). This can be useful when running chains in a server environment where you want to handle large number of requests concurrently.\\n- **Simplify streaming**: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(time elapsed until the first chunk of output from a [chat model](/docs/concepts/chat_models) or [llm](/docs/concepts/text_llms) comes out).\\n\\nOther benefits include:\\n\\n- [**Seamless LangSmith tracing**](https://docs.smith.langchain.com)\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, **all** steps are automatically logged to [LangSmith](https://docs.smith.langchain.com/) for maximum observability and debuggability.\\n- **Standard API**: Because all chains are built using the Runnable interface, they can be used in the same way as any other Runnable.\\n- [**Deployable with LangServe**](/docs/concepts/architecture#langserve): Chains built with LCEL can be deployed using for production use.\\n\\n## Should I use LCEL?\\n\\nLCEL is an [orchestration solution](https://en.wikipedia.org/wiki/Orchestration_(computing)) -- it allows LangChain to handle run-time execution of chains in an optimized way.\\n\\nWhile we have seen users run chains with hundreds of steps in production, we generally recommend using LCEL for simpler orchestration tasks. When the application requires complex state management, branching, cycles or multiple agents, we recommend that users take advantage of [LangGraph](/docs/concepts/architecture#langgraph).\\n\\nIn LangGraph, users define graphs that specify the application\\'s flow. This allows users to keep using LCEL within individual nodes when LCEL is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.\\n\\nHere are some guidelines:\\n\\n* If you are making a single LLM call, you don\\'t need LCEL; instead call the underlying [chat model](/docs/concepts/chat_models) directly.\\n* If you have a simple chain (e.g., prompt + llm + parser, simple retrieval set up etc.), LCEL is a reasonable fit, if you\\'re taking advantage of the LCEL benefits.\\n* If you\\'re building a complex chain (e.g., with branching, cycles, multiple agents, etc.) use [LangGraph](/docs/concepts/architecture#langgraph) instead. Remember that you can always use LCEL within individual nodes in LangGraph.\\n\\n## Composition Primitives\\n\\n`LCEL` chains are built by composing existing `Runnables` together. The two main composition primitives are [RunnableSequence](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableSequence.html#langchain_core.runnables.base.RunnableSequence) and [RunnableParallel](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableParallel.html#langchain_core.runnables.base.RunnableParallel).\\n\\nMany other composition primitives (e.g., [RunnableAssign](\\nhttps://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnableAssign.html#langchain_core.runnables.passthrough.RunnableAssign\\n)) can be thought of as variations of these two primitives.\\n\\n:::note\\nYou can find a list of all composition primitives in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html).\\n:::\\n\\n### RunnableSequence\\n\\n`RunnableSequence` is a composition primitive that allows you \"chain\" multiple runnables sequentially, with the output of one runnable serving as the input to the next.\\n\\n```python\\nfrom langchain_core.runnables import RunnableSequence\\nchain = RunnableSequence([runnable1, runnable2])\\n```\\n\\nInvoking the `chain` with some input:\\n\\n```python\\nfinal_output = chain.invoke(some_input)\\n```\\n\\ncorresponds to the following:\\n\\n```python\\noutput1 = runnable1.invoke(some_input)\\nfinal_output = runnable2.invoke(output1)\\n```\\n\\n:::note\\n`runnable1` and `runnable2` are placeholders for any `Runnable` that you want to chain together.\\n:::\\n\\n### RunnableParallel\\n\\n`RunnableParallel` is a composition primitive that allows you to run multiple runnables concurrently, with the same input provided to each.\\n\\n```python\\nfrom langchain_core.runnables import RunnableParallel\\nchain = RunnableParallel({\\n    \"key1\": runnable1,\\n    \"key2\": runnable2,\\n})\\n```\\n\\nInvoking the `chain` with some input:\\n\\n```python\\nfinal_output = chain.invoke(some_input)\\n```\\n\\nWill yield a `final_output` dictionary with the same keys as the input dictionary, but with the values replaced by the output of the corresponding runnable.\\n\\n```python\\n{\\n    \"key1\": runnable1.invoke(some_input),\\n    \"key2\": runnable2.invoke(some_input),\\n}\\n```\\n\\nRecall, that the runnables are executed in parallel, so while the result is the same as\\ndictionary comprehension shown above, the execution time is much faster.\\n\\n:::note\\n`RunnableParallel`supports both synchronous and asynchronous execution (as all `Runnables` do).\\n\\n* For synchronous execution, `RunnableParallel` uses a [ThreadPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor) to run the runnables concurrently.\\n* For asynchronous execution, `RunnableParallel` uses [asyncio.gather](https://docs.python.org/3/library/asyncio.html#asyncio.gather) to run the runnables concurrently.\\n:::\\n\\n## Composition Syntax\\n\\nThe usage of `RunnableSequence` and `RunnableParallel` is so common that we created a shorthand syntax for using them. This helps\\nto make the code more readable and concise.\\n\\n### The `|` operator\\n\\nWe have [overloaded](https://docs.python.org/3/reference/datamodel.html#special-method-names) the `|` operator to create a `RunnableSequence` from two `Runnables`.\\n\\n```python\\nchain = runnable1 | runnable2\\n```\\n\\nis Equivalent to:\\n\\n```python\\nchain = RunnableSequence([runnable1, runnable2])\\n```\\n\\n### The `.pipe` method\\n\\nIf you have moral qualms with operator overloading, you can use the `.pipe` method instead. This is equivalent to the `|` operator.\\n\\n```python\\nchain = runnable1.pipe(runnable2)\\n```\\n\\n### Coercion\\n\\nLCEL applies automatic type coercion to make it easier to compose chains.\\n\\nIf you do not understand the type coercion, you can always use the `RunnableSequence` and `RunnableParallel` classes directly.\\n\\nThis will make the code more verbose, but it will also make it more explicit.\\n\\n#### Dictionary to RunnableParallel\\n\\nInside an LCEL expression, a dictionary is automatically converted to a `RunnableParallel`.\\n\\nFor example, the following code:\\n\\n```python\\nmapping = {\\n    \"key1\": runnable1,\\n    \"key2\": runnable2,\\n}\\n\\nchain = mapping | runnable3\\n```\\n\\nIt gets automatically converted to the following:\\n\\n```python\\nchain = RunnableSequence([RunnableParallel(mapping), runnable3])\\n```\\n\\n:::caution\\nYou have to be careful because the `mapping` dictionary is not a `RunnableParallel` object, it is just a dictionary. This means that the following code will raise an `AttributeError`:\\n\\n```python\\nmapping.invoke(some_input)\\n```\\n:::\\n\\n#### Function to RunnableLambda\\n\\nInside an LCEL expression, a function is automatically converted to a `RunnableLambda`.\\n\\n```\\ndef some_func(x):\\n    return x\\n\\nchain = some_func | runnable1\\n```\\n\\nIt gets automatically converted to the following:\\n\\n```python\\nchain = RunnableSequence([RunnableLambda(some_func), runnable1])\\n```\\n\\n:::caution\\nYou have to be careful because the lambda function is not a `RunnableLambda` object, it is just a function. This means that the following code will raise an `AttributeError`:\\n\\n```python\\nlambda x: x + 1.invoke(some_input)\\n```\\n:::\\n\\n## Legacy chains\\n\\nLCEL aims to provide consistency around behavior and customization over legacy subclassed chains such as `LLMChain` and\\n`ConversationalRetrievalChain`. Many of these legacy chains hide important details like prompts, and as a wider variety\\nof viable models emerge, customization has become more and more important.\\n\\nIf you are currently using one of these legacy chains, please see [this guide for guidance on how to migrate](/docs/versions/migrating_chains).\\n\\nFor guides on how to do specific tasks with LCEL, check out [the relevant how-to guides](/docs/how_to/#langchain-expression-language-lcel).\\n'), Document(metadata={'file_name': 'index.mdx', 'file_path': 'docs\\\\docs\\\\concepts\\\\index.mdx', 'file_type': '.mdx', 'source': 'docs\\\\docs\\\\concepts\\\\index.mdx'}, page_content='# Conceptual guide\\n\\nThis guide provides explanations of the key concepts behind the LangChain framework and AI applications more broadly.\\n\\nWe recommend that you go through at least one of the [Tutorials](/docs/tutorials) before diving into the conceptual guide. This will provide practical context that will make it easier to understand the concepts discussed here.\\n\\nThe conceptual guide does not cover step-by-step instructions or specific implementation examples ‚Äî those are found in the [How-to guides](/docs/how_to/) and [Tutorials](/docs/tutorials). For detailed reference material, please see the [API reference](https://python.langchain.com/api_reference/).\\n\\n## High level\\n\\n- **[Why LangChain?](/docs/concepts/why_langchain)**: Overview of the value that LangChain provides.\\n- **[Architecture](/docs/concepts/architecture)**: How packages are organized in the LangChain ecosystem.\\n\\n## Concepts\\n\\n- **[Chat models](/docs/concepts/chat_models)**: LLMs exposed via a chat API that process sequences of messages as input and output a message.\\n- **[Messages](/docs/concepts/messages)**: The unit of communication in chat models, used to represent model input and output.\\n- **[Chat history](/docs/concepts/chat_history)**: A conversation represented as a sequence of messages, alternating between user messages and model responses.\\n- **[Tools](/docs/concepts/tools)**: A function with an associated schema defining the function\\'s name, description, and the arguments it accepts.\\n- **[Tool calling](/docs/concepts/tool_calling)**: A type of chat model API that accepts tool schemas, along with messages, as input and returns invocations of those tools as part of the output message.\\n- **[Structured output](/docs/concepts/structured_outputs)**: A technique to make a chat model respond in a structured format, such as JSON that matches a given schema.\\n- **[Memory](https://langchain-ai.github.io/langgraph/concepts/memory/)**: Information about a conversation that is persisted so that it can be used in future conversations.\\n- **[Multimodality](/docs/concepts/multimodality)**: The ability to work with data that comes in different forms, such as text, audio, images, and video.\\n- **[Runnable interface](/docs/concepts/runnables)**: The base abstraction that many LangChain components and the LangChain Expression Language are built on.\\n- **[Streaming](/docs/concepts/streaming)**: LangChain streaming APIs for surfacing results as they are generated.\\n- **[LangChain Expression Language (LCEL)](/docs/concepts/lcel)**: A syntax for orchestrating LangChain components. Most useful for simpler applications.\\n- **[Document loaders](/docs/concepts/document_loaders)**: Load a source as a list of documents.\\n- **[Retrieval](/docs/concepts/retrieval)**: Information retrieval systems can retrieve structured or unstructured data from a datasource in response to a query.\\n- **[Text splitters](/docs/concepts/text_splitters)**: Split long text into smaller chunks that can be individually indexed to enable granular retrieval.\\n- **[Embedding models](/docs/concepts/embedding_models)**: Models that represent data such as text or images in a vector space.\\n- **[Vector stores](/docs/concepts/vectorstores)**: Storage of and efficient search over vectors and associated metadata.\\n- **[Retriever](/docs/concepts/retrievers)**: A component that returns relevant documents from a knowledge base in response to a query.\\n- **[Retrieval Augmented Generation (RAG)](/docs/concepts/rag)**: A technique that enhances language models by combining them with external knowledge bases.\\n- **[Agents](/docs/concepts/agents)**: Use a [language model](/docs/concepts/chat_models) to choose a sequence of actions to take. Agents can interact with external resources via [tool](/docs/concepts/tools).\\n- **[Prompt templates](/docs/concepts/prompt_templates)**: Component for factoring out the static parts of a model \"prompt\" (usually a sequence of messages). Useful for serializing, versioning, and reusing these static parts.\\n- **[Output parsers](/docs/concepts/output_parsers)**: Responsible for taking the output of a model and transforming it into a more suitable format for downstream tasks. Output parsers were primarily useful prior to the general availability of [tool calling](/docs/concepts/tool_calling) and [structured outputs](/docs/concepts/structured_outputs).\\n- **[Few-shot prompting](/docs/concepts/few_shot_prompting)**: A technique for improving model performance by providing a few examples of the task to perform in the prompt.\\n- **[Example selectors](/docs/concepts/example_selectors)**: Used to select the most relevant examples from a dataset based on a given input. Example selectors are used in few-shot prompting to select examples for a prompt.\\n- **[Async programming](/docs/concepts/async)**: The basics that one should know to use LangChain in an asynchronous context.\\n- **[Callbacks](/docs/concepts/callbacks)**: Callbacks enable the execution of custom auxiliary code in built-in components. Callbacks are used to stream outputs from LLMs in LangChain, trace the intermediate steps of an application, and more.\\n- **[Tracing](/docs/concepts/tracing)**: The process of recording the steps that an application takes to go from input to output. Tracing is essential for debugging and diagnosing issues in complex applications.\\n- **[Evaluation](/docs/concepts/evaluation)**: The process of assessing the performance and effectiveness of AI applications. This involves testing the model\\'s responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose. This process is vital for building reliable applications.\\n- **[Testing](/docs/concepts/testing)**: The process of verifying that a component of an integration or application works as expected. Testing is essential for ensuring that the application behaves correctly and that changes to the codebase do not introduce new bugs.\\n\\n## Glossary\\n\\n- **[AIMessageChunk](/docs/concepts/messages#aimessagechunk)**: A partial response from an AI message. Used when streaming responses from a chat model.\\n- **[AIMessage](/docs/concepts/messages#aimessage)**: Represents a complete response from an AI model.\\n- **[astream_events](/docs/concepts/chat_models#key-methods)**: Stream granular information from [LCEL](/docs/concepts/lcel) chains.\\n- **[BaseTool](/docs/concepts/tools/#tool-interface)**: The base class for all tools in LangChain.\\n- **[batch](/docs/concepts/runnables)**: Use to execute a runnable with batch inputs.\\n- **[bind_tools](/docs/concepts/tool_calling/#tool-binding)**: Allows models to interact with tools.\\n- **[Caching](/docs/concepts/chat_models#caching)**: Storing results to avoid redundant calls to a chat model.\\n- **[Chat models](/docs/concepts/multimodality/#multimodality-in-chat-models)**: Chat models that handle multiple data modalities.\\n- **[Configurable runnables](/docs/concepts/runnables/#configurable-runnables)**: Creating configurable Runnables.\\n- **[Context window](/docs/concepts/chat_models#context-window)**: The maximum size of input a chat model can process.\\n- **[Conversation patterns](/docs/concepts/chat_history#conversation-patterns)**: Common patterns in chat interactions.\\n- **[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)**: LangChain\\'s representation of a document.\\n- **[Embedding models](/docs/concepts/multimodality/#multimodality-in-embedding-models)**: Models that generate vector embeddings for various data types.\\n- **[HumanMessage](/docs/concepts/messages#humanmessage)**: Represents a message from a human user.\\n- **[InjectedState](/docs/concepts/tools#injectedstate)**: A state injected into a tool function.\\n- **[InjectedStore](/docs/concepts/tools#injectedstore)**: A store that can be injected into a tool for data persistence.\\n- **[InjectedToolArg](/docs/concepts/tools#injectedtoolarg)**: Mechanism to inject arguments into tool functions.\\n- **[input and output types](/docs/concepts/runnables#input-and-output-types)**: Types used for input and output in Runnables.\\n- **[Integration packages](/docs/concepts/architecture/#integration-packages)**: Third-party packages that integrate with LangChain.\\n- **[Integration tests](/docs/concepts/testing#integration-tests)**: Tests that verify the correctness of the interaction between components, usually run with access to the underlying API that powers an integration.\\n- **[invoke](/docs/concepts/runnables)**: A standard method to invoke a Runnable.\\n- **[JSON mode](/docs/concepts/structured_outputs#json-mode)**: Returning responses in JSON format.\\n- **[langchain-community](/docs/concepts/architecture#langchain-community)**: Community-driven components for LangChain.\\n- **[langchain-core](/docs/concepts/architecture#langchain-core)**: Core langchain package. Includes base interfaces and in-memory implementations.\\n- **[langchain](/docs/concepts/architecture#langchain)**: A package for higher level components (e.g., some pre-built chains).\\n- **[langgraph](/docs/concepts/architecture#langgraph)**: Powerful orchestration layer for LangChain. Use to build complex pipelines and workflows.\\n- **[langserve](/docs/concepts/architecture#langserve)**: Used to deploy LangChain Runnables as REST endpoints. Uses FastAPI. Works primarily for LangChain Runnables, does not currently integrate with LangGraph.\\n- **[LLMs (legacy)](/docs/concepts/text_llms)**: Older language models that take a string as input and return a string as output.\\n- **[Managing chat history](/docs/concepts/chat_history#managing-chat-history)**: Techniques to maintain and manage the chat history.\\n- **[OpenAI format](/docs/concepts/messages#openai-format)**: OpenAI\\'s message format for chat models.\\n- **[Propagation of RunnableConfig](/docs/concepts/runnables/#propagation-of-runnableconfig)**: Propagating configuration through Runnables. Read if working with python 3.9, 3.10 and async.\\n- **[rate-limiting](/docs/concepts/chat_models#rate-limiting)**: Client side rate limiting for chat models.\\n- **[RemoveMessage](/docs/concepts/messages/#removemessage)**: An abstraction used to remove a message from chat history, used primarily in LangGraph.\\n- **[role](/docs/concepts/messages#role)**: Represents the role (e.g., user, assistant) of a chat message.\\n- **[RunnableConfig](/docs/concepts/runnables/#runnableconfig)**: Use to pass run time information to Runnables (e.g., `run_name`, `run_id`, `tags`, `metadata`, `max_concurrency`, `recursion_limit`, `configurable`).\\n- **[Standard parameters for chat models](/docs/concepts/chat_models#standard-parameters)**: Parameters such as API key, `temperature`, and `max_tokens`.\\n- **[Standard tests](/docs/concepts/testing#standard-tests)**: A defined set of unit and integration tests that all integrations must pass.\\n- **[stream](/docs/concepts/streaming)**: Use to stream output from a Runnable or a graph.\\n- **[Tokenization](/docs/concepts/tokens)**: The process of converting data into tokens and vice versa.\\n- **[Tokens](/docs/concepts/tokens)**: The basic unit that a language model reads, processes, and generates under the hood.\\n- **[Tool artifacts](/docs/concepts/tools#tool-artifacts)**: Add artifacts to the output of a tool that will not be sent to the model, but will be available for downstream processing.\\n- **[Tool binding](/docs/concepts/tool_calling#tool-binding)**: Binding tools to models.\\n- **[@tool](/docs/concepts/tools/#create-tools-using-the-tool-decorator)**: Decorator for creating tools in LangChain.\\n- **[Toolkits](/docs/concepts/tools#toolkits)**: A collection of tools that can be used together.\\n- **[ToolMessage](/docs/concepts/messages#toolmessage)**: Represents a message that contains the results of a tool execution.\\n- **[Unit tests](/docs/concepts/testing#unit-tests)**: Tests that verify the correctness of individual components, run in isolation without access to the Internet.\\n- **[Vector stores](/docs/concepts/vectorstores)**: Datastores specialized for storing and efficiently searching vector embeddings.\\n- **[with_structured_output](/docs/concepts/structured_outputs/#structured-output-method)**: A helper method for chat models that natively support [tool calling](/docs/concepts/tool_calling) to get structured output matching a given schema specified via Pydantic, JSON schema or a function.\\n- **[with_types](/docs/concepts/runnables#with_types)**: Method to overwrite the input and output types of a runnable. Useful when working with complex LCEL chains and deploying with LangServe.\\n')]\n",
      "rank:0\n",
      "rank:1\n",
      "rank:2\n",
      "rank:3\n",
      "docs:[Document(metadata={'file_name': 'why_langchain.mdx', 'file_path': 'docs\\\\docs\\\\concepts\\\\why_langchain.mdx', 'file_type': '.mdx', 'source': 'docs\\\\docs\\\\concepts\\\\why_langchain.mdx'}, page_content='# Why LangChain?\\n\\nThe goal of `langchain` the Python package and LangChain the company is to make it as easy as possible for developers to build applications that reason.\\nWhile LangChain originally started as a single open source package, it has evolved into a company and a whole ecosystem.\\nThis page will talk about the LangChain ecosystem as a whole.\\nMost of the components within the LangChain ecosystem can be used by themselves - so if you feel particularly drawn to certain components but not others, that is totally fine! Pick and choose whichever components you like best for your own use case!\\n\\n## Features\\n\\nThere are several primary needs that LangChain aims to address:\\n\\n1. **Standardized component interfaces:** The growing number of [models](/docs/integrations/chat/) and [related components](/docs/integrations/vectorstores/) for AI applications has resulted in a wide variety of different APIs that developers need to learn and use.\\nThis diversity can make it challenging for developers to switch between providers or combine components when building applications.\\nLangChain exposes a standard interface for key components, making it easy to switch between providers.\\n\\n2. **Orchestration:** As applications become more complex, combining multiple components and models, there\\'s [a growing need to efficiently connect these elements into control flows](https://lilianweng.github.io/posts/2023-06-23-agent/) that can [accomplish diverse tasks](https://www.sequoiacap.com/article/generative-ais-act-o1/).\\n[Orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)) is crucial for building such applications.\\n\\n3. **Observability and evaluation:** As applications become more complex, it becomes increasingly difficult to understand what is happening within them.\\nFurthermore, the pace of development can become rate-limited by the [paradox of choice](https://en.wikipedia.org/wiki/Paradox_of_choice).\\nFor example, developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\n[Observability](https://en.wikipedia.org/wiki/Observability) and evaluations can help developers monitor their applications and rapidly answer these types of questions with confidence.\\n\\n\\n## Standardized component interfaces\\n\\nLangChain provides common interfaces for components that are central to many AI applications.\\nAs an example, all [chat models](/docs/concepts/chat_models/) implement the [BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) interface.\\nThis provides a standard way to interact with chat models, supporting important but often provider-specific features like [tool calling](/docs/concepts/tool_calling/) and [structured outputs](/docs/concepts/structured_outputs/).\\n\\n\\n### Example: chat models \\n\\nMany [model providers](/docs/concepts/chat_models/) support [tool calling](/docs/concepts/tool_calling/), a critical feature for many applications (e.g., [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/)), that allows a developer to request model responses that match a particular schema.\\nThe APIs for each provider differ. \\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to bind [tools](/docs/concepts/tools) to a model in order to support [tool calling](/docs/concepts/tool_calling/):\\n\\n```python\\n# Tool creation\\ntools = [my_tool]\\n# Tool binding\\nmodel_with_tools = model.bind_tools(tools)\\n```\\n\\nSimilarly, getting models to produce [structured outputs](/docs/concepts/structured_outputs/) is an extremely common use case. \\nProviders support different approaches for this, including [JSON mode or tool calling](https://platform.openai.com/docs/guides/structured-outputs), with different APIs.\\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to produce structured outputs using the `with_structured_output()` method:\\n\\n```python\\n# Define schema\\nschema = ...\\n# Bind schema to model\\nmodel_with_structure = model.with_structured_output(schema)\\n```\\n\\n### Example: retrievers\\n\\nIn the context of [RAG](/docs/concepts/rag/) and LLM application components, LangChain\\'s [retriever](/docs/concepts/retrievers/) interface provides a standard way to connect to many different types of data services or databases (e.g., [vector stores](/docs/concepts/vectorstores) or databases).\\nThe underlying implementation of the retriever depends on the type of data store or database you are connecting to, but all retrievers implement the [runnable interface](/docs/concepts/runnables/), meaning they can be invoked in a common manner.\\n\\n```python\\ndocuments = my_retriever.invoke(\"What is the meaning of life?\")\\n```\\n\\n## Orchestration \\n\\nWhile standardization for individual components is useful, we\\'ve increasingly seen that developers want to *combine* components into more complex applications. \\nThis motivates the need for [orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)).\\nThere are several common characteristics of LLM applications that this orchestration layer should support:\\n\\n* **Complex control flow:** The application requires complex patterns such as cycles (e.g., a loop that reiterates until a condition is met).\\n* **[Persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/):** The application needs to maintain [short-term and / or long-term memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n* **[Human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/):** The application needs human interaction, e.g., pausing, reviewing, editing, approving certain steps.\\n\\nThe recommended way to orchestrate components for complex applications is [LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/).\\nLangGraph is a library that gives developers a high degree of control by expressing the flow of the application as a set of nodes and edges.\\nLangGraph comes with built-in support for [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), [memory](https://langchain-ai.github.io/langgraph/concepts/memory/), and other features.\\nIt\\'s particularly well suited for building [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/) or [multi-agent](https://langchain-ai.github.io/langgraph/concepts/multi_agent/) applications. \\nImportantly, individual LangChain components can be used as LangGraph nodes, but you can also use LangGraph **without** using LangChain components.\\n\\n:::info[Further reading]\\n\\nHave a look at our free course, [Introduction to LangGraph](https://academy.langchain.com/courses/intro-to-langgraph), to learn more about how to use LangGraph to build complex applications.\\n\\n:::\\n\\n## Observability and evaluation\\n\\nThe pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. \\nDevelopers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\nHigh quality tracing and evaluations can help you rapidly answer these types of questions with confidence.\\n[LangSmith](https://docs.smith.langchain.com/) is our platform that supports observability and evaluation for AI applications.\\nSee our conceptual guides on [evaluations](https://docs.smith.langchain.com/concepts/evaluation) and [tracing](https://docs.smith.langchain.com/concepts/tracing) for more details.\\n\\n:::info[Further reading]\\n\\nSee our video playlist on [LangSmith tracing and evaluations](https://youtube.com/playlist?list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S&feature=shared) for more details.\\n\\n:::\\n\\n## Conclusion\\n\\nLangChain offers standard interfaces for components that are central to many AI applications, which offers a few specific advantages:\\n- **Ease of swapping providers:** It allows you to swap out different component providers without having to change the underlying code.\\n- **Advanced features:** It provides common methods for more advanced features, such as [streaming](/docs/concepts/streaming) and [tool calling](/docs/concepts/tool_calling/).\\n\\n[LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/) makes it possible to orchestrate complex applications (e.g., [agents](/docs/concepts/agents/)) and provide features like including [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), or [memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n\\n[LangSmith](https://docs.smith.langchain.com/) makes it possible to iterate with confidence on your applications, by providing LLM-specific observability and framework for testing and evaluating your application.\\n'), Document(metadata={'file_name': 'lcel.mdx', 'file_path': 'docs\\\\docs\\\\concepts\\\\lcel.mdx', 'file_type': '.mdx', 'source': 'docs\\\\docs\\\\concepts\\\\lcel.mdx'}, page_content='# LangChain Expression Language (LCEL)\\n\\n:::info Prerequisites\\n* [Runnable Interface](/docs/concepts/runnables)\\n:::\\n\\nThe **L**ang**C**hain **E**xpression **L**anguage (LCEL) takes a [declarative](https://en.wikipedia.org/wiki/Declarative_programming) approach to building new [Runnables](/docs/concepts/runnables) from existing Runnables.\\n\\nThis means that you describe what *should* happen, rather than *how* it should happen, allowing LangChain to optimize the run-time execution of the chains.\\n\\nWe often refer to a `Runnable` created using LCEL as a \"chain\". It\\'s important to remember that a \"chain\" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).\\n\\n:::note\\n* The [LCEL cheatsheet](/docs/how_to/lcel_cheatsheet/) shows common patterns that involve the Runnable interface and LCEL expressions.\\n* Please see the following list of [how-to guides](/docs/how_to/#langchain-expression-language-lcel) that cover common tasks with LCEL.\\n* A list of built-in `Runnables` can be found in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html). Many of these Runnables are useful when composing custom \"chains\" in LangChain using LCEL.\\n:::\\n\\n## Benefits of LCEL\\n\\nLangChain optimizes the run-time execution of chains built with LCEL in a number of ways:\\n\\n- **Optimized parallel execution**: Run Runnables in parallel using [RunnableParallel](#runnableparallel) or run multiple inputs through a given chain in parallel using the [Runnable Batch API](/docs/concepts/runnables/#optimized-parallel-execution-batch). Parallel execution can significantly reduce the latency as processing can be done in parallel instead of sequentially.\\n- **Guaranteed Async support**: Any chain built with LCEL can be run asynchronously using the [Runnable Async API](/docs/concepts/runnables/#asynchronous-support). This can be useful when running chains in a server environment where you want to handle large number of requests concurrently.\\n- **Simplify streaming**: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(time elapsed until the first chunk of output from a [chat model](/docs/concepts/chat_models) or [llm](/docs/concepts/text_llms) comes out).\\n\\nOther benefits include:\\n\\n- [**Seamless LangSmith tracing**](https://docs.smith.langchain.com)\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, **all** steps are automatically logged to [LangSmith](https://docs.smith.langchain.com/) for maximum observability and debuggability.\\n- **Standard API**: Because all chains are built using the Runnable interface, they can be used in the same way as any other Runnable.\\n- [**Deployable with LangServe**](/docs/concepts/architecture#langserve): Chains built with LCEL can be deployed using for production use.\\n\\n## Should I use LCEL?\\n\\nLCEL is an [orchestration solution](https://en.wikipedia.org/wiki/Orchestration_(computing)) -- it allows LangChain to handle run-time execution of chains in an optimized way.\\n\\nWhile we have seen users run chains with hundreds of steps in production, we generally recommend using LCEL for simpler orchestration tasks. When the application requires complex state management, branching, cycles or multiple agents, we recommend that users take advantage of [LangGraph](/docs/concepts/architecture#langgraph).\\n\\nIn LangGraph, users define graphs that specify the application\\'s flow. This allows users to keep using LCEL within individual nodes when LCEL is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.\\n\\nHere are some guidelines:\\n\\n* If you are making a single LLM call, you don\\'t need LCEL; instead call the underlying [chat model](/docs/concepts/chat_models) directly.\\n* If you have a simple chain (e.g., prompt + llm + parser, simple retrieval set up etc.), LCEL is a reasonable fit, if you\\'re taking advantage of the LCEL benefits.\\n* If you\\'re building a complex chain (e.g., with branching, cycles, multiple agents, etc.) use [LangGraph](/docs/concepts/architecture#langgraph) instead. Remember that you can always use LCEL within individual nodes in LangGraph.\\n\\n## Composition Primitives\\n\\n`LCEL` chains are built by composing existing `Runnables` together. The two main composition primitives are [RunnableSequence](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableSequence.html#langchain_core.runnables.base.RunnableSequence) and [RunnableParallel](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableParallel.html#langchain_core.runnables.base.RunnableParallel).\\n\\nMany other composition primitives (e.g., [RunnableAssign](\\nhttps://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnableAssign.html#langchain_core.runnables.passthrough.RunnableAssign\\n)) can be thought of as variations of these two primitives.\\n\\n:::note\\nYou can find a list of all composition primitives in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html).\\n:::\\n\\n### RunnableSequence\\n\\n`RunnableSequence` is a composition primitive that allows you \"chain\" multiple runnables sequentially, with the output of one runnable serving as the input to the next.\\n\\n```python\\nfrom langchain_core.runnables import RunnableSequence\\nchain = RunnableSequence([runnable1, runnable2])\\n```\\n\\nInvoking the `chain` with some input:\\n\\n```python\\nfinal_output = chain.invoke(some_input)\\n```\\n\\ncorresponds to the following:\\n\\n```python\\noutput1 = runnable1.invoke(some_input)\\nfinal_output = runnable2.invoke(output1)\\n```\\n\\n:::note\\n`runnable1` and `runnable2` are placeholders for any `Runnable` that you want to chain together.\\n:::\\n\\n### RunnableParallel\\n\\n`RunnableParallel` is a composition primitive that allows you to run multiple runnables concurrently, with the same input provided to each.\\n\\n```python\\nfrom langchain_core.runnables import RunnableParallel\\nchain = RunnableParallel({\\n    \"key1\": runnable1,\\n    \"key2\": runnable2,\\n})\\n```\\n\\nInvoking the `chain` with some input:\\n\\n```python\\nfinal_output = chain.invoke(some_input)\\n```\\n\\nWill yield a `final_output` dictionary with the same keys as the input dictionary, but with the values replaced by the output of the corresponding runnable.\\n\\n```python\\n{\\n    \"key1\": runnable1.invoke(some_input),\\n    \"key2\": runnable2.invoke(some_input),\\n}\\n```\\n\\nRecall, that the runnables are executed in parallel, so while the result is the same as\\ndictionary comprehension shown above, the execution time is much faster.\\n\\n:::note\\n`RunnableParallel`supports both synchronous and asynchronous execution (as all `Runnables` do).\\n\\n* For synchronous execution, `RunnableParallel` uses a [ThreadPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor) to run the runnables concurrently.\\n* For asynchronous execution, `RunnableParallel` uses [asyncio.gather](https://docs.python.org/3/library/asyncio.html#asyncio.gather) to run the runnables concurrently.\\n:::\\n\\n## Composition Syntax\\n\\nThe usage of `RunnableSequence` and `RunnableParallel` is so common that we created a shorthand syntax for using them. This helps\\nto make the code more readable and concise.\\n\\n### The `|` operator\\n\\nWe have [overloaded](https://docs.python.org/3/reference/datamodel.html#special-method-names) the `|` operator to create a `RunnableSequence` from two `Runnables`.\\n\\n```python\\nchain = runnable1 | runnable2\\n```\\n\\nis Equivalent to:\\n\\n```python\\nchain = RunnableSequence([runnable1, runnable2])\\n```\\n\\n### The `.pipe` method\\n\\nIf you have moral qualms with operator overloading, you can use the `.pipe` method instead. This is equivalent to the `|` operator.\\n\\n```python\\nchain = runnable1.pipe(runnable2)\\n```\\n\\n### Coercion\\n\\nLCEL applies automatic type coercion to make it easier to compose chains.\\n\\nIf you do not understand the type coercion, you can always use the `RunnableSequence` and `RunnableParallel` classes directly.\\n\\nThis will make the code more verbose, but it will also make it more explicit.\\n\\n#### Dictionary to RunnableParallel\\n\\nInside an LCEL expression, a dictionary is automatically converted to a `RunnableParallel`.\\n\\nFor example, the following code:\\n\\n```python\\nmapping = {\\n    \"key1\": runnable1,\\n    \"key2\": runnable2,\\n}\\n\\nchain = mapping | runnable3\\n```\\n\\nIt gets automatically converted to the following:\\n\\n```python\\nchain = RunnableSequence([RunnableParallel(mapping), runnable3])\\n```\\n\\n:::caution\\nYou have to be careful because the `mapping` dictionary is not a `RunnableParallel` object, it is just a dictionary. This means that the following code will raise an `AttributeError`:\\n\\n```python\\nmapping.invoke(some_input)\\n```\\n:::\\n\\n#### Function to RunnableLambda\\n\\nInside an LCEL expression, a function is automatically converted to a `RunnableLambda`.\\n\\n```\\ndef some_func(x):\\n    return x\\n\\nchain = some_func | runnable1\\n```\\n\\nIt gets automatically converted to the following:\\n\\n```python\\nchain = RunnableSequence([RunnableLambda(some_func), runnable1])\\n```\\n\\n:::caution\\nYou have to be careful because the lambda function is not a `RunnableLambda` object, it is just a function. This means that the following code will raise an `AttributeError`:\\n\\n```python\\nlambda x: x + 1.invoke(some_input)\\n```\\n:::\\n\\n## Legacy chains\\n\\nLCEL aims to provide consistency around behavior and customization over legacy subclassed chains such as `LLMChain` and\\n`ConversationalRetrievalChain`. Many of these legacy chains hide important details like prompts, and as a wider variety\\nof viable models emerge, customization has become more and more important.\\n\\nIf you are currently using one of these legacy chains, please see [this guide for guidance on how to migrate](/docs/versions/migrating_chains).\\n\\nFor guides on how to do specific tasks with LCEL, check out [the relevant how-to guides](/docs/how_to/#langchain-expression-language-lcel).\\n'), Document(metadata={'file_name': 'introduction.mdx', 'file_path': 'docs\\\\docs\\\\introduction.mdx', 'file_type': '.mdx', 'source': 'docs\\\\docs\\\\introduction.mdx'}, page_content='---\\nsidebar_position: 0\\nsidebar_class_name: hidden\\n---\\n\\n# Introduction\\n\\n**LangChain** is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:\\n- **Development**: Build your applications using LangChain\\'s open-source [components](/docs/concepts) and [third-party integrations](/docs/integrations/providers/).\\nUse [LangGraph](/docs/concepts/architecture/#langgraph) to build stateful agents with first-class streaming and human-in-the-loop support.\\n- **Productionization**: Use [LangSmith](https://docs.smith.langchain.com/) to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\n- **Deployment**: Turn your LangGraph applications into production-ready APIs and Assistants with [LangGraph Platform](https://langchain-ai.github.io/langgraph/cloud/).\\n\\nimport ThemedImage from \\'@theme/ThemedImage\\';\\nimport useBaseUrl from \\'@docusaurus/useBaseUrl\\';\\n\\n<ThemedImage\\n  alt=\"Diagram outlining the hierarchical organization of the LangChain framework, displaying the interconnected parts across multiple layers.\"\\n  sources={{\\n    light: useBaseUrl(\\'/svg/langchain_stack_112024.svg\\'),\\n    dark: useBaseUrl(\\'/svg/langchain_stack_112024_dark.svg\\'),\\n  }}\\n  style={{ width: \"100%\" }}\\n  title=\"LangChain Framework Overview\"\\n/>\\n\\nLangChain implements a standard interface for large language models and related\\ntechnologies, such as embedding models and vector stores, and integrates with\\nhundreds of providers. See the [integrations](/docs/integrations/providers/) page for\\nmore.\\n\\nimport ChatModelTabs from \"@theme/ChatModelTabs\";\\n\\n<ChatModelTabs/>\\n\\n```python\\nmodel.invoke(\"Hello, world!\")\\n```\\n\\n:::note\\n\\nThese docs focus on the Python LangChain library. [Head here](https://js.langchain.com) for docs on the JavaScript LangChain library.\\n\\n:::\\n\\n## Architecture\\n\\nThe LangChain framework consists of multiple open-source libraries. Read more in the\\n[Architecture](/docs/concepts/architecture/) page.\\n\\n- **`langchain-core`**: Base abstractions for chat models and other components.\\n- **Integration packages** (e.g. `langchain-openai`, `langchain-anthropic`, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\\n- **`langchain`**: Chains, agents, and retrieval strategies that make up an application\\'s cognitive architecture.\\n- **`langchain-community`**: Third-party integrations that are community maintained.\\n- **`langgraph`**: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See [LangGraph documentation](https://langchain-ai.github.io/langgraph/).\\n\\n## Guides\\n\\n### [Tutorials](/docs/tutorials)\\n\\nIf you\\'re looking to build something specific or are more of a hands-on learner, check out our [tutorials section](/docs/tutorials).\\nThis is the best place to get started.\\n\\nThese are the best ones to get started with:\\n\\n- [Build a Simple LLM Application](/docs/tutorials/llm_chain)\\n- [Build a Chatbot](/docs/tutorials/chatbot)\\n- [Build an Agent](/docs/tutorials/agents)\\n- [Introduction to LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/)\\n\\nExplore the full list of LangChain tutorials [here](/docs/tutorials), and check out other [LangGraph tutorials here](https://langchain-ai.github.io/langgraph/tutorials/). To learn more about LangGraph, check out our first LangChain Academy course, *Introduction to LangGraph*, available [here](https://academy.langchain.com/courses/intro-to-langgraph).\\n\\n\\n### [How-to guides](/docs/how_to)\\n\\n[Here](/docs/how_to) you‚Äôll find short answers to ‚ÄúHow do I‚Ä¶.?‚Äù types of questions.\\nThese how-to guides don‚Äôt cover topics in depth ‚Äì you‚Äôll find that material in the [Tutorials](/docs/tutorials) and the [API Reference](https://python.langchain.com/api_reference/).\\nHowever, these guides will help you quickly accomplish common tasks using [chat models](/docs/how_to/#chat-models),\\n[vector stores](/docs/how_to/#vector-stores), and other common LangChain components.\\n\\nCheck out [LangGraph-specific how-tos here](https://langchain-ai.github.io/langgraph/how-tos/).\\n\\n### [Conceptual guide](/docs/concepts)\\n\\nIntroductions to all the key parts of LangChain you‚Äôll need to know! [Here](/docs/concepts) you\\'ll find high level explanations of all LangChain concepts.\\n\\nFor a deeper dive into LangGraph concepts, check out [this page](https://langchain-ai.github.io/langgraph/concepts/).\\n\\n### [Integrations](integrations/providers/index.mdx)\\n\\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\\nIf you\\'re looking to get up and running quickly with [chat models](/docs/integrations/chat/), [vector stores](/docs/integrations/vectorstores/),\\nor other LangChain components from a specific provider, check out our growing list of [integrations](/docs/integrations/providers/).\\n\\n\\n### [API reference](https://python.langchain.com/api_reference/)\\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\\n\\n## Ecosystem\\n\\n### [ü¶úüõ†Ô∏è LangSmith](https://docs.smith.langchain.com)\\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\\n\\n### [ü¶úüï∏Ô∏è LangGraph](https://langchain-ai.github.io/langgraph)\\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by Linkedin, Uber, Klarna, GitLab, and many more.\\n\\n## Additional resources\\n\\n### [Versions](/docs/versions/v0_3/)\\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\\n\\n### [Security](/docs/security)\\nRead up on [security](/docs/security) best practices to make sure you\\'re developing safely with LangChain.\\n\\n### [Contributing](contributing/index.mdx)\\nCheck out the developer\\'s guide for guidelines on contributing and help getting your dev environment set up.\\n'), Document(metadata={'file_name': 'index.mdx', 'file_path': 'docs\\\\docs\\\\how_to\\\\index.mdx', 'file_type': '.mdx', 'source': 'docs\\\\docs\\\\how_to\\\\index.mdx'}, page_content='---\\nsidebar_position: 0\\nsidebar_class_name: hidden\\n---\\n\\n# How-to guides\\n\\nHere you‚Äôll find answers to ‚ÄúHow do I‚Ä¶.?‚Äù types of questions.\\nThese guides are *goal-oriented* and *concrete*; they\\'re meant to help you complete a specific task.\\nFor conceptual explanations see the [Conceptual guide](/docs/concepts/).\\nFor end-to-end walkthroughs see [Tutorials](/docs/tutorials).\\nFor comprehensive descriptions of every class and function see the [API Reference](https://python.langchain.com/api_reference/).\\n\\n## Installation\\n\\n- [How to: install LangChain packages](/docs/how_to/installation/)\\n- [How to: use LangChain with different Pydantic versions](/docs/how_to/pydantic_compatibility)\\n\\n## Key features\\n\\nThis highlights functionality that is core to using LangChain.\\n\\n- [How to: return structured data from a model](/docs/how_to/structured_output/)\\n- [How to: use a model to call tools](/docs/how_to/tool_calling)\\n- [How to: stream runnables](/docs/how_to/streaming)\\n- [How to: debug your LLM apps](/docs/how_to/debugging/)\\n\\n## Components\\n\\nThese are the core building blocks you can use when building applications.\\n\\n### Chat models\\n\\n[Chat Models](/docs/concepts/chat_models) are newer forms of language models that take messages in and output a message.\\nSee [supported integrations](/docs/integrations/chat/) for details on getting started with chat models from a specific provider.\\n\\n- [How to: do function/tool calling](/docs/how_to/tool_calling)\\n- [How to: get models to return structured output](/docs/how_to/structured_output)\\n- [How to: cache model responses](/docs/how_to/chat_model_caching)\\n- [How to: get log probabilities](/docs/how_to/logprobs)\\n- [How to: create a custom chat model class](/docs/how_to/custom_chat_model)\\n- [How to: stream a response back](/docs/how_to/chat_streaming)\\n- [How to: track token usage](/docs/how_to/chat_token_usage_tracking)\\n- [How to: track response metadata across providers](/docs/how_to/response_metadata)\\n- [How to: use chat model to call tools](/docs/how_to/tool_calling)\\n- [How to: stream tool calls](/docs/how_to/tool_streaming)\\n- [How to: handle rate limits](/docs/how_to/chat_model_rate_limiting)\\n- [How to: few shot prompt tool behavior](/docs/how_to/tools_few_shot)\\n- [How to: bind model-specific formatted tools](/docs/how_to/tools_model_specific)\\n- [How to: force a specific tool call](/docs/how_to/tool_choice)\\n- [How to: work with local models](/docs/how_to/local_llms)\\n- [How to: init any model in one line](/docs/how_to/chat_models_universal_init/)\\n- [How to: pass multimodal data directly to models](/docs/how_to/multimodal_inputs/)\\n\\n### Messages\\n\\n[Messages](/docs/concepts/messages) are the input and output of chat models. They have some `content` and a `role`, which describes the source of the message.\\n\\n- [How to: trim messages](/docs/how_to/trim_messages/)\\n- [How to: filter messages](/docs/how_to/filter_messages/)\\n- [How to: merge consecutive messages of the same type](/docs/how_to/merge_message_runs/)\\n\\n### Prompt templates\\n\\n[Prompt Templates](/docs/concepts/prompt_templates) are responsible for formatting user input into a format that can be passed to a language model.\\n\\n- [How to: use few shot examples](/docs/how_to/few_shot_examples)\\n- [How to: use few shot examples in chat models](/docs/how_to/few_shot_examples_chat/)\\n- [How to: partially format prompt templates](/docs/how_to/prompts_partial)\\n- [How to: compose prompts together](/docs/how_to/prompts_composition)\\n- [How to: use multimodal prompts](/docs/how_to/multimodal_prompts/)\\n\\n### Example selectors\\n\\n[Example Selectors](/docs/concepts/example_selectors) are responsible for selecting the correct few shot examples to pass to the prompt.\\n\\n- [How to: use example selectors](/docs/how_to/example_selectors)\\n- [How to: select examples by length](/docs/how_to/example_selectors_length_based)\\n- [How to: select examples by semantic similarity](/docs/how_to/example_selectors_similarity)\\n- [How to: select examples by semantic ngram overlap](/docs/how_to/example_selectors_ngram)\\n- [How to: select examples by maximal marginal relevance](/docs/how_to/example_selectors_mmr)\\n- [How to: select examples from LangSmith few-shot datasets](/docs/how_to/example_selectors_langsmith/)\\n\\n### LLMs\\n\\nWhat LangChain calls [LLMs](/docs/concepts/text_llms) are older forms of language models that take a string in and output a string.\\n\\n- [How to: cache model responses](/docs/how_to/llm_caching)\\n- [How to: create a custom LLM class](/docs/how_to/custom_llm)\\n- [How to: stream a response back](/docs/how_to/streaming_llm)\\n- [How to: track token usage](/docs/how_to/llm_token_usage_tracking)\\n- [How to: work with local models](/docs/how_to/local_llms)\\n\\n### Output parsers\\n\\n[Output Parsers](/docs/concepts/output_parsers) are responsible for taking the output of an LLM and parsing into more structured format.\\n\\n- [How to: parse text from message objects](/docs/how_to/output_parser_string)\\n- [How to: use output parsers to parse an LLM response into structured format](/docs/how_to/output_parser_structured)\\n- [How to: parse JSON output](/docs/how_to/output_parser_json)\\n- [How to: parse XML output](/docs/how_to/output_parser_xml)\\n- [How to: parse YAML output](/docs/how_to/output_parser_yaml)\\n- [How to: retry when output parsing errors occur](/docs/how_to/output_parser_retry)\\n- [How to: try to fix errors in output parsing](/docs/how_to/output_parser_fixing)\\n- [How to: write a custom output parser class](/docs/how_to/output_parser_custom)\\n\\n### Document loaders\\n\\n[Document Loaders](/docs/concepts/document_loaders) are responsible for loading documents from a variety of sources.\\n\\n- [How to: load PDF files](/docs/how_to/document_loader_pdf)\\n- [How to: load web pages](/docs/how_to/document_loader_web)\\n- [How to: load CSV data](/docs/how_to/document_loader_csv)\\n- [How to: load data from a directory](/docs/how_to/document_loader_directory)\\n- [How to: load HTML data](/docs/how_to/document_loader_html)\\n- [How to: load JSON data](/docs/how_to/document_loader_json)\\n- [How to: load Markdown data](/docs/how_to/document_loader_markdown)\\n- [How to: load Microsoft Office data](/docs/how_to/document_loader_office_file)\\n- [How to: write a custom document loader](/docs/how_to/document_loader_custom)\\n\\n### Text splitters\\n\\n[Text Splitters](/docs/concepts/text_splitters) take a document and split into chunks that can be used for retrieval.\\n\\n- [How to: recursively split text](/docs/how_to/recursive_text_splitter)\\n- [How to: split HTML](/docs/how_to/split_html)\\n- [How to: split by character](/docs/how_to/character_text_splitter)\\n- [How to: split code](/docs/how_to/code_splitter)\\n- [How to: split Markdown by headers](/docs/how_to/markdown_header_metadata_splitter)\\n- [How to: recursively split JSON](/docs/how_to/recursive_json_splitter)\\n- [How to: split text into semantic chunks](/docs/how_to/semantic-chunker)\\n- [How to: split by tokens](/docs/how_to/split_by_token)\\n\\n### Embedding models\\n\\n[Embedding Models](/docs/concepts/embedding_models) take a piece of text and create a numerical representation of it.\\nSee [supported integrations](/docs/integrations/text_embedding/) for details on getting started with embedding models from a specific provider.\\n\\n- [How to: embed text data](/docs/how_to/embed_text)\\n- [How to: cache embedding results](/docs/how_to/caching_embeddings)\\n- [How to: create a custom embeddings class](/docs/how_to/custom_embeddings)\\n\\n### Vector stores\\n\\n[Vector stores](/docs/concepts/vectorstores) are databases that can efficiently store and retrieve embeddings.\\nSee [supported integrations](/docs/integrations/vectorstores/) for details on getting started with vector stores from a specific provider.\\n\\n- [How to: use a vector store to retrieve data](/docs/how_to/vectorstores)\\n\\n### Retrievers\\n\\n[Retrievers](/docs/concepts/retrievers) are responsible for taking a query and returning relevant documents.\\n\\n- [How to: use a vector store to retrieve data](/docs/how_to/vectorstore_retriever)\\n- [How to: generate multiple queries to retrieve data for](/docs/how_to/MultiQueryRetriever)\\n- [How to: use contextual compression to compress the data retrieved](/docs/how_to/contextual_compression)\\n- [How to: write a custom retriever class](/docs/how_to/custom_retriever)\\n- [How to: add similarity scores to retriever results](/docs/how_to/add_scores_retriever)\\n- [How to: combine the results from multiple retrievers](/docs/how_to/ensemble_retriever)\\n- [How to: reorder retrieved results to mitigate the \"lost in the middle\" effect](/docs/how_to/long_context_reorder)\\n- [How to: generate multiple embeddings per document](/docs/how_to/multi_vector)\\n- [How to: retrieve the whole document for a chunk](/docs/how_to/parent_document_retriever)\\n- [How to: generate metadata filters](/docs/how_to/self_query)\\n- [How to: create a time-weighted retriever](/docs/how_to/time_weighted_vectorstore)\\n- [How to: use hybrid vector and keyword retrieval](/docs/how_to/hybrid)\\n\\n### Indexing\\n\\nIndexing is the process of keeping your vectorstore in-sync with the underlying data source.\\n\\n- [How to: reindex data to keep your vectorstore in-sync with the underlying data source](/docs/how_to/indexing)\\n\\n### Tools\\n\\nLangChain [Tools](/docs/concepts/tools) contain a description of the tool (to pass to the language model) as well as the implementation of the function to call. Refer [here](/docs/integrations/tools/) for a list of pre-built tools. \\n\\n- [How to: create tools](/docs/how_to/custom_tools)\\n- [How to: use built-in tools and toolkits](/docs/how_to/tools_builtin)\\n- [How to: use chat models to call tools](/docs/how_to/tool_calling)\\n- [How to: pass tool outputs to chat models](/docs/how_to/tool_results_pass_to_model)\\n- [How to: pass run time values to tools](/docs/how_to/tool_runtime)\\n- [How to: add a human-in-the-loop for tools](/docs/how_to/tools_human)\\n- [How to: handle tool errors](/docs/how_to/tools_error)\\n- [How to: force models to call a tool](/docs/how_to/tool_choice)\\n- [How to: disable parallel tool calling](/docs/how_to/tool_calling_parallel)\\n- [How to: access the `RunnableConfig` from a tool](/docs/how_to/tool_configure)\\n- [How to: stream events from a tool](/docs/how_to/tool_stream_events)\\n- [How to: return artifacts from a tool](/docs/how_to/tool_artifacts/)\\n- [How to: convert Runnables to tools](/docs/how_to/convert_runnable_to_tool)\\n- [How to: add ad-hoc tool calling capability to models](/docs/how_to/tools_prompting)\\n- [How to: pass in runtime secrets](/docs/how_to/runnable_runtime_secrets)\\n\\n### Multimodal\\n\\n- [How to: pass multimodal data directly to models](/docs/how_to/multimodal_inputs/)\\n- [How to: use multimodal prompts](/docs/how_to/multimodal_prompts/)\\n\\n\\n### Agents\\n\\n:::note\\n\\nFor in depth how-to guides for agents, please check out [LangGraph](https://langchain-ai.github.io/langgraph/) documentation.\\n\\n:::\\n\\n- [How to: use legacy LangChain Agents (AgentExecutor)](/docs/how_to/agent_executor)\\n- [How to: migrate from legacy LangChain agents to LangGraph](/docs/how_to/migrate_agent)\\n\\n### Callbacks\\n\\n[Callbacks](/docs/concepts/callbacks) allow you to hook into the various stages of your LLM application\\'s execution.\\n\\n- [How to: pass in callbacks at runtime](/docs/how_to/callbacks_runtime)\\n- [How to: attach callbacks to a module](/docs/how_to/callbacks_attach)\\n- [How to: pass callbacks into a module constructor](/docs/how_to/callbacks_constructor)\\n- [How to: create custom callback handlers](/docs/how_to/custom_callbacks)\\n- [How to: use callbacks in async environments](/docs/how_to/callbacks_async)\\n- [How to: dispatch custom callback events](/docs/how_to/callbacks_custom_events)\\n\\n### Custom\\n\\nAll of LangChain components can easily be extended to support your own versions.\\n\\n- [How to: create a custom chat model class](/docs/how_to/custom_chat_model)\\n- [How to: create a custom LLM class](/docs/how_to/custom_llm)\\n- [How to: create a custom embeddings class](/docs/how_to/custom_embeddings)\\n- [How to: write a custom retriever class](/docs/how_to/custom_retriever)\\n- [How to: write a custom document loader](/docs/how_to/document_loader_custom)\\n- [How to: write a custom output parser class](/docs/how_to/output_parser_custom)\\n- [How to: create custom callback handlers](/docs/how_to/custom_callbacks)\\n- [How to: define a custom tool](/docs/how_to/custom_tools)\\n- [How to: dispatch custom callback events](/docs/how_to/callbacks_custom_events)\\n\\n### Serialization\\n- [How to: save and load LangChain objects](/docs/how_to/serialization)\\n\\n## Use cases\\n\\nThese guides cover use-case specific details.\\n\\n### Q&A with RAG\\n\\nRetrieval Augmented Generation (RAG) is a way to connect LLMs to external sources of data.\\nFor a high-level tutorial on RAG, check out [this guide](/docs/tutorials/rag/).\\n\\n- [How to: add chat history](/docs/how_to/qa_chat_history_how_to/)\\n- [How to: stream](/docs/how_to/qa_streaming/)\\n- [How to: return sources](/docs/how_to/qa_sources/)\\n- [How to: return citations](/docs/how_to/qa_citations/)\\n- [How to: do per-user retrieval](/docs/how_to/qa_per_user/)\\n\\n\\n### Extraction\\n\\nExtraction is when you use LLMs to extract structured information from unstructured text.\\nFor a high level tutorial on extraction, check out [this guide](/docs/tutorials/extraction/).\\n\\n- [How to: use reference examples](/docs/how_to/extraction_examples/)\\n- [How to: handle long text](/docs/how_to/extraction_long_text/)\\n- [How to: do extraction without using function calling](/docs/how_to/extraction_parse)\\n\\n### Chatbots\\n\\nChatbots involve using an LLM to have a conversation.\\nFor a high-level tutorial on building chatbots, check out [this guide](/docs/tutorials/chatbot/).\\n\\n- [How to: manage memory](/docs/how_to/chatbots_memory)\\n- [How to: do retrieval](/docs/how_to/chatbots_retrieval)\\n- [How to: use tools](/docs/how_to/chatbots_tools)\\n- [How to: manage large chat history](/docs/how_to/trim_messages/)\\n\\n### Query analysis\\n\\nQuery Analysis is the task of using an LLM to generate a query to send to a retriever.\\nFor a high-level tutorial on query analysis, check out [this guide](/docs/tutorials/rag/#query-analysis).\\n\\n- [How to: add examples to the prompt](/docs/how_to/query_few_shot)\\n- [How to: handle cases where no queries are generated](/docs/how_to/query_no_queries)\\n- [How to: handle multiple queries](/docs/how_to/query_multiple_queries)\\n- [How to: handle multiple retrievers](/docs/how_to/query_multiple_retrievers)\\n- [How to: construct filters](/docs/how_to/query_constructing_filters)\\n- [How to: deal with high cardinality categorical variables](/docs/how_to/query_high_cardinality)\\n\\n### Q&A over SQL + CSV\\n\\nYou can use LLMs to do question answering over tabular data.\\nFor a high-level tutorial, check out [this guide](/docs/tutorials/sql_qa/).\\n\\n- [How to: use prompting to improve results](/docs/how_to/sql_prompting)\\n- [How to: do query validation](/docs/how_to/sql_query_checking)\\n- [How to: deal with large databases](/docs/how_to/sql_large_db)\\n- [How to: deal with CSV files](/docs/how_to/sql_csv)\\n\\n### Q&A over graph databases\\n\\nYou can use an LLM to do question answering over graph databases.\\nFor a high-level tutorial, check out [this guide](/docs/tutorials/graph/).\\n\\n- [How to: add a semantic layer over the database](/docs/how_to/graph_semantic)\\n- [How to: construct knowledge graphs](/docs/how_to/graph_constructing)\\n\\n### Summarization\\n\\nLLMs can summarize and otherwise distill desired information from text, including\\nlarge volumes of text. For a high-level tutorial, check out [this guide](/docs/tutorials/summarization).\\n\\n- [How to: summarize text in a single LLM call](/docs/how_to/summarize_stuff)\\n- [How to: summarize text through parallelization](/docs/how_to/summarize_map_reduce)\\n- [How to: summarize text through iterative refinement](/docs/how_to/summarize_refine)\\n\\n## LangChain Expression Language (LCEL)\\n\\n:::note Should I use LCEL?\\n\\nLCEL is an orchestration solution. See our\\n[concepts page](/docs/concepts/lcel/#should-i-use-lcel) for recommendations on when to\\nuse LCEL.\\n\\n:::\\n\\n[LangChain Expression Language](/docs/concepts/lcel) is a way to create arbitrary custom chains. It is built on the [Runnable](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html) protocol.\\n\\n[**LCEL cheatsheet**](/docs/how_to/lcel_cheatsheet/): For a quick overview of how to use the main LCEL primitives.\\n\\n[**Migration guide**](/docs/versions/migrating_chains): For migrating legacy chain abstractions to LCEL.\\n\\n- [How to: chain runnables](/docs/how_to/sequence)\\n- [How to: stream runnables](/docs/how_to/streaming)\\n- [How to: invoke runnables in parallel](/docs/how_to/parallel/)\\n- [How to: add default invocation args to runnables](/docs/how_to/binding/)\\n- [How to: turn any function into a runnable](/docs/how_to/functions)\\n- [How to: pass through inputs from one chain step to the next](/docs/how_to/passthrough)\\n- [How to: configure runnable behavior at runtime](/docs/how_to/configure)\\n- [How to: add message history (memory) to a chain](/docs/how_to/message_history)\\n- [How to: route between sub-chains](/docs/how_to/routing)\\n- [How to: create a dynamic (self-constructing) chain](/docs/how_to/dynamic_chain/)\\n- [How to: inspect runnables](/docs/how_to/inspect)\\n- [How to: add fallbacks to a runnable](/docs/how_to/fallbacks)\\n- [How to: pass runtime secrets to a runnable](/docs/how_to/runnable_runtime_secrets)\\n\\n## [LangGraph](https://langchain-ai.github.io/langgraph)\\n\\nLangGraph is an extension of LangChain aimed at\\nbuilding robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.\\n\\nLangGraph documentation is currently hosted on a separate site.\\nYou can peruse [LangGraph how-to guides here](https://langchain-ai.github.io/langgraph/how-tos/).\\n\\n## [LangSmith](https://docs.smith.langchain.com/)\\n\\nLangSmith allows you to closely trace, monitor and evaluate your LLM application.\\nIt seamlessly integrates with LangChain and LangGraph, and you can use it to inspect and debug individual steps of your chains and agents as you build.\\n\\nLangSmith documentation is hosted on a separate site.\\nYou can peruse [LangSmith how-to guides here](https://docs.smith.langchain.com/), but we\\'ll highlight a few sections that are particularly\\nrelevant to LangChain below:\\n\\n### Evaluation\\n<span data-heading-keywords=\"evaluation,evaluate\"></span>\\n\\nEvaluating performance is a vital part of building LLM-powered applications.\\nLangSmith helps with every step of the process from creating a dataset to defining metrics to running evaluators.\\n\\nTo learn more, check out the [LangSmith evaluation how-to guides](https://docs.smith.langchain.com/how_to_guides#evaluation).\\n\\n### Tracing\\n<span data-heading-keywords=\"trace,tracing\"></span>\\n\\nTracing gives you observability inside your chains and agents, and is vital in diagnosing issues.\\n\\n- [How to: trace with LangChain](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain)\\n- [How to: add metadata and tags to traces](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#add-metadata-and-tags-to-traces)\\n\\nYou can see general tracing-related how-tos [in this section of the LangSmith docs](https://docs.smith.langchain.com/how_to_guides/tracing).\\n')]\n",
      "rank:0\n",
      "rank:1\n",
      "rank:2\n",
      "rank:3\n",
      "docs:[Document(metadata={'file_name': 'why_langchain.mdx', 'file_path': 'docs\\\\docs\\\\concepts\\\\why_langchain.mdx', 'file_type': '.mdx', 'source': 'docs\\\\docs\\\\concepts\\\\why_langchain.mdx'}, page_content='# Why LangChain?\\n\\nThe goal of `langchain` the Python package and LangChain the company is to make it as easy as possible for developers to build applications that reason.\\nWhile LangChain originally started as a single open source package, it has evolved into a company and a whole ecosystem.\\nThis page will talk about the LangChain ecosystem as a whole.\\nMost of the components within the LangChain ecosystem can be used by themselves - so if you feel particularly drawn to certain components but not others, that is totally fine! Pick and choose whichever components you like best for your own use case!\\n\\n## Features\\n\\nThere are several primary needs that LangChain aims to address:\\n\\n1. **Standardized component interfaces:** The growing number of [models](/docs/integrations/chat/) and [related components](/docs/integrations/vectorstores/) for AI applications has resulted in a wide variety of different APIs that developers need to learn and use.\\nThis diversity can make it challenging for developers to switch between providers or combine components when building applications.\\nLangChain exposes a standard interface for key components, making it easy to switch between providers.\\n\\n2. **Orchestration:** As applications become more complex, combining multiple components and models, there\\'s [a growing need to efficiently connect these elements into control flows](https://lilianweng.github.io/posts/2023-06-23-agent/) that can [accomplish diverse tasks](https://www.sequoiacap.com/article/generative-ais-act-o1/).\\n[Orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)) is crucial for building such applications.\\n\\n3. **Observability and evaluation:** As applications become more complex, it becomes increasingly difficult to understand what is happening within them.\\nFurthermore, the pace of development can become rate-limited by the [paradox of choice](https://en.wikipedia.org/wiki/Paradox_of_choice).\\nFor example, developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\n[Observability](https://en.wikipedia.org/wiki/Observability) and evaluations can help developers monitor their applications and rapidly answer these types of questions with confidence.\\n\\n\\n## Standardized component interfaces\\n\\nLangChain provides common interfaces for components that are central to many AI applications.\\nAs an example, all [chat models](/docs/concepts/chat_models/) implement the [BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) interface.\\nThis provides a standard way to interact with chat models, supporting important but often provider-specific features like [tool calling](/docs/concepts/tool_calling/) and [structured outputs](/docs/concepts/structured_outputs/).\\n\\n\\n### Example: chat models \\n\\nMany [model providers](/docs/concepts/chat_models/) support [tool calling](/docs/concepts/tool_calling/), a critical feature for many applications (e.g., [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/)), that allows a developer to request model responses that match a particular schema.\\nThe APIs for each provider differ. \\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to bind [tools](/docs/concepts/tools) to a model in order to support [tool calling](/docs/concepts/tool_calling/):\\n\\n```python\\n# Tool creation\\ntools = [my_tool]\\n# Tool binding\\nmodel_with_tools = model.bind_tools(tools)\\n```\\n\\nSimilarly, getting models to produce [structured outputs](/docs/concepts/structured_outputs/) is an extremely common use case. \\nProviders support different approaches for this, including [JSON mode or tool calling](https://platform.openai.com/docs/guides/structured-outputs), with different APIs.\\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to produce structured outputs using the `with_structured_output()` method:\\n\\n```python\\n# Define schema\\nschema = ...\\n# Bind schema to model\\nmodel_with_structure = model.with_structured_output(schema)\\n```\\n\\n### Example: retrievers\\n\\nIn the context of [RAG](/docs/concepts/rag/) and LLM application components, LangChain\\'s [retriever](/docs/concepts/retrievers/) interface provides a standard way to connect to many different types of data services or databases (e.g., [vector stores](/docs/concepts/vectorstores) or databases).\\nThe underlying implementation of the retriever depends on the type of data store or database you are connecting to, but all retrievers implement the [runnable interface](/docs/concepts/runnables/), meaning they can be invoked in a common manner.\\n\\n```python\\ndocuments = my_retriever.invoke(\"What is the meaning of life?\")\\n```\\n\\n## Orchestration \\n\\nWhile standardization for individual components is useful, we\\'ve increasingly seen that developers want to *combine* components into more complex applications. \\nThis motivates the need for [orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)).\\nThere are several common characteristics of LLM applications that this orchestration layer should support:\\n\\n* **Complex control flow:** The application requires complex patterns such as cycles (e.g., a loop that reiterates until a condition is met).\\n* **[Persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/):** The application needs to maintain [short-term and / or long-term memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n* **[Human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/):** The application needs human interaction, e.g., pausing, reviewing, editing, approving certain steps.\\n\\nThe recommended way to orchestrate components for complex applications is [LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/).\\nLangGraph is a library that gives developers a high degree of control by expressing the flow of the application as a set of nodes and edges.\\nLangGraph comes with built-in support for [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), [memory](https://langchain-ai.github.io/langgraph/concepts/memory/), and other features.\\nIt\\'s particularly well suited for building [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/) or [multi-agent](https://langchain-ai.github.io/langgraph/concepts/multi_agent/) applications. \\nImportantly, individual LangChain components can be used as LangGraph nodes, but you can also use LangGraph **without** using LangChain components.\\n\\n:::info[Further reading]\\n\\nHave a look at our free course, [Introduction to LangGraph](https://academy.langchain.com/courses/intro-to-langgraph), to learn more about how to use LangGraph to build complex applications.\\n\\n:::\\n\\n## Observability and evaluation\\n\\nThe pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. \\nDevelopers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\nHigh quality tracing and evaluations can help you rapidly answer these types of questions with confidence.\\n[LangSmith](https://docs.smith.langchain.com/) is our platform that supports observability and evaluation for AI applications.\\nSee our conceptual guides on [evaluations](https://docs.smith.langchain.com/concepts/evaluation) and [tracing](https://docs.smith.langchain.com/concepts/tracing) for more details.\\n\\n:::info[Further reading]\\n\\nSee our video playlist on [LangSmith tracing and evaluations](https://youtube.com/playlist?list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S&feature=shared) for more details.\\n\\n:::\\n\\n## Conclusion\\n\\nLangChain offers standard interfaces for components that are central to many AI applications, which offers a few specific advantages:\\n- **Ease of swapping providers:** It allows you to swap out different component providers without having to change the underlying code.\\n- **Advanced features:** It provides common methods for more advanced features, such as [streaming](/docs/concepts/streaming) and [tool calling](/docs/concepts/tool_calling/).\\n\\n[LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/) makes it possible to orchestrate complex applications (e.g., [agents](/docs/concepts/agents/)) and provide features like including [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), or [memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n\\n[LangSmith](https://docs.smith.langchain.com/) makes it possible to iterate with confidence on your applications, by providing LLM-specific observability and framework for testing and evaluating your application.\\n'), Document(metadata={'file_name': 'introduction.mdx', 'file_path': 'docs\\\\docs\\\\introduction.mdx', 'file_type': '.mdx', 'source': 'docs\\\\docs\\\\introduction.mdx'}, page_content='---\\nsidebar_position: 0\\nsidebar_class_name: hidden\\n---\\n\\n# Introduction\\n\\n**LangChain** is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:\\n- **Development**: Build your applications using LangChain\\'s open-source [components](/docs/concepts) and [third-party integrations](/docs/integrations/providers/).\\nUse [LangGraph](/docs/concepts/architecture/#langgraph) to build stateful agents with first-class streaming and human-in-the-loop support.\\n- **Productionization**: Use [LangSmith](https://docs.smith.langchain.com/) to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\n- **Deployment**: Turn your LangGraph applications into production-ready APIs and Assistants with [LangGraph Platform](https://langchain-ai.github.io/langgraph/cloud/).\\n\\nimport ThemedImage from \\'@theme/ThemedImage\\';\\nimport useBaseUrl from \\'@docusaurus/useBaseUrl\\';\\n\\n<ThemedImage\\n  alt=\"Diagram outlining the hierarchical organization of the LangChain framework, displaying the interconnected parts across multiple layers.\"\\n  sources={{\\n    light: useBaseUrl(\\'/svg/langchain_stack_112024.svg\\'),\\n    dark: useBaseUrl(\\'/svg/langchain_stack_112024_dark.svg\\'),\\n  }}\\n  style={{ width: \"100%\" }}\\n  title=\"LangChain Framework Overview\"\\n/>\\n\\nLangChain implements a standard interface for large language models and related\\ntechnologies, such as embedding models and vector stores, and integrates with\\nhundreds of providers. See the [integrations](/docs/integrations/providers/) page for\\nmore.\\n\\nimport ChatModelTabs from \"@theme/ChatModelTabs\";\\n\\n<ChatModelTabs/>\\n\\n```python\\nmodel.invoke(\"Hello, world!\")\\n```\\n\\n:::note\\n\\nThese docs focus on the Python LangChain library. [Head here](https://js.langchain.com) for docs on the JavaScript LangChain library.\\n\\n:::\\n\\n## Architecture\\n\\nThe LangChain framework consists of multiple open-source libraries. Read more in the\\n[Architecture](/docs/concepts/architecture/) page.\\n\\n- **`langchain-core`**: Base abstractions for chat models and other components.\\n- **Integration packages** (e.g. `langchain-openai`, `langchain-anthropic`, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\\n- **`langchain`**: Chains, agents, and retrieval strategies that make up an application\\'s cognitive architecture.\\n- **`langchain-community`**: Third-party integrations that are community maintained.\\n- **`langgraph`**: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See [LangGraph documentation](https://langchain-ai.github.io/langgraph/).\\n\\n## Guides\\n\\n### [Tutorials](/docs/tutorials)\\n\\nIf you\\'re looking to build something specific or are more of a hands-on learner, check out our [tutorials section](/docs/tutorials).\\nThis is the best place to get started.\\n\\nThese are the best ones to get started with:\\n\\n- [Build a Simple LLM Application](/docs/tutorials/llm_chain)\\n- [Build a Chatbot](/docs/tutorials/chatbot)\\n- [Build an Agent](/docs/tutorials/agents)\\n- [Introduction to LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/)\\n\\nExplore the full list of LangChain tutorials [here](/docs/tutorials), and check out other [LangGraph tutorials here](https://langchain-ai.github.io/langgraph/tutorials/). To learn more about LangGraph, check out our first LangChain Academy course, *Introduction to LangGraph*, available [here](https://academy.langchain.com/courses/intro-to-langgraph).\\n\\n\\n### [How-to guides](/docs/how_to)\\n\\n[Here](/docs/how_to) you‚Äôll find short answers to ‚ÄúHow do I‚Ä¶.?‚Äù types of questions.\\nThese how-to guides don‚Äôt cover topics in depth ‚Äì you‚Äôll find that material in the [Tutorials](/docs/tutorials) and the [API Reference](https://python.langchain.com/api_reference/).\\nHowever, these guides will help you quickly accomplish common tasks using [chat models](/docs/how_to/#chat-models),\\n[vector stores](/docs/how_to/#vector-stores), and other common LangChain components.\\n\\nCheck out [LangGraph-specific how-tos here](https://langchain-ai.github.io/langgraph/how-tos/).\\n\\n### [Conceptual guide](/docs/concepts)\\n\\nIntroductions to all the key parts of LangChain you‚Äôll need to know! [Here](/docs/concepts) you\\'ll find high level explanations of all LangChain concepts.\\n\\nFor a deeper dive into LangGraph concepts, check out [this page](https://langchain-ai.github.io/langgraph/concepts/).\\n\\n### [Integrations](integrations/providers/index.mdx)\\n\\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\\nIf you\\'re looking to get up and running quickly with [chat models](/docs/integrations/chat/), [vector stores](/docs/integrations/vectorstores/),\\nor other LangChain components from a specific provider, check out our growing list of [integrations](/docs/integrations/providers/).\\n\\n\\n### [API reference](https://python.langchain.com/api_reference/)\\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\\n\\n## Ecosystem\\n\\n### [ü¶úüõ†Ô∏è LangSmith](https://docs.smith.langchain.com)\\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\\n\\n### [ü¶úüï∏Ô∏è LangGraph](https://langchain-ai.github.io/langgraph)\\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by Linkedin, Uber, Klarna, GitLab, and many more.\\n\\n## Additional resources\\n\\n### [Versions](/docs/versions/v0_3/)\\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\\n\\n### [Security](/docs/security)\\nRead up on [security](/docs/security) best practices to make sure you\\'re developing safely with LangChain.\\n\\n### [Contributing](contributing/index.mdx)\\nCheck out the developer\\'s guide for guidelines on contributing and help getting your dev environment set up.\\n'), Document(metadata={'file_name': 'lcel.mdx', 'file_path': 'docs\\\\docs\\\\concepts\\\\lcel.mdx', 'file_type': '.mdx', 'source': 'docs\\\\docs\\\\concepts\\\\lcel.mdx'}, page_content='# LangChain Expression Language (LCEL)\\n\\n:::info Prerequisites\\n* [Runnable Interface](/docs/concepts/runnables)\\n:::\\n\\nThe **L**ang**C**hain **E**xpression **L**anguage (LCEL) takes a [declarative](https://en.wikipedia.org/wiki/Declarative_programming) approach to building new [Runnables](/docs/concepts/runnables) from existing Runnables.\\n\\nThis means that you describe what *should* happen, rather than *how* it should happen, allowing LangChain to optimize the run-time execution of the chains.\\n\\nWe often refer to a `Runnable` created using LCEL as a \"chain\". It\\'s important to remember that a \"chain\" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).\\n\\n:::note\\n* The [LCEL cheatsheet](/docs/how_to/lcel_cheatsheet/) shows common patterns that involve the Runnable interface and LCEL expressions.\\n* Please see the following list of [how-to guides](/docs/how_to/#langchain-expression-language-lcel) that cover common tasks with LCEL.\\n* A list of built-in `Runnables` can be found in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html). Many of these Runnables are useful when composing custom \"chains\" in LangChain using LCEL.\\n:::\\n\\n## Benefits of LCEL\\n\\nLangChain optimizes the run-time execution of chains built with LCEL in a number of ways:\\n\\n- **Optimized parallel execution**: Run Runnables in parallel using [RunnableParallel](#runnableparallel) or run multiple inputs through a given chain in parallel using the [Runnable Batch API](/docs/concepts/runnables/#optimized-parallel-execution-batch). Parallel execution can significantly reduce the latency as processing can be done in parallel instead of sequentially.\\n- **Guaranteed Async support**: Any chain built with LCEL can be run asynchronously using the [Runnable Async API](/docs/concepts/runnables/#asynchronous-support). This can be useful when running chains in a server environment where you want to handle large number of requests concurrently.\\n- **Simplify streaming**: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(time elapsed until the first chunk of output from a [chat model](/docs/concepts/chat_models) or [llm](/docs/concepts/text_llms) comes out).\\n\\nOther benefits include:\\n\\n- [**Seamless LangSmith tracing**](https://docs.smith.langchain.com)\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, **all** steps are automatically logged to [LangSmith](https://docs.smith.langchain.com/) for maximum observability and debuggability.\\n- **Standard API**: Because all chains are built using the Runnable interface, they can be used in the same way as any other Runnable.\\n- [**Deployable with LangServe**](/docs/concepts/architecture#langserve): Chains built with LCEL can be deployed using for production use.\\n\\n## Should I use LCEL?\\n\\nLCEL is an [orchestration solution](https://en.wikipedia.org/wiki/Orchestration_(computing)) -- it allows LangChain to handle run-time execution of chains in an optimized way.\\n\\nWhile we have seen users run chains with hundreds of steps in production, we generally recommend using LCEL for simpler orchestration tasks. When the application requires complex state management, branching, cycles or multiple agents, we recommend that users take advantage of [LangGraph](/docs/concepts/architecture#langgraph).\\n\\nIn LangGraph, users define graphs that specify the application\\'s flow. This allows users to keep using LCEL within individual nodes when LCEL is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.\\n\\nHere are some guidelines:\\n\\n* If you are making a single LLM call, you don\\'t need LCEL; instead call the underlying [chat model](/docs/concepts/chat_models) directly.\\n* If you have a simple chain (e.g., prompt + llm + parser, simple retrieval set up etc.), LCEL is a reasonable fit, if you\\'re taking advantage of the LCEL benefits.\\n* If you\\'re building a complex chain (e.g., with branching, cycles, multiple agents, etc.) use [LangGraph](/docs/concepts/architecture#langgraph) instead. Remember that you can always use LCEL within individual nodes in LangGraph.\\n\\n## Composition Primitives\\n\\n`LCEL` chains are built by composing existing `Runnables` together. The two main composition primitives are [RunnableSequence](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableSequence.html#langchain_core.runnables.base.RunnableSequence) and [RunnableParallel](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableParallel.html#langchain_core.runnables.base.RunnableParallel).\\n\\nMany other composition primitives (e.g., [RunnableAssign](\\nhttps://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnableAssign.html#langchain_core.runnables.passthrough.RunnableAssign\\n)) can be thought of as variations of these two primitives.\\n\\n:::note\\nYou can find a list of all composition primitives in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html).\\n:::\\n\\n### RunnableSequence\\n\\n`RunnableSequence` is a composition primitive that allows you \"chain\" multiple runnables sequentially, with the output of one runnable serving as the input to the next.\\n\\n```python\\nfrom langchain_core.runnables import RunnableSequence\\nchain = RunnableSequence([runnable1, runnable2])\\n```\\n\\nInvoking the `chain` with some input:\\n\\n```python\\nfinal_output = chain.invoke(some_input)\\n```\\n\\ncorresponds to the following:\\n\\n```python\\noutput1 = runnable1.invoke(some_input)\\nfinal_output = runnable2.invoke(output1)\\n```\\n\\n:::note\\n`runnable1` and `runnable2` are placeholders for any `Runnable` that you want to chain together.\\n:::\\n\\n### RunnableParallel\\n\\n`RunnableParallel` is a composition primitive that allows you to run multiple runnables concurrently, with the same input provided to each.\\n\\n```python\\nfrom langchain_core.runnables import RunnableParallel\\nchain = RunnableParallel({\\n    \"key1\": runnable1,\\n    \"key2\": runnable2,\\n})\\n```\\n\\nInvoking the `chain` with some input:\\n\\n```python\\nfinal_output = chain.invoke(some_input)\\n```\\n\\nWill yield a `final_output` dictionary with the same keys as the input dictionary, but with the values replaced by the output of the corresponding runnable.\\n\\n```python\\n{\\n    \"key1\": runnable1.invoke(some_input),\\n    \"key2\": runnable2.invoke(some_input),\\n}\\n```\\n\\nRecall, that the runnables are executed in parallel, so while the result is the same as\\ndictionary comprehension shown above, the execution time is much faster.\\n\\n:::note\\n`RunnableParallel`supports both synchronous and asynchronous execution (as all `Runnables` do).\\n\\n* For synchronous execution, `RunnableParallel` uses a [ThreadPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor) to run the runnables concurrently.\\n* For asynchronous execution, `RunnableParallel` uses [asyncio.gather](https://docs.python.org/3/library/asyncio.html#asyncio.gather) to run the runnables concurrently.\\n:::\\n\\n## Composition Syntax\\n\\nThe usage of `RunnableSequence` and `RunnableParallel` is so common that we created a shorthand syntax for using them. This helps\\nto make the code more readable and concise.\\n\\n### The `|` operator\\n\\nWe have [overloaded](https://docs.python.org/3/reference/datamodel.html#special-method-names) the `|` operator to create a `RunnableSequence` from two `Runnables`.\\n\\n```python\\nchain = runnable1 | runnable2\\n```\\n\\nis Equivalent to:\\n\\n```python\\nchain = RunnableSequence([runnable1, runnable2])\\n```\\n\\n### The `.pipe` method\\n\\nIf you have moral qualms with operator overloading, you can use the `.pipe` method instead. This is equivalent to the `|` operator.\\n\\n```python\\nchain = runnable1.pipe(runnable2)\\n```\\n\\n### Coercion\\n\\nLCEL applies automatic type coercion to make it easier to compose chains.\\n\\nIf you do not understand the type coercion, you can always use the `RunnableSequence` and `RunnableParallel` classes directly.\\n\\nThis will make the code more verbose, but it will also make it more explicit.\\n\\n#### Dictionary to RunnableParallel\\n\\nInside an LCEL expression, a dictionary is automatically converted to a `RunnableParallel`.\\n\\nFor example, the following code:\\n\\n```python\\nmapping = {\\n    \"key1\": runnable1,\\n    \"key2\": runnable2,\\n}\\n\\nchain = mapping | runnable3\\n```\\n\\nIt gets automatically converted to the following:\\n\\n```python\\nchain = RunnableSequence([RunnableParallel(mapping), runnable3])\\n```\\n\\n:::caution\\nYou have to be careful because the `mapping` dictionary is not a `RunnableParallel` object, it is just a dictionary. This means that the following code will raise an `AttributeError`:\\n\\n```python\\nmapping.invoke(some_input)\\n```\\n:::\\n\\n#### Function to RunnableLambda\\n\\nInside an LCEL expression, a function is automatically converted to a `RunnableLambda`.\\n\\n```\\ndef some_func(x):\\n    return x\\n\\nchain = some_func | runnable1\\n```\\n\\nIt gets automatically converted to the following:\\n\\n```python\\nchain = RunnableSequence([RunnableLambda(some_func), runnable1])\\n```\\n\\n:::caution\\nYou have to be careful because the lambda function is not a `RunnableLambda` object, it is just a function. This means that the following code will raise an `AttributeError`:\\n\\n```python\\nlambda x: x + 1.invoke(some_input)\\n```\\n:::\\n\\n## Legacy chains\\n\\nLCEL aims to provide consistency around behavior and customization over legacy subclassed chains such as `LLMChain` and\\n`ConversationalRetrievalChain`. Many of these legacy chains hide important details like prompts, and as a wider variety\\nof viable models emerge, customization has become more and more important.\\n\\nIf you are currently using one of these legacy chains, please see [this guide for guidance on how to migrate](/docs/versions/migrating_chains).\\n\\nFor guides on how to do specific tasks with LCEL, check out [the relevant how-to guides](/docs/how_to/#langchain-expression-language-lcel).\\n'), Document(metadata={'file_name': 'index.mdx', 'file_path': 'docs\\\\docs\\\\how_to\\\\index.mdx', 'file_type': '.mdx', 'source': 'docs\\\\docs\\\\how_to\\\\index.mdx'}, page_content='---\\nsidebar_position: 0\\nsidebar_class_name: hidden\\n---\\n\\n# How-to guides\\n\\nHere you‚Äôll find answers to ‚ÄúHow do I‚Ä¶.?‚Äù types of questions.\\nThese guides are *goal-oriented* and *concrete*; they\\'re meant to help you complete a specific task.\\nFor conceptual explanations see the [Conceptual guide](/docs/concepts/).\\nFor end-to-end walkthroughs see [Tutorials](/docs/tutorials).\\nFor comprehensive descriptions of every class and function see the [API Reference](https://python.langchain.com/api_reference/).\\n\\n## Installation\\n\\n- [How to: install LangChain packages](/docs/how_to/installation/)\\n- [How to: use LangChain with different Pydantic versions](/docs/how_to/pydantic_compatibility)\\n\\n## Key features\\n\\nThis highlights functionality that is core to using LangChain.\\n\\n- [How to: return structured data from a model](/docs/how_to/structured_output/)\\n- [How to: use a model to call tools](/docs/how_to/tool_calling)\\n- [How to: stream runnables](/docs/how_to/streaming)\\n- [How to: debug your LLM apps](/docs/how_to/debugging/)\\n\\n## Components\\n\\nThese are the core building blocks you can use when building applications.\\n\\n### Chat models\\n\\n[Chat Models](/docs/concepts/chat_models) are newer forms of language models that take messages in and output a message.\\nSee [supported integrations](/docs/integrations/chat/) for details on getting started with chat models from a specific provider.\\n\\n- [How to: do function/tool calling](/docs/how_to/tool_calling)\\n- [How to: get models to return structured output](/docs/how_to/structured_output)\\n- [How to: cache model responses](/docs/how_to/chat_model_caching)\\n- [How to: get log probabilities](/docs/how_to/logprobs)\\n- [How to: create a custom chat model class](/docs/how_to/custom_chat_model)\\n- [How to: stream a response back](/docs/how_to/chat_streaming)\\n- [How to: track token usage](/docs/how_to/chat_token_usage_tracking)\\n- [How to: track response metadata across providers](/docs/how_to/response_metadata)\\n- [How to: use chat model to call tools](/docs/how_to/tool_calling)\\n- [How to: stream tool calls](/docs/how_to/tool_streaming)\\n- [How to: handle rate limits](/docs/how_to/chat_model_rate_limiting)\\n- [How to: few shot prompt tool behavior](/docs/how_to/tools_few_shot)\\n- [How to: bind model-specific formatted tools](/docs/how_to/tools_model_specific)\\n- [How to: force a specific tool call](/docs/how_to/tool_choice)\\n- [How to: work with local models](/docs/how_to/local_llms)\\n- [How to: init any model in one line](/docs/how_to/chat_models_universal_init/)\\n- [How to: pass multimodal data directly to models](/docs/how_to/multimodal_inputs/)\\n\\n### Messages\\n\\n[Messages](/docs/concepts/messages) are the input and output of chat models. They have some `content` and a `role`, which describes the source of the message.\\n\\n- [How to: trim messages](/docs/how_to/trim_messages/)\\n- [How to: filter messages](/docs/how_to/filter_messages/)\\n- [How to: merge consecutive messages of the same type](/docs/how_to/merge_message_runs/)\\n\\n### Prompt templates\\n\\n[Prompt Templates](/docs/concepts/prompt_templates) are responsible for formatting user input into a format that can be passed to a language model.\\n\\n- [How to: use few shot examples](/docs/how_to/few_shot_examples)\\n- [How to: use few shot examples in chat models](/docs/how_to/few_shot_examples_chat/)\\n- [How to: partially format prompt templates](/docs/how_to/prompts_partial)\\n- [How to: compose prompts together](/docs/how_to/prompts_composition)\\n- [How to: use multimodal prompts](/docs/how_to/multimodal_prompts/)\\n\\n### Example selectors\\n\\n[Example Selectors](/docs/concepts/example_selectors) are responsible for selecting the correct few shot examples to pass to the prompt.\\n\\n- [How to: use example selectors](/docs/how_to/example_selectors)\\n- [How to: select examples by length](/docs/how_to/example_selectors_length_based)\\n- [How to: select examples by semantic similarity](/docs/how_to/example_selectors_similarity)\\n- [How to: select examples by semantic ngram overlap](/docs/how_to/example_selectors_ngram)\\n- [How to: select examples by maximal marginal relevance](/docs/how_to/example_selectors_mmr)\\n- [How to: select examples from LangSmith few-shot datasets](/docs/how_to/example_selectors_langsmith/)\\n\\n### LLMs\\n\\nWhat LangChain calls [LLMs](/docs/concepts/text_llms) are older forms of language models that take a string in and output a string.\\n\\n- [How to: cache model responses](/docs/how_to/llm_caching)\\n- [How to: create a custom LLM class](/docs/how_to/custom_llm)\\n- [How to: stream a response back](/docs/how_to/streaming_llm)\\n- [How to: track token usage](/docs/how_to/llm_token_usage_tracking)\\n- [How to: work with local models](/docs/how_to/local_llms)\\n\\n### Output parsers\\n\\n[Output Parsers](/docs/concepts/output_parsers) are responsible for taking the output of an LLM and parsing into more structured format.\\n\\n- [How to: parse text from message objects](/docs/how_to/output_parser_string)\\n- [How to: use output parsers to parse an LLM response into structured format](/docs/how_to/output_parser_structured)\\n- [How to: parse JSON output](/docs/how_to/output_parser_json)\\n- [How to: parse XML output](/docs/how_to/output_parser_xml)\\n- [How to: parse YAML output](/docs/how_to/output_parser_yaml)\\n- [How to: retry when output parsing errors occur](/docs/how_to/output_parser_retry)\\n- [How to: try to fix errors in output parsing](/docs/how_to/output_parser_fixing)\\n- [How to: write a custom output parser class](/docs/how_to/output_parser_custom)\\n\\n### Document loaders\\n\\n[Document Loaders](/docs/concepts/document_loaders) are responsible for loading documents from a variety of sources.\\n\\n- [How to: load PDF files](/docs/how_to/document_loader_pdf)\\n- [How to: load web pages](/docs/how_to/document_loader_web)\\n- [How to: load CSV data](/docs/how_to/document_loader_csv)\\n- [How to: load data from a directory](/docs/how_to/document_loader_directory)\\n- [How to: load HTML data](/docs/how_to/document_loader_html)\\n- [How to: load JSON data](/docs/how_to/document_loader_json)\\n- [How to: load Markdown data](/docs/how_to/document_loader_markdown)\\n- [How to: load Microsoft Office data](/docs/how_to/document_loader_office_file)\\n- [How to: write a custom document loader](/docs/how_to/document_loader_custom)\\n\\n### Text splitters\\n\\n[Text Splitters](/docs/concepts/text_splitters) take a document and split into chunks that can be used for retrieval.\\n\\n- [How to: recursively split text](/docs/how_to/recursive_text_splitter)\\n- [How to: split HTML](/docs/how_to/split_html)\\n- [How to: split by character](/docs/how_to/character_text_splitter)\\n- [How to: split code](/docs/how_to/code_splitter)\\n- [How to: split Markdown by headers](/docs/how_to/markdown_header_metadata_splitter)\\n- [How to: recursively split JSON](/docs/how_to/recursive_json_splitter)\\n- [How to: split text into semantic chunks](/docs/how_to/semantic-chunker)\\n- [How to: split by tokens](/docs/how_to/split_by_token)\\n\\n### Embedding models\\n\\n[Embedding Models](/docs/concepts/embedding_models) take a piece of text and create a numerical representation of it.\\nSee [supported integrations](/docs/integrations/text_embedding/) for details on getting started with embedding models from a specific provider.\\n\\n- [How to: embed text data](/docs/how_to/embed_text)\\n- [How to: cache embedding results](/docs/how_to/caching_embeddings)\\n- [How to: create a custom embeddings class](/docs/how_to/custom_embeddings)\\n\\n### Vector stores\\n\\n[Vector stores](/docs/concepts/vectorstores) are databases that can efficiently store and retrieve embeddings.\\nSee [supported integrations](/docs/integrations/vectorstores/) for details on getting started with vector stores from a specific provider.\\n\\n- [How to: use a vector store to retrieve data](/docs/how_to/vectorstores)\\n\\n### Retrievers\\n\\n[Retrievers](/docs/concepts/retrievers) are responsible for taking a query and returning relevant documents.\\n\\n- [How to: use a vector store to retrieve data](/docs/how_to/vectorstore_retriever)\\n- [How to: generate multiple queries to retrieve data for](/docs/how_to/MultiQueryRetriever)\\n- [How to: use contextual compression to compress the data retrieved](/docs/how_to/contextual_compression)\\n- [How to: write a custom retriever class](/docs/how_to/custom_retriever)\\n- [How to: add similarity scores to retriever results](/docs/how_to/add_scores_retriever)\\n- [How to: combine the results from multiple retrievers](/docs/how_to/ensemble_retriever)\\n- [How to: reorder retrieved results to mitigate the \"lost in the middle\" effect](/docs/how_to/long_context_reorder)\\n- [How to: generate multiple embeddings per document](/docs/how_to/multi_vector)\\n- [How to: retrieve the whole document for a chunk](/docs/how_to/parent_document_retriever)\\n- [How to: generate metadata filters](/docs/how_to/self_query)\\n- [How to: create a time-weighted retriever](/docs/how_to/time_weighted_vectorstore)\\n- [How to: use hybrid vector and keyword retrieval](/docs/how_to/hybrid)\\n\\n### Indexing\\n\\nIndexing is the process of keeping your vectorstore in-sync with the underlying data source.\\n\\n- [How to: reindex data to keep your vectorstore in-sync with the underlying data source](/docs/how_to/indexing)\\n\\n### Tools\\n\\nLangChain [Tools](/docs/concepts/tools) contain a description of the tool (to pass to the language model) as well as the implementation of the function to call. Refer [here](/docs/integrations/tools/) for a list of pre-built tools. \\n\\n- [How to: create tools](/docs/how_to/custom_tools)\\n- [How to: use built-in tools and toolkits](/docs/how_to/tools_builtin)\\n- [How to: use chat models to call tools](/docs/how_to/tool_calling)\\n- [How to: pass tool outputs to chat models](/docs/how_to/tool_results_pass_to_model)\\n- [How to: pass run time values to tools](/docs/how_to/tool_runtime)\\n- [How to: add a human-in-the-loop for tools](/docs/how_to/tools_human)\\n- [How to: handle tool errors](/docs/how_to/tools_error)\\n- [How to: force models to call a tool](/docs/how_to/tool_choice)\\n- [How to: disable parallel tool calling](/docs/how_to/tool_calling_parallel)\\n- [How to: access the `RunnableConfig` from a tool](/docs/how_to/tool_configure)\\n- [How to: stream events from a tool](/docs/how_to/tool_stream_events)\\n- [How to: return artifacts from a tool](/docs/how_to/tool_artifacts/)\\n- [How to: convert Runnables to tools](/docs/how_to/convert_runnable_to_tool)\\n- [How to: add ad-hoc tool calling capability to models](/docs/how_to/tools_prompting)\\n- [How to: pass in runtime secrets](/docs/how_to/runnable_runtime_secrets)\\n\\n### Multimodal\\n\\n- [How to: pass multimodal data directly to models](/docs/how_to/multimodal_inputs/)\\n- [How to: use multimodal prompts](/docs/how_to/multimodal_prompts/)\\n\\n\\n### Agents\\n\\n:::note\\n\\nFor in depth how-to guides for agents, please check out [LangGraph](https://langchain-ai.github.io/langgraph/) documentation.\\n\\n:::\\n\\n- [How to: use legacy LangChain Agents (AgentExecutor)](/docs/how_to/agent_executor)\\n- [How to: migrate from legacy LangChain agents to LangGraph](/docs/how_to/migrate_agent)\\n\\n### Callbacks\\n\\n[Callbacks](/docs/concepts/callbacks) allow you to hook into the various stages of your LLM application\\'s execution.\\n\\n- [How to: pass in callbacks at runtime](/docs/how_to/callbacks_runtime)\\n- [How to: attach callbacks to a module](/docs/how_to/callbacks_attach)\\n- [How to: pass callbacks into a module constructor](/docs/how_to/callbacks_constructor)\\n- [How to: create custom callback handlers](/docs/how_to/custom_callbacks)\\n- [How to: use callbacks in async environments](/docs/how_to/callbacks_async)\\n- [How to: dispatch custom callback events](/docs/how_to/callbacks_custom_events)\\n\\n### Custom\\n\\nAll of LangChain components can easily be extended to support your own versions.\\n\\n- [How to: create a custom chat model class](/docs/how_to/custom_chat_model)\\n- [How to: create a custom LLM class](/docs/how_to/custom_llm)\\n- [How to: create a custom embeddings class](/docs/how_to/custom_embeddings)\\n- [How to: write a custom retriever class](/docs/how_to/custom_retriever)\\n- [How to: write a custom document loader](/docs/how_to/document_loader_custom)\\n- [How to: write a custom output parser class](/docs/how_to/output_parser_custom)\\n- [How to: create custom callback handlers](/docs/how_to/custom_callbacks)\\n- [How to: define a custom tool](/docs/how_to/custom_tools)\\n- [How to: dispatch custom callback events](/docs/how_to/callbacks_custom_events)\\n\\n### Serialization\\n- [How to: save and load LangChain objects](/docs/how_to/serialization)\\n\\n## Use cases\\n\\nThese guides cover use-case specific details.\\n\\n### Q&A with RAG\\n\\nRetrieval Augmented Generation (RAG) is a way to connect LLMs to external sources of data.\\nFor a high-level tutorial on RAG, check out [this guide](/docs/tutorials/rag/).\\n\\n- [How to: add chat history](/docs/how_to/qa_chat_history_how_to/)\\n- [How to: stream](/docs/how_to/qa_streaming/)\\n- [How to: return sources](/docs/how_to/qa_sources/)\\n- [How to: return citations](/docs/how_to/qa_citations/)\\n- [How to: do per-user retrieval](/docs/how_to/qa_per_user/)\\n\\n\\n### Extraction\\n\\nExtraction is when you use LLMs to extract structured information from unstructured text.\\nFor a high level tutorial on extraction, check out [this guide](/docs/tutorials/extraction/).\\n\\n- [How to: use reference examples](/docs/how_to/extraction_examples/)\\n- [How to: handle long text](/docs/how_to/extraction_long_text/)\\n- [How to: do extraction without using function calling](/docs/how_to/extraction_parse)\\n\\n### Chatbots\\n\\nChatbots involve using an LLM to have a conversation.\\nFor a high-level tutorial on building chatbots, check out [this guide](/docs/tutorials/chatbot/).\\n\\n- [How to: manage memory](/docs/how_to/chatbots_memory)\\n- [How to: do retrieval](/docs/how_to/chatbots_retrieval)\\n- [How to: use tools](/docs/how_to/chatbots_tools)\\n- [How to: manage large chat history](/docs/how_to/trim_messages/)\\n\\n### Query analysis\\n\\nQuery Analysis is the task of using an LLM to generate a query to send to a retriever.\\nFor a high-level tutorial on query analysis, check out [this guide](/docs/tutorials/rag/#query-analysis).\\n\\n- [How to: add examples to the prompt](/docs/how_to/query_few_shot)\\n- [How to: handle cases where no queries are generated](/docs/how_to/query_no_queries)\\n- [How to: handle multiple queries](/docs/how_to/query_multiple_queries)\\n- [How to: handle multiple retrievers](/docs/how_to/query_multiple_retrievers)\\n- [How to: construct filters](/docs/how_to/query_constructing_filters)\\n- [How to: deal with high cardinality categorical variables](/docs/how_to/query_high_cardinality)\\n\\n### Q&A over SQL + CSV\\n\\nYou can use LLMs to do question answering over tabular data.\\nFor a high-level tutorial, check out [this guide](/docs/tutorials/sql_qa/).\\n\\n- [How to: use prompting to improve results](/docs/how_to/sql_prompting)\\n- [How to: do query validation](/docs/how_to/sql_query_checking)\\n- [How to: deal with large databases](/docs/how_to/sql_large_db)\\n- [How to: deal with CSV files](/docs/how_to/sql_csv)\\n\\n### Q&A over graph databases\\n\\nYou can use an LLM to do question answering over graph databases.\\nFor a high-level tutorial, check out [this guide](/docs/tutorials/graph/).\\n\\n- [How to: add a semantic layer over the database](/docs/how_to/graph_semantic)\\n- [How to: construct knowledge graphs](/docs/how_to/graph_constructing)\\n\\n### Summarization\\n\\nLLMs can summarize and otherwise distill desired information from text, including\\nlarge volumes of text. For a high-level tutorial, check out [this guide](/docs/tutorials/summarization).\\n\\n- [How to: summarize text in a single LLM call](/docs/how_to/summarize_stuff)\\n- [How to: summarize text through parallelization](/docs/how_to/summarize_map_reduce)\\n- [How to: summarize text through iterative refinement](/docs/how_to/summarize_refine)\\n\\n## LangChain Expression Language (LCEL)\\n\\n:::note Should I use LCEL?\\n\\nLCEL is an orchestration solution. See our\\n[concepts page](/docs/concepts/lcel/#should-i-use-lcel) for recommendations on when to\\nuse LCEL.\\n\\n:::\\n\\n[LangChain Expression Language](/docs/concepts/lcel) is a way to create arbitrary custom chains. It is built on the [Runnable](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html) protocol.\\n\\n[**LCEL cheatsheet**](/docs/how_to/lcel_cheatsheet/): For a quick overview of how to use the main LCEL primitives.\\n\\n[**Migration guide**](/docs/versions/migrating_chains): For migrating legacy chain abstractions to LCEL.\\n\\n- [How to: chain runnables](/docs/how_to/sequence)\\n- [How to: stream runnables](/docs/how_to/streaming)\\n- [How to: invoke runnables in parallel](/docs/how_to/parallel/)\\n- [How to: add default invocation args to runnables](/docs/how_to/binding/)\\n- [How to: turn any function into a runnable](/docs/how_to/functions)\\n- [How to: pass through inputs from one chain step to the next](/docs/how_to/passthrough)\\n- [How to: configure runnable behavior at runtime](/docs/how_to/configure)\\n- [How to: add message history (memory) to a chain](/docs/how_to/message_history)\\n- [How to: route between sub-chains](/docs/how_to/routing)\\n- [How to: create a dynamic (self-constructing) chain](/docs/how_to/dynamic_chain/)\\n- [How to: inspect runnables](/docs/how_to/inspect)\\n- [How to: add fallbacks to a runnable](/docs/how_to/fallbacks)\\n- [How to: pass runtime secrets to a runnable](/docs/how_to/runnable_runtime_secrets)\\n\\n## [LangGraph](https://langchain-ai.github.io/langgraph)\\n\\nLangGraph is an extension of LangChain aimed at\\nbuilding robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.\\n\\nLangGraph documentation is currently hosted on a separate site.\\nYou can peruse [LangGraph how-to guides here](https://langchain-ai.github.io/langgraph/how-tos/).\\n\\n## [LangSmith](https://docs.smith.langchain.com/)\\n\\nLangSmith allows you to closely trace, monitor and evaluate your LLM application.\\nIt seamlessly integrates with LangChain and LangGraph, and you can use it to inspect and debug individual steps of your chains and agents as you build.\\n\\nLangSmith documentation is hosted on a separate site.\\nYou can peruse [LangSmith how-to guides here](https://docs.smith.langchain.com/), but we\\'ll highlight a few sections that are particularly\\nrelevant to LangChain below:\\n\\n### Evaluation\\n<span data-heading-keywords=\"evaluation,evaluate\"></span>\\n\\nEvaluating performance is a vital part of building LLM-powered applications.\\nLangSmith helps with every step of the process from creating a dataset to defining metrics to running evaluators.\\n\\nTo learn more, check out the [LangSmith evaluation how-to guides](https://docs.smith.langchain.com/how_to_guides#evaluation).\\n\\n### Tracing\\n<span data-heading-keywords=\"trace,tracing\"></span>\\n\\nTracing gives you observability inside your chains and agents, and is vital in diagnosing issues.\\n\\n- [How to: trace with LangChain](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain)\\n- [How to: add metadata and tags to traces](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#add-metadata-and-tags-to-traces)\\n\\nYou can see general tracing-related how-tos [in this section of the LangSmith docs](https://docs.smith.langchain.com/how_to_guides/tracing).\\n')]\n",
      "rank:0\n",
      "rank:1\n",
      "rank:2\n",
      "rank:3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LangChain„ÅØ„ÄÅÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÔºàLLMÔºâ„ÇíÊ¥ªÁî®„Åó„Åü„Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥„ÇíÈñãÁô∫„Åô„Çã„Åü„ÇÅ„ÅÆ„Éï„É¨„Éº„É†„ÉØ„Éº„ÇØ„Åß„Åô„ÄÇLangChain„ÅÆÁõÆÁöÑ„ÅØ„ÄÅÈñãÁô∫ËÄÖ„ÅåÊé®Ë´ñ„ÇíË°å„ÅÜ„Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥„Çí„Åß„Åç„Çã„Å†„ÅëÁ∞°Âçò„Å´ÊßãÁØâ„Åß„Åç„Çã„Çà„ÅÜ„Å´„Åô„Çã„Åì„Å®„Åß„Åô„ÄÇÂÖÉ„ÄÖ„ÅØ„Ç™„Éº„Éó„É≥„ÇΩ„Éº„Çπ„ÅÆÂçò‰∏Ä„Éë„ÉÉ„Ç±„Éº„Ç∏„Å®„Åó„Å¶Âßã„Åæ„Çä„Åæ„Åó„Åü„Åå„ÄÅÁèæÂú®„ÅØ‰ºÅÊ•≠„Å®„Ç®„Ç≥„Ç∑„Çπ„ÉÜ„É†ÂÖ®‰Ωì„Å´ÈÄ≤Âåñ„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ\\n\\nLangChain„ÅÆ‰∏ª„Å™ÁâπÂæ¥„Å´„ÅØ‰ª•‰∏ã„ÅåÂê´„Åæ„Çå„Åæ„ÅôÔºö\\n\\n1. **Ê®ôÊ∫ñÂåñ„Åï„Çå„Åü„Ç≥„É≥„Éù„Éº„Éç„É≥„Éà„Ç§„É≥„Çø„Éº„Éï„Çß„Éº„Çπ**: Â§öÊßò„Å™AI„Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥„ÅÆ„É¢„Éá„É´„ÇÑÈñ¢ÈÄ£„Ç≥„É≥„Éù„Éº„Éç„É≥„Éà„ÅÆAPI„ÇíÁµ±‰∏Ä„Åó„ÄÅÈñãÁô∫ËÄÖ„Åå„Éó„É≠„Éê„Ç§„ÉÄ„ÉºÈñì„ÅßÁ∞°Âçò„Å´Âàá„ÇäÊõø„Åà„Çâ„Çå„Çã„Çà„ÅÜ„Å´„Åó„Åæ„Åô„ÄÇ\\n\\n2. **„Ç™„Éº„Ç±„Çπ„Éà„É¨„Éº„Ç∑„Éß„É≥**: Ë§áÊï∞„ÅÆ„Ç≥„É≥„Éù„Éº„Éç„É≥„Éà„ÇÑ„É¢„Éá„É´„ÇíÂäπÁéáÁöÑ„Å´Êé•Á∂ö„Åó„ÄÅË§áÈõë„Å™„Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥„ÇíÊßãÁØâ„Åô„Çã„Åü„ÇÅ„ÅÆÂà∂Âæ°„Éï„É≠„Éº„ÇíÊèê‰æõ„Åó„Åæ„Åô„ÄÇ\\n\\n3. **ÂèØË¶ñÂåñ„Å®Ë©ï‰æ°**: „Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥„ÅÆË§áÈõë„Åï„ÅåÂ¢ó„Åô‰∏≠„Åß„ÄÅ‰Ωï„ÅåËµ∑„Åì„Å£„Å¶„ÅÑ„Çã„ÅÆ„Åã„ÇíÁêÜËß£„Åó„ÇÑ„Åô„Åè„Åó„ÄÅÈñãÁô∫ËÄÖ„ÅåËøÖÈÄü„Å´Ë©ï‰æ°„ÇíË°å„Åà„Çã„Çà„ÅÜ„Å´„Åó„Åæ„Åô„ÄÇ\\n\\nLangChain„ÅØ„ÄÅÈñãÁô∫„ÄÅÈÅãÁî®„ÄÅ„Éá„Éó„É≠„Ç§„ÅÆÂêÑÊÆµÈöé„ÇíÁ∞°Á¥†Âåñ„Åó„ÄÅÈñãÁô∫ËÄÖ„ÅåËá™ÂàÜ„ÅÆ„É¶„Éº„Çπ„Ç±„Éº„Çπ„Å´ÊúÄÈÅ©„Å™„Ç≥„É≥„Éù„Éº„Éç„É≥„Éà„ÇíÈÅ∏Êäû„Åó„Å¶‰ΩøÁî®„Åß„Åç„Çã„Çà„ÅÜ„Å´Ë®≠Ë®à„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Åæ„Åü„ÄÅLangGraph„ÇÑLangSmith„Å®„ÅÑ„Å£„ÅüÈñ¢ÈÄ£„ÉÑ„Éº„É´„ÇíÈÄö„Åò„Å¶„ÄÅË§áÈõë„Å™„Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥„ÅÆÊßãÁØâ„ÇÑË©ï‰æ°„Çí„Çµ„Éù„Éº„Éà„Åó„Åæ„Åô„ÄÇ'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_fusion_chain = {\n",
    "    \"question\": RunnablePassthrough(),\n",
    "    \"context\": query_generation_chain | retriever.map() | reciprocal_rank_fusion,\n",
    "} | prompt | model | StrOutputParser()\n",
    "\n",
    "rag_fusion_chain.invoke(\"LangChain„ÅÆÊ¶ÇË¶Å„ÇíÊïô„Åà„Å¶\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-5. Ë§áÊï∞„ÅÆ Retriever „Çí‰Ωø„ÅÜÂ∑•Â§´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import TavilySearchAPIRetriever\n",
    "\n",
    "langchain_document_retriever = retriever.with_config(\n",
    "    {\"run_name\": \"langchain_document_retriever\"}\n",
    ")\n",
    "\n",
    "web_retriever = TavilySearchAPIRetriever(k=3).with_config(\n",
    "    {\"run_name\": \"web_retriever\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class Route(str, Enum):\n",
    "    langchain_document = \"langchain_document\"\n",
    "    web = \"web\"\n",
    "\n",
    "\n",
    "class RouteOutput(BaseModel):\n",
    "    route: Route\n",
    "\n",
    "\n",
    "route_prompt = ChatPromptTemplate.from_template(\"\"\"\\\n",
    "Ë≥™Âïè„Å´ÂõûÁ≠î„Åô„Çã„Åü„ÇÅ„Å´ÈÅ©Âàá„Å™Retriever„ÇíÈÅ∏Êäû„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n",
    "\n",
    "Ë≥™Âïè: {question}\n",
    "\"\"\")\n",
    "\n",
    "route_chain = (\n",
    "    route_prompt\n",
    "    | model.with_structured_output(RouteOutput)\n",
    "    | (lambda x: x.route)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "def routed_retriever(inp: dict[str, Any]) -> list[Document]:\n",
    "    question = inp[\"question\"]\n",
    "    route = inp[\"route\"]\n",
    "    # any„ÅåÂ§ßÊñáÂ≠ó„Å†„Å®„Ç®„É©„Éº„Åô„Çã„ÅÆ„Åß„ÄÅÂ∞èÊñáÂ≠ó„Å´Â§âÊõ¥\n",
    "    if route == Route.langchain_document:\n",
    "        return langchain_document_retriever.invoke(question)\n",
    "    elif route == Route.web:\n",
    "        return web_retriever.invoke(question)\n",
    "\n",
    "    raise ValueError(f\"Unknown route: {route}\")\n",
    "\n",
    "\n",
    "route_rag_chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"route\": route_chain,\n",
    "    }\n",
    "    | RunnablePassthrough.assign(context=routed_retriever)\n",
    "    | prompt | model | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain„ÅØ„ÄÅÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÔºàLLMÔºâ„ÇíÊ¥ªÁî®„Åó„Åü„Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥„ÇíÈñãÁô∫„Åô„Çã„Åü„ÇÅ„ÅÆ„Éï„É¨„Éº„É†„ÉØ„Éº„ÇØ„Åß„Åô„ÄÇ„Åì„ÅÆ„Éï„É¨„Éº„É†„ÉØ„Éº„ÇØ„ÅØ„ÄÅLLM„Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥„ÅÆ„É©„Ç§„Éï„Çµ„Ç§„ÇØ„É´„ÅÆÂêÑÊÆµÈöé„ÇíÁ∞°Á¥†Âåñ„Åó„Åæ„Åô„ÄÇÂÖ∑‰ΩìÁöÑ„Å´„ÅØ„ÄÅ‰ª•‰∏ã„ÅÆ„Çà„ÅÜ„Å™Ê©üËÉΩ„ÇíÊèê‰æõ„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ\\n\\n1. **ÈñãÁô∫**: LangChain„ÅÆ„Ç™„Éº„Éó„É≥„ÇΩ„Éº„Çπ„Ç≥„É≥„Éù„Éº„Éç„É≥„Éà„ÇÑ„Çµ„Éº„Éâ„Éë„Éº„ÉÜ„Ç£Áµ±Âêà„Çí‰ΩøÁî®„Åó„Å¶„Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥„ÇíÊßãÁØâ„Åß„Åç„Åæ„Åô„ÄÇLangGraph„ÇíÂà©Áî®„Åô„Çã„Åì„Å®„Åß„ÄÅÁä∂ÊÖã„ÇíÊåÅ„Å§„Ç®„Éº„Ç∏„Çß„É≥„Éà„Çí‰ΩúÊàê„Åó„ÄÅ„Çπ„Éà„É™„Éº„Éü„É≥„Ç∞„ÇÑ‰∫∫Èñì„ÅÆ‰ªãÂÖ•„Çí„Çµ„Éù„Éº„Éà„Åó„Åæ„Åô„ÄÇ\\n\\n2. **ÁîüÁî£Âåñ**: LangSmith„Çí‰ΩøÁî®„Åó„Å¶„Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥„ÇíÊ§úÊüª„ÄÅÁõ£Ë¶ñ„ÄÅË©ï‰æ°„Åó„ÄÅÁ∂ôÁ∂öÁöÑ„Å´ÊúÄÈÅ©Âåñ„Åó„Å¶Ëá™‰ø°„ÇíÊåÅ„Å£„Å¶„Éá„Éó„É≠„Ç§„Åß„Åç„Åæ„Åô„ÄÇ\\n\\n3. **„Éá„Éó„É≠„Ç§**: LangGraph„Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥„ÇíÁîüÁî£Ê∫ñÂÇô„ÅåÊï¥„Å£„ÅüAPI„ÇÑ„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„Å´Â§âÊèõ„Åô„Çã„Åì„Å®„Åå„Åß„Åç„Åæ„Åô„ÄÇ\\n\\nLangChain„ÅØ„ÄÅLLM„ÇÑÈñ¢ÈÄ£ÊäÄË°ìÔºàÂüã„ÇÅËæº„Åø„É¢„Éá„É´„ÇÑ„Éô„ÇØ„Éà„É´„Çπ„Éà„Ç¢„Å™„Å©Ôºâ„Å´ÂØæ„Åô„ÇãÊ®ôÊ∫ñ„Ç§„É≥„Çø„Éº„Éï„Çß„Éº„Çπ„ÇíÂÆüË£Ö„Åó„Å¶„Åä„Çä„ÄÅÊï∞Áôæ„ÅÆ„Éó„É≠„Éê„Ç§„ÉÄ„Éº„Å®Áµ±Âêà„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Åæ„Åü„ÄÅË§áÊï∞„ÅÆ„Ç™„Éº„Éó„É≥„ÇΩ„Éº„Çπ„É©„Ç§„Éñ„É©„É™„ÅßÊßãÊàê„Åï„Çå„Å¶„Åä„Çä„ÄÅ„É¶„Éº„Ç∂„Éº„ÅØËá™ÂàÜ„ÅÆ„Éã„Éº„Ç∫„Å´Âøú„Åò„Å¶„Ç≥„É≥„Éù„Éº„Éç„É≥„Éà„ÇíÈÅ∏Êäû„Åó„Å¶‰ΩøÁî®„Åß„Åç„Åæ„Åô„ÄÇ'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "route_rag_chain.invoke(\"LangChain„ÅÆÊ¶ÇË¶Å„ÇíÊïô„Åà„Å¶\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'TAVILY_API_KEY'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m route_rag_chain\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mÊù±‰∫¨„ÅÆ‰ªäÊó•„ÅÆÂ§©Ê∞ó„ÅØÔºü\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ktgu1\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3013\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3011\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3012\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3013\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[0;32m   3014\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   3015\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\ktgu1\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\passthrough.py:497\u001b[0m, in \u001b[0;36mRunnableAssign.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    492\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28minput\u001b[39m: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m    494\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    496\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m--> 497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ktgu1\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1916\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, serialized, **kwargs)\u001b[0m\n\u001b[0;32m   1912\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[0;32m   1913\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[0;32m   1914\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[0;32m   1915\u001b[0m         Output,\n\u001b[1;32m-> 1916\u001b[0m         context\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m   1917\u001b[0m             call_func_with_variable_args,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1918\u001b[0m             func,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m             \u001b[38;5;28minput\u001b[39m,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1920\u001b[0m             config,\n\u001b[0;32m   1921\u001b[0m             run_manager,\n\u001b[0;32m   1922\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1923\u001b[0m         ),\n\u001b[0;32m   1924\u001b[0m     )\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1926\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mc:\\Users\\ktgu1\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\config.py:398\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    397\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 398\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ktgu1\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\passthrough.py:484\u001b[0m, in \u001b[0;36mRunnableAssign._invoke\u001b[1;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_invoke\u001b[39m(\n\u001b[0;32m    472\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;28minput\u001b[39m: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    477\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    479\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mdict\u001b[39m\n\u001b[0;32m    480\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe input to RunnablePassthrough.assign() must be a dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    483\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m--> 484\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmapper\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m    485\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    486\u001b[0m             patch_config(config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child()),\n\u001b[0;32m    487\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    488\u001b[0m         ),\n\u001b[0;32m    489\u001b[0m     }\n",
      "File \u001b[1;32mc:\\Users\\ktgu1\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3713\u001b[0m, in \u001b[0;36mRunnableParallel.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m   3708\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m   3709\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   3710\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(_invoke_step, step, \u001b[38;5;28minput\u001b[39m, config, key)\n\u001b[0;32m   3711\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   3712\u001b[0m         ]\n\u001b[1;32m-> 3713\u001b[0m         output \u001b[38;5;241m=\u001b[39m {key: future\u001b[38;5;241m.\u001b[39mresult() \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[0;32m   3714\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   3715\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\ktgu1\\anaconda3\\Lib\\concurrent\\futures\\_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32mc:\\Users\\ktgu1\\anaconda3\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ktgu1\\anaconda3\\Lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32mc:\\Users\\ktgu1\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3697\u001b[0m, in \u001b[0;36mRunnableParallel.invoke.<locals>._invoke_step\u001b[1;34m(step, input, config, key)\u001b[0m\n\u001b[0;32m   3695\u001b[0m context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[0;32m   3696\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[1;32m-> 3697\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m context\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m   3698\u001b[0m     step\u001b[38;5;241m.\u001b[39minvoke,\n\u001b[0;32m   3699\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   3700\u001b[0m     child_config,\n\u001b[0;32m   3701\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ktgu1\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4680\u001b[0m, in \u001b[0;36mRunnableLambda.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   4666\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke this Runnable synchronously.\u001b[39;00m\n\u001b[0;32m   4667\u001b[0m \n\u001b[0;32m   4668\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4677\u001b[0m \u001b[38;5;124;03m    TypeError: If the Runnable is a coroutine function.\u001b[39;00m\n\u001b[0;32m   4678\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 4680\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[0;32m   4681\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invoke,\n\u001b[0;32m   4682\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   4683\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config(config, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc),\n\u001b[0;32m   4684\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4685\u001b[0m     )\n\u001b[0;32m   4686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4687\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   4688\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke a coroutine function synchronously.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4689\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `ainvoke` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4690\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ktgu1\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1916\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, serialized, **kwargs)\u001b[0m\n\u001b[0;32m   1912\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[0;32m   1913\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[0;32m   1914\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[0;32m   1915\u001b[0m         Output,\n\u001b[1;32m-> 1916\u001b[0m         context\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m   1917\u001b[0m             call_func_with_variable_args,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1918\u001b[0m             func,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m             \u001b[38;5;28minput\u001b[39m,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1920\u001b[0m             config,\n\u001b[0;32m   1921\u001b[0m             run_manager,\n\u001b[0;32m   1922\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1923\u001b[0m         ),\n\u001b[0;32m   1924\u001b[0m     )\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1926\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mc:\\Users\\ktgu1\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\config.py:398\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    397\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 398\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ktgu1\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4536\u001b[0m, in \u001b[0;36mRunnableLambda._invoke\u001b[1;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m   4534\u001b[0m                 output \u001b[38;5;241m=\u001b[39m chunk\n\u001b[0;32m   4535\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4536\u001b[0m     output \u001b[38;5;241m=\u001b[39m call_func_with_variable_args(\n\u001b[0;32m   4537\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28minput\u001b[39m, config, run_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   4538\u001b[0m     )\n\u001b[0;32m   4539\u001b[0m \u001b[38;5;66;03m# If the output is a Runnable, invoke it\u001b[39;00m\n\u001b[0;32m   4540\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "File \u001b[1;32mc:\\Users\\ktgu1\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\config.py:398\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    397\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 398\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[34], line 10\u001b[0m, in \u001b[0;36mrouted_retriever\u001b[1;34m(inp)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m langchain_document_retriever\u001b[38;5;241m.\u001b[39minvoke(question)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m route \u001b[38;5;241m==\u001b[39m Route\u001b[38;5;241m.\u001b[39mweb:\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m web_retriever\u001b[38;5;241m.\u001b[39minvoke(question)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown route: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroute\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ktgu1\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5313\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   5307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m   5308\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5309\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[0;32m   5310\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   5311\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   5312\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[1;32m-> 5313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m   5314\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   5315\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[0;32m   5316\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[0;32m   5317\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ktgu1\\anaconda3\\Lib\\site-packages\\langchain_core\\retrievers.py:254\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    253\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_error(e)\n\u001b[1;32m--> 254\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    256\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_end(\n\u001b[0;32m    257\u001b[0m         result,\n\u001b[0;32m    258\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ktgu1\\anaconda3\\Lib\\site-packages\\langchain_core\\retrievers.py:247\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    245\u001b[0m _kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_arg_supported:\n\u001b[1;32m--> 247\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\n\u001b[0;32m    248\u001b[0m         \u001b[38;5;28minput\u001b[39m, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs\n\u001b[0;32m    249\u001b[0m     )\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    251\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\ktgu1\\anaconda3\\Lib\\site-packages\\langchain_community\\retrievers\\tavily_search_api.py:109\u001b[0m, in \u001b[0;36mTavilySearchAPIRetriever._get_relevant_documents\u001b[1;34m(self, query, run_manager)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m    105\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTavily python package not found. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install tavily-python`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    107\u001b[0m     )\n\u001b[1;32m--> 109\u001b[0m tavily \u001b[38;5;241m=\u001b[39m TavilyClient(api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;129;01mor\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTAVILY_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    110\u001b[0m max_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_generated_answer \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    111\u001b[0m response \u001b[38;5;241m=\u001b[39m tavily\u001b[38;5;241m.\u001b[39msearch(\n\u001b[0;32m    112\u001b[0m     query\u001b[38;5;241m=\u001b[39mquery,\n\u001b[0;32m    113\u001b[0m     max_results\u001b[38;5;241m=\u001b[39mmax_results,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs,\n\u001b[0;32m    121\u001b[0m )\n",
      "File \u001b[1;32m<frozen os>:714\u001b[0m, in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'TAVILY_API_KEY'"
     ]
    }
   ],
   "source": [
    "route_rag_chain.invoke(\"Êù±‰∫¨„ÅÆ‰ªäÊó•„ÅÆÂ§©Ê∞ó„ÅØÔºü\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### „Éè„Ç§„Éñ„É™„ÉÉ„ÉâÊ§úÁ¥¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rank-bm25==0.2.2\n",
      "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from rank-bm25==0.2.2) (1.26.4)\n",
      "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Installing collected packages: rank-bm25\n",
      "Successfully installed rank-bm25-0.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install rank-bm25==0.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "chroma_retriever = retriever.with_config(\n",
    "    {\"run_name\": \"chroma_retriever\"}\n",
    ")\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(documents).with_config(\n",
    "    {\"run_name\": \"bm25_retriever\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "hybrid_retriever = (\n",
    "    RunnableParallel({\n",
    "        \"chroma_documents\": chroma_retriever,\n",
    "        \"bm25_documents\": bm25_retriever,\n",
    "    })\n",
    "    | (lambda x: [x[\"chroma_documents\"], x[\"bm25_documents\"]])\n",
    "    | reciprocal_rank_fusion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs:[Document(metadata={'file_name': 'introduction.mdx', 'file_path': 'docs\\\\docs\\\\introduction.mdx', 'file_type': '.mdx', 'source': 'docs\\\\docs\\\\introduction.mdx'}, page_content='---\\nsidebar_position: 0\\nsidebar_class_name: hidden\\n---\\n\\n# Introduction\\n\\n**LangChain** is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:\\n- **Development**: Build your applications using LangChain\\'s open-source [components](/docs/concepts) and [third-party integrations](/docs/integrations/providers/).\\nUse [LangGraph](/docs/concepts/architecture/#langgraph) to build stateful agents with first-class streaming and human-in-the-loop support.\\n- **Productionization**: Use [LangSmith](https://docs.smith.langchain.com/) to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\n- **Deployment**: Turn your LangGraph applications into production-ready APIs and Assistants with [LangGraph Platform](https://langchain-ai.github.io/langgraph/cloud/).\\n\\nimport ThemedImage from \\'@theme/ThemedImage\\';\\nimport useBaseUrl from \\'@docusaurus/useBaseUrl\\';\\n\\n<ThemedImage\\n  alt=\"Diagram outlining the hierarchical organization of the LangChain framework, displaying the interconnected parts across multiple layers.\"\\n  sources={{\\n    light: useBaseUrl(\\'/svg/langchain_stack_112024.svg\\'),\\n    dark: useBaseUrl(\\'/svg/langchain_stack_112024_dark.svg\\'),\\n  }}\\n  style={{ width: \"100%\" }}\\n  title=\"LangChain Framework Overview\"\\n/>\\n\\nLangChain implements a standard interface for large language models and related\\ntechnologies, such as embedding models and vector stores, and integrates with\\nhundreds of providers. See the [integrations](/docs/integrations/providers/) page for\\nmore.\\n\\nimport ChatModelTabs from \"@theme/ChatModelTabs\";\\n\\n<ChatModelTabs/>\\n\\n```python\\nmodel.invoke(\"Hello, world!\")\\n```\\n\\n:::note\\n\\nThese docs focus on the Python LangChain library. [Head here](https://js.langchain.com) for docs on the JavaScript LangChain library.\\n\\n:::\\n\\n## Architecture\\n\\nThe LangChain framework consists of multiple open-source libraries. Read more in the\\n[Architecture](/docs/concepts/architecture/) page.\\n\\n- **`langchain-core`**: Base abstractions for chat models and other components.\\n- **Integration packages** (e.g. `langchain-openai`, `langchain-anthropic`, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\\n- **`langchain`**: Chains, agents, and retrieval strategies that make up an application\\'s cognitive architecture.\\n- **`langchain-community`**: Third-party integrations that are community maintained.\\n- **`langgraph`**: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See [LangGraph documentation](https://langchain-ai.github.io/langgraph/).\\n\\n## Guides\\n\\n### [Tutorials](/docs/tutorials)\\n\\nIf you\\'re looking to build something specific or are more of a hands-on learner, check out our [tutorials section](/docs/tutorials).\\nThis is the best place to get started.\\n\\nThese are the best ones to get started with:\\n\\n- [Build a Simple LLM Application](/docs/tutorials/llm_chain)\\n- [Build a Chatbot](/docs/tutorials/chatbot)\\n- [Build an Agent](/docs/tutorials/agents)\\n- [Introduction to LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/)\\n\\nExplore the full list of LangChain tutorials [here](/docs/tutorials), and check out other [LangGraph tutorials here](https://langchain-ai.github.io/langgraph/tutorials/). To learn more about LangGraph, check out our first LangChain Academy course, *Introduction to LangGraph*, available [here](https://academy.langchain.com/courses/intro-to-langgraph).\\n\\n\\n### [How-to guides](/docs/how_to)\\n\\n[Here](/docs/how_to) you‚Äôll find short answers to ‚ÄúHow do I‚Ä¶.?‚Äù types of questions.\\nThese how-to guides don‚Äôt cover topics in depth ‚Äì you‚Äôll find that material in the [Tutorials](/docs/tutorials) and the [API Reference](https://python.langchain.com/api_reference/).\\nHowever, these guides will help you quickly accomplish common tasks using [chat models](/docs/how_to/#chat-models),\\n[vector stores](/docs/how_to/#vector-stores), and other common LangChain components.\\n\\nCheck out [LangGraph-specific how-tos here](https://langchain-ai.github.io/langgraph/how-tos/).\\n\\n### [Conceptual guide](/docs/concepts)\\n\\nIntroductions to all the key parts of LangChain you‚Äôll need to know! [Here](/docs/concepts) you\\'ll find high level explanations of all LangChain concepts.\\n\\nFor a deeper dive into LangGraph concepts, check out [this page](https://langchain-ai.github.io/langgraph/concepts/).\\n\\n### [Integrations](integrations/providers/index.mdx)\\n\\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\\nIf you\\'re looking to get up and running quickly with [chat models](/docs/integrations/chat/), [vector stores](/docs/integrations/vectorstores/),\\nor other LangChain components from a specific provider, check out our growing list of [integrations](/docs/integrations/providers/).\\n\\n\\n### [API reference](https://python.langchain.com/api_reference/)\\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\\n\\n## Ecosystem\\n\\n### [ü¶úüõ†Ô∏è LangSmith](https://docs.smith.langchain.com)\\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\\n\\n### [ü¶úüï∏Ô∏è LangGraph](https://langchain-ai.github.io/langgraph)\\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by Linkedin, Uber, Klarna, GitLab, and many more.\\n\\n## Additional resources\\n\\n### [Versions](/docs/versions/v0_3/)\\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\\n\\n### [Security](/docs/security)\\nRead up on [security](/docs/security) best practices to make sure you\\'re developing safely with LangChain.\\n\\n### [Contributing](contributing/index.mdx)\\nCheck out the developer\\'s guide for guidelines on contributing and help getting your dev environment set up.\\n'), Document(metadata={'file_name': 'lcel.mdx', 'file_path': 'docs\\\\docs\\\\concepts\\\\lcel.mdx', 'file_type': '.mdx', 'source': 'docs\\\\docs\\\\concepts\\\\lcel.mdx'}, page_content='# LangChain Expression Language (LCEL)\\n\\n:::info Prerequisites\\n* [Runnable Interface](/docs/concepts/runnables)\\n:::\\n\\nThe **L**ang**C**hain **E**xpression **L**anguage (LCEL) takes a [declarative](https://en.wikipedia.org/wiki/Declarative_programming) approach to building new [Runnables](/docs/concepts/runnables) from existing Runnables.\\n\\nThis means that you describe what *should* happen, rather than *how* it should happen, allowing LangChain to optimize the run-time execution of the chains.\\n\\nWe often refer to a `Runnable` created using LCEL as a \"chain\". It\\'s important to remember that a \"chain\" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).\\n\\n:::note\\n* The [LCEL cheatsheet](/docs/how_to/lcel_cheatsheet/) shows common patterns that involve the Runnable interface and LCEL expressions.\\n* Please see the following list of [how-to guides](/docs/how_to/#langchain-expression-language-lcel) that cover common tasks with LCEL.\\n* A list of built-in `Runnables` can be found in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html). Many of these Runnables are useful when composing custom \"chains\" in LangChain using LCEL.\\n:::\\n\\n## Benefits of LCEL\\n\\nLangChain optimizes the run-time execution of chains built with LCEL in a number of ways:\\n\\n- **Optimized parallel execution**: Run Runnables in parallel using [RunnableParallel](#runnableparallel) or run multiple inputs through a given chain in parallel using the [Runnable Batch API](/docs/concepts/runnables/#optimized-parallel-execution-batch). Parallel execution can significantly reduce the latency as processing can be done in parallel instead of sequentially.\\n- **Guaranteed Async support**: Any chain built with LCEL can be run asynchronously using the [Runnable Async API](/docs/concepts/runnables/#asynchronous-support). This can be useful when running chains in a server environment where you want to handle large number of requests concurrently.\\n- **Simplify streaming**: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(time elapsed until the first chunk of output from a [chat model](/docs/concepts/chat_models) or [llm](/docs/concepts/text_llms) comes out).\\n\\nOther benefits include:\\n\\n- [**Seamless LangSmith tracing**](https://docs.smith.langchain.com)\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, **all** steps are automatically logged to [LangSmith](https://docs.smith.langchain.com/) for maximum observability and debuggability.\\n- **Standard API**: Because all chains are built using the Runnable interface, they can be used in the same way as any other Runnable.\\n- [**Deployable with LangServe**](/docs/concepts/architecture#langserve): Chains built with LCEL can be deployed using for production use.\\n\\n## Should I use LCEL?\\n\\nLCEL is an [orchestration solution](https://en.wikipedia.org/wiki/Orchestration_(computing)) -- it allows LangChain to handle run-time execution of chains in an optimized way.\\n\\nWhile we have seen users run chains with hundreds of steps in production, we generally recommend using LCEL for simpler orchestration tasks. When the application requires complex state management, branching, cycles or multiple agents, we recommend that users take advantage of [LangGraph](/docs/concepts/architecture#langgraph).\\n\\nIn LangGraph, users define graphs that specify the application\\'s flow. This allows users to keep using LCEL within individual nodes when LCEL is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.\\n\\nHere are some guidelines:\\n\\n* If you are making a single LLM call, you don\\'t need LCEL; instead call the underlying [chat model](/docs/concepts/chat_models) directly.\\n* If you have a simple chain (e.g., prompt + llm + parser, simple retrieval set up etc.), LCEL is a reasonable fit, if you\\'re taking advantage of the LCEL benefits.\\n* If you\\'re building a complex chain (e.g., with branching, cycles, multiple agents, etc.) use [LangGraph](/docs/concepts/architecture#langgraph) instead. Remember that you can always use LCEL within individual nodes in LangGraph.\\n\\n## Composition Primitives\\n\\n`LCEL` chains are built by composing existing `Runnables` together. The two main composition primitives are [RunnableSequence](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableSequence.html#langchain_core.runnables.base.RunnableSequence) and [RunnableParallel](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableParallel.html#langchain_core.runnables.base.RunnableParallel).\\n\\nMany other composition primitives (e.g., [RunnableAssign](\\nhttps://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnableAssign.html#langchain_core.runnables.passthrough.RunnableAssign\\n)) can be thought of as variations of these two primitives.\\n\\n:::note\\nYou can find a list of all composition primitives in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html).\\n:::\\n\\n### RunnableSequence\\n\\n`RunnableSequence` is a composition primitive that allows you \"chain\" multiple runnables sequentially, with the output of one runnable serving as the input to the next.\\n\\n```python\\nfrom langchain_core.runnables import RunnableSequence\\nchain = RunnableSequence([runnable1, runnable2])\\n```\\n\\nInvoking the `chain` with some input:\\n\\n```python\\nfinal_output = chain.invoke(some_input)\\n```\\n\\ncorresponds to the following:\\n\\n```python\\noutput1 = runnable1.invoke(some_input)\\nfinal_output = runnable2.invoke(output1)\\n```\\n\\n:::note\\n`runnable1` and `runnable2` are placeholders for any `Runnable` that you want to chain together.\\n:::\\n\\n### RunnableParallel\\n\\n`RunnableParallel` is a composition primitive that allows you to run multiple runnables concurrently, with the same input provided to each.\\n\\n```python\\nfrom langchain_core.runnables import RunnableParallel\\nchain = RunnableParallel({\\n    \"key1\": runnable1,\\n    \"key2\": runnable2,\\n})\\n```\\n\\nInvoking the `chain` with some input:\\n\\n```python\\nfinal_output = chain.invoke(some_input)\\n```\\n\\nWill yield a `final_output` dictionary with the same keys as the input dictionary, but with the values replaced by the output of the corresponding runnable.\\n\\n```python\\n{\\n    \"key1\": runnable1.invoke(some_input),\\n    \"key2\": runnable2.invoke(some_input),\\n}\\n```\\n\\nRecall, that the runnables are executed in parallel, so while the result is the same as\\ndictionary comprehension shown above, the execution time is much faster.\\n\\n:::note\\n`RunnableParallel`supports both synchronous and asynchronous execution (as all `Runnables` do).\\n\\n* For synchronous execution, `RunnableParallel` uses a [ThreadPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor) to run the runnables concurrently.\\n* For asynchronous execution, `RunnableParallel` uses [asyncio.gather](https://docs.python.org/3/library/asyncio.html#asyncio.gather) to run the runnables concurrently.\\n:::\\n\\n## Composition Syntax\\n\\nThe usage of `RunnableSequence` and `RunnableParallel` is so common that we created a shorthand syntax for using them. This helps\\nto make the code more readable and concise.\\n\\n### The `|` operator\\n\\nWe have [overloaded](https://docs.python.org/3/reference/datamodel.html#special-method-names) the `|` operator to create a `RunnableSequence` from two `Runnables`.\\n\\n```python\\nchain = runnable1 | runnable2\\n```\\n\\nis Equivalent to:\\n\\n```python\\nchain = RunnableSequence([runnable1, runnable2])\\n```\\n\\n### The `.pipe` method\\n\\nIf you have moral qualms with operator overloading, you can use the `.pipe` method instead. This is equivalent to the `|` operator.\\n\\n```python\\nchain = runnable1.pipe(runnable2)\\n```\\n\\n### Coercion\\n\\nLCEL applies automatic type coercion to make it easier to compose chains.\\n\\nIf you do not understand the type coercion, you can always use the `RunnableSequence` and `RunnableParallel` classes directly.\\n\\nThis will make the code more verbose, but it will also make it more explicit.\\n\\n#### Dictionary to RunnableParallel\\n\\nInside an LCEL expression, a dictionary is automatically converted to a `RunnableParallel`.\\n\\nFor example, the following code:\\n\\n```python\\nmapping = {\\n    \"key1\": runnable1,\\n    \"key2\": runnable2,\\n}\\n\\nchain = mapping | runnable3\\n```\\n\\nIt gets automatically converted to the following:\\n\\n```python\\nchain = RunnableSequence([RunnableParallel(mapping), runnable3])\\n```\\n\\n:::caution\\nYou have to be careful because the `mapping` dictionary is not a `RunnableParallel` object, it is just a dictionary. This means that the following code will raise an `AttributeError`:\\n\\n```python\\nmapping.invoke(some_input)\\n```\\n:::\\n\\n#### Function to RunnableLambda\\n\\nInside an LCEL expression, a function is automatically converted to a `RunnableLambda`.\\n\\n```\\ndef some_func(x):\\n    return x\\n\\nchain = some_func | runnable1\\n```\\n\\nIt gets automatically converted to the following:\\n\\n```python\\nchain = RunnableSequence([RunnableLambda(some_func), runnable1])\\n```\\n\\n:::caution\\nYou have to be careful because the lambda function is not a `RunnableLambda` object, it is just a function. This means that the following code will raise an `AttributeError`:\\n\\n```python\\nlambda x: x + 1.invoke(some_input)\\n```\\n:::\\n\\n## Legacy chains\\n\\nLCEL aims to provide consistency around behavior and customization over legacy subclassed chains such as `LLMChain` and\\n`ConversationalRetrievalChain`. Many of these legacy chains hide important details like prompts, and as a wider variety\\nof viable models emerge, customization has become more and more important.\\n\\nIf you are currently using one of these legacy chains, please see [this guide for guidance on how to migrate](/docs/versions/migrating_chains).\\n\\nFor guides on how to do specific tasks with LCEL, check out [the relevant how-to guides](/docs/how_to/#langchain-expression-language-lcel).\\n'), Document(metadata={'file_name': 'index.mdx', 'file_path': 'docs\\\\docs\\\\how_to\\\\index.mdx', 'file_type': '.mdx', 'source': 'docs\\\\docs\\\\how_to\\\\index.mdx'}, page_content='---\\nsidebar_position: 0\\nsidebar_class_name: hidden\\n---\\n\\n# How-to guides\\n\\nHere you‚Äôll find answers to ‚ÄúHow do I‚Ä¶.?‚Äù types of questions.\\nThese guides are *goal-oriented* and *concrete*; they\\'re meant to help you complete a specific task.\\nFor conceptual explanations see the [Conceptual guide](/docs/concepts/).\\nFor end-to-end walkthroughs see [Tutorials](/docs/tutorials).\\nFor comprehensive descriptions of every class and function see the [API Reference](https://python.langchain.com/api_reference/).\\n\\n## Installation\\n\\n- [How to: install LangChain packages](/docs/how_to/installation/)\\n- [How to: use LangChain with different Pydantic versions](/docs/how_to/pydantic_compatibility)\\n\\n## Key features\\n\\nThis highlights functionality that is core to using LangChain.\\n\\n- [How to: return structured data from a model](/docs/how_to/structured_output/)\\n- [How to: use a model to call tools](/docs/how_to/tool_calling)\\n- [How to: stream runnables](/docs/how_to/streaming)\\n- [How to: debug your LLM apps](/docs/how_to/debugging/)\\n\\n## Components\\n\\nThese are the core building blocks you can use when building applications.\\n\\n### Chat models\\n\\n[Chat Models](/docs/concepts/chat_models) are newer forms of language models that take messages in and output a message.\\nSee [supported integrations](/docs/integrations/chat/) for details on getting started with chat models from a specific provider.\\n\\n- [How to: do function/tool calling](/docs/how_to/tool_calling)\\n- [How to: get models to return structured output](/docs/how_to/structured_output)\\n- [How to: cache model responses](/docs/how_to/chat_model_caching)\\n- [How to: get log probabilities](/docs/how_to/logprobs)\\n- [How to: create a custom chat model class](/docs/how_to/custom_chat_model)\\n- [How to: stream a response back](/docs/how_to/chat_streaming)\\n- [How to: track token usage](/docs/how_to/chat_token_usage_tracking)\\n- [How to: track response metadata across providers](/docs/how_to/response_metadata)\\n- [How to: use chat model to call tools](/docs/how_to/tool_calling)\\n- [How to: stream tool calls](/docs/how_to/tool_streaming)\\n- [How to: handle rate limits](/docs/how_to/chat_model_rate_limiting)\\n- [How to: few shot prompt tool behavior](/docs/how_to/tools_few_shot)\\n- [How to: bind model-specific formatted tools](/docs/how_to/tools_model_specific)\\n- [How to: force a specific tool call](/docs/how_to/tool_choice)\\n- [How to: work with local models](/docs/how_to/local_llms)\\n- [How to: init any model in one line](/docs/how_to/chat_models_universal_init/)\\n- [How to: pass multimodal data directly to models](/docs/how_to/multimodal_inputs/)\\n\\n### Messages\\n\\n[Messages](/docs/concepts/messages) are the input and output of chat models. They have some `content` and a `role`, which describes the source of the message.\\n\\n- [How to: trim messages](/docs/how_to/trim_messages/)\\n- [How to: filter messages](/docs/how_to/filter_messages/)\\n- [How to: merge consecutive messages of the same type](/docs/how_to/merge_message_runs/)\\n\\n### Prompt templates\\n\\n[Prompt Templates](/docs/concepts/prompt_templates) are responsible for formatting user input into a format that can be passed to a language model.\\n\\n- [How to: use few shot examples](/docs/how_to/few_shot_examples)\\n- [How to: use few shot examples in chat models](/docs/how_to/few_shot_examples_chat/)\\n- [How to: partially format prompt templates](/docs/how_to/prompts_partial)\\n- [How to: compose prompts together](/docs/how_to/prompts_composition)\\n- [How to: use multimodal prompts](/docs/how_to/multimodal_prompts/)\\n\\n### Example selectors\\n\\n[Example Selectors](/docs/concepts/example_selectors) are responsible for selecting the correct few shot examples to pass to the prompt.\\n\\n- [How to: use example selectors](/docs/how_to/example_selectors)\\n- [How to: select examples by length](/docs/how_to/example_selectors_length_based)\\n- [How to: select examples by semantic similarity](/docs/how_to/example_selectors_similarity)\\n- [How to: select examples by semantic ngram overlap](/docs/how_to/example_selectors_ngram)\\n- [How to: select examples by maximal marginal relevance](/docs/how_to/example_selectors_mmr)\\n- [How to: select examples from LangSmith few-shot datasets](/docs/how_to/example_selectors_langsmith/)\\n\\n### LLMs\\n\\nWhat LangChain calls [LLMs](/docs/concepts/text_llms) are older forms of language models that take a string in and output a string.\\n\\n- [How to: cache model responses](/docs/how_to/llm_caching)\\n- [How to: create a custom LLM class](/docs/how_to/custom_llm)\\n- [How to: stream a response back](/docs/how_to/streaming_llm)\\n- [How to: track token usage](/docs/how_to/llm_token_usage_tracking)\\n- [How to: work with local models](/docs/how_to/local_llms)\\n\\n### Output parsers\\n\\n[Output Parsers](/docs/concepts/output_parsers) are responsible for taking the output of an LLM and parsing into more structured format.\\n\\n- [How to: parse text from message objects](/docs/how_to/output_parser_string)\\n- [How to: use output parsers to parse an LLM response into structured format](/docs/how_to/output_parser_structured)\\n- [How to: parse JSON output](/docs/how_to/output_parser_json)\\n- [How to: parse XML output](/docs/how_to/output_parser_xml)\\n- [How to: parse YAML output](/docs/how_to/output_parser_yaml)\\n- [How to: retry when output parsing errors occur](/docs/how_to/output_parser_retry)\\n- [How to: try to fix errors in output parsing](/docs/how_to/output_parser_fixing)\\n- [How to: write a custom output parser class](/docs/how_to/output_parser_custom)\\n\\n### Document loaders\\n\\n[Document Loaders](/docs/concepts/document_loaders) are responsible for loading documents from a variety of sources.\\n\\n- [How to: load PDF files](/docs/how_to/document_loader_pdf)\\n- [How to: load web pages](/docs/how_to/document_loader_web)\\n- [How to: load CSV data](/docs/how_to/document_loader_csv)\\n- [How to: load data from a directory](/docs/how_to/document_loader_directory)\\n- [How to: load HTML data](/docs/how_to/document_loader_html)\\n- [How to: load JSON data](/docs/how_to/document_loader_json)\\n- [How to: load Markdown data](/docs/how_to/document_loader_markdown)\\n- [How to: load Microsoft Office data](/docs/how_to/document_loader_office_file)\\n- [How to: write a custom document loader](/docs/how_to/document_loader_custom)\\n\\n### Text splitters\\n\\n[Text Splitters](/docs/concepts/text_splitters) take a document and split into chunks that can be used for retrieval.\\n\\n- [How to: recursively split text](/docs/how_to/recursive_text_splitter)\\n- [How to: split HTML](/docs/how_to/split_html)\\n- [How to: split by character](/docs/how_to/character_text_splitter)\\n- [How to: split code](/docs/how_to/code_splitter)\\n- [How to: split Markdown by headers](/docs/how_to/markdown_header_metadata_splitter)\\n- [How to: recursively split JSON](/docs/how_to/recursive_json_splitter)\\n- [How to: split text into semantic chunks](/docs/how_to/semantic-chunker)\\n- [How to: split by tokens](/docs/how_to/split_by_token)\\n\\n### Embedding models\\n\\n[Embedding Models](/docs/concepts/embedding_models) take a piece of text and create a numerical representation of it.\\nSee [supported integrations](/docs/integrations/text_embedding/) for details on getting started with embedding models from a specific provider.\\n\\n- [How to: embed text data](/docs/how_to/embed_text)\\n- [How to: cache embedding results](/docs/how_to/caching_embeddings)\\n- [How to: create a custom embeddings class](/docs/how_to/custom_embeddings)\\n\\n### Vector stores\\n\\n[Vector stores](/docs/concepts/vectorstores) are databases that can efficiently store and retrieve embeddings.\\nSee [supported integrations](/docs/integrations/vectorstores/) for details on getting started with vector stores from a specific provider.\\n\\n- [How to: use a vector store to retrieve data](/docs/how_to/vectorstores)\\n\\n### Retrievers\\n\\n[Retrievers](/docs/concepts/retrievers) are responsible for taking a query and returning relevant documents.\\n\\n- [How to: use a vector store to retrieve data](/docs/how_to/vectorstore_retriever)\\n- [How to: generate multiple queries to retrieve data for](/docs/how_to/MultiQueryRetriever)\\n- [How to: use contextual compression to compress the data retrieved](/docs/how_to/contextual_compression)\\n- [How to: write a custom retriever class](/docs/how_to/custom_retriever)\\n- [How to: add similarity scores to retriever results](/docs/how_to/add_scores_retriever)\\n- [How to: combine the results from multiple retrievers](/docs/how_to/ensemble_retriever)\\n- [How to: reorder retrieved results to mitigate the \"lost in the middle\" effect](/docs/how_to/long_context_reorder)\\n- [How to: generate multiple embeddings per document](/docs/how_to/multi_vector)\\n- [How to: retrieve the whole document for a chunk](/docs/how_to/parent_document_retriever)\\n- [How to: generate metadata filters](/docs/how_to/self_query)\\n- [How to: create a time-weighted retriever](/docs/how_to/time_weighted_vectorstore)\\n- [How to: use hybrid vector and keyword retrieval](/docs/how_to/hybrid)\\n\\n### Indexing\\n\\nIndexing is the process of keeping your vectorstore in-sync with the underlying data source.\\n\\n- [How to: reindex data to keep your vectorstore in-sync with the underlying data source](/docs/how_to/indexing)\\n\\n### Tools\\n\\nLangChain [Tools](/docs/concepts/tools) contain a description of the tool (to pass to the language model) as well as the implementation of the function to call. Refer [here](/docs/integrations/tools/) for a list of pre-built tools. \\n\\n- [How to: create tools](/docs/how_to/custom_tools)\\n- [How to: use built-in tools and toolkits](/docs/how_to/tools_builtin)\\n- [How to: use chat models to call tools](/docs/how_to/tool_calling)\\n- [How to: pass tool outputs to chat models](/docs/how_to/tool_results_pass_to_model)\\n- [How to: pass run time values to tools](/docs/how_to/tool_runtime)\\n- [How to: add a human-in-the-loop for tools](/docs/how_to/tools_human)\\n- [How to: handle tool errors](/docs/how_to/tools_error)\\n- [How to: force models to call a tool](/docs/how_to/tool_choice)\\n- [How to: disable parallel tool calling](/docs/how_to/tool_calling_parallel)\\n- [How to: access the `RunnableConfig` from a tool](/docs/how_to/tool_configure)\\n- [How to: stream events from a tool](/docs/how_to/tool_stream_events)\\n- [How to: return artifacts from a tool](/docs/how_to/tool_artifacts/)\\n- [How to: convert Runnables to tools](/docs/how_to/convert_runnable_to_tool)\\n- [How to: add ad-hoc tool calling capability to models](/docs/how_to/tools_prompting)\\n- [How to: pass in runtime secrets](/docs/how_to/runnable_runtime_secrets)\\n\\n### Multimodal\\n\\n- [How to: pass multimodal data directly to models](/docs/how_to/multimodal_inputs/)\\n- [How to: use multimodal prompts](/docs/how_to/multimodal_prompts/)\\n\\n\\n### Agents\\n\\n:::note\\n\\nFor in depth how-to guides for agents, please check out [LangGraph](https://langchain-ai.github.io/langgraph/) documentation.\\n\\n:::\\n\\n- [How to: use legacy LangChain Agents (AgentExecutor)](/docs/how_to/agent_executor)\\n- [How to: migrate from legacy LangChain agents to LangGraph](/docs/how_to/migrate_agent)\\n\\n### Callbacks\\n\\n[Callbacks](/docs/concepts/callbacks) allow you to hook into the various stages of your LLM application\\'s execution.\\n\\n- [How to: pass in callbacks at runtime](/docs/how_to/callbacks_runtime)\\n- [How to: attach callbacks to a module](/docs/how_to/callbacks_attach)\\n- [How to: pass callbacks into a module constructor](/docs/how_to/callbacks_constructor)\\n- [How to: create custom callback handlers](/docs/how_to/custom_callbacks)\\n- [How to: use callbacks in async environments](/docs/how_to/callbacks_async)\\n- [How to: dispatch custom callback events](/docs/how_to/callbacks_custom_events)\\n\\n### Custom\\n\\nAll of LangChain components can easily be extended to support your own versions.\\n\\n- [How to: create a custom chat model class](/docs/how_to/custom_chat_model)\\n- [How to: create a custom LLM class](/docs/how_to/custom_llm)\\n- [How to: create a custom embeddings class](/docs/how_to/custom_embeddings)\\n- [How to: write a custom retriever class](/docs/how_to/custom_retriever)\\n- [How to: write a custom document loader](/docs/how_to/document_loader_custom)\\n- [How to: write a custom output parser class](/docs/how_to/output_parser_custom)\\n- [How to: create custom callback handlers](/docs/how_to/custom_callbacks)\\n- [How to: define a custom tool](/docs/how_to/custom_tools)\\n- [How to: dispatch custom callback events](/docs/how_to/callbacks_custom_events)\\n\\n### Serialization\\n- [How to: save and load LangChain objects](/docs/how_to/serialization)\\n\\n## Use cases\\n\\nThese guides cover use-case specific details.\\n\\n### Q&A with RAG\\n\\nRetrieval Augmented Generation (RAG) is a way to connect LLMs to external sources of data.\\nFor a high-level tutorial on RAG, check out [this guide](/docs/tutorials/rag/).\\n\\n- [How to: add chat history](/docs/how_to/qa_chat_history_how_to/)\\n- [How to: stream](/docs/how_to/qa_streaming/)\\n- [How to: return sources](/docs/how_to/qa_sources/)\\n- [How to: return citations](/docs/how_to/qa_citations/)\\n- [How to: do per-user retrieval](/docs/how_to/qa_per_user/)\\n\\n\\n### Extraction\\n\\nExtraction is when you use LLMs to extract structured information from unstructured text.\\nFor a high level tutorial on extraction, check out [this guide](/docs/tutorials/extraction/).\\n\\n- [How to: use reference examples](/docs/how_to/extraction_examples/)\\n- [How to: handle long text](/docs/how_to/extraction_long_text/)\\n- [How to: do extraction without using function calling](/docs/how_to/extraction_parse)\\n\\n### Chatbots\\n\\nChatbots involve using an LLM to have a conversation.\\nFor a high-level tutorial on building chatbots, check out [this guide](/docs/tutorials/chatbot/).\\n\\n- [How to: manage memory](/docs/how_to/chatbots_memory)\\n- [How to: do retrieval](/docs/how_to/chatbots_retrieval)\\n- [How to: use tools](/docs/how_to/chatbots_tools)\\n- [How to: manage large chat history](/docs/how_to/trim_messages/)\\n\\n### Query analysis\\n\\nQuery Analysis is the task of using an LLM to generate a query to send to a retriever.\\nFor a high-level tutorial on query analysis, check out [this guide](/docs/tutorials/rag/#query-analysis).\\n\\n- [How to: add examples to the prompt](/docs/how_to/query_few_shot)\\n- [How to: handle cases where no queries are generated](/docs/how_to/query_no_queries)\\n- [How to: handle multiple queries](/docs/how_to/query_multiple_queries)\\n- [How to: handle multiple retrievers](/docs/how_to/query_multiple_retrievers)\\n- [How to: construct filters](/docs/how_to/query_constructing_filters)\\n- [How to: deal with high cardinality categorical variables](/docs/how_to/query_high_cardinality)\\n\\n### Q&A over SQL + CSV\\n\\nYou can use LLMs to do question answering over tabular data.\\nFor a high-level tutorial, check out [this guide](/docs/tutorials/sql_qa/).\\n\\n- [How to: use prompting to improve results](/docs/how_to/sql_prompting)\\n- [How to: do query validation](/docs/how_to/sql_query_checking)\\n- [How to: deal with large databases](/docs/how_to/sql_large_db)\\n- [How to: deal with CSV files](/docs/how_to/sql_csv)\\n\\n### Q&A over graph databases\\n\\nYou can use an LLM to do question answering over graph databases.\\nFor a high-level tutorial, check out [this guide](/docs/tutorials/graph/).\\n\\n- [How to: add a semantic layer over the database](/docs/how_to/graph_semantic)\\n- [How to: construct knowledge graphs](/docs/how_to/graph_constructing)\\n\\n### Summarization\\n\\nLLMs can summarize and otherwise distill desired information from text, including\\nlarge volumes of text. For a high-level tutorial, check out [this guide](/docs/tutorials/summarization).\\n\\n- [How to: summarize text in a single LLM call](/docs/how_to/summarize_stuff)\\n- [How to: summarize text through parallelization](/docs/how_to/summarize_map_reduce)\\n- [How to: summarize text through iterative refinement](/docs/how_to/summarize_refine)\\n\\n## LangChain Expression Language (LCEL)\\n\\n:::note Should I use LCEL?\\n\\nLCEL is an orchestration solution. See our\\n[concepts page](/docs/concepts/lcel/#should-i-use-lcel) for recommendations on when to\\nuse LCEL.\\n\\n:::\\n\\n[LangChain Expression Language](/docs/concepts/lcel) is a way to create arbitrary custom chains. It is built on the [Runnable](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html) protocol.\\n\\n[**LCEL cheatsheet**](/docs/how_to/lcel_cheatsheet/): For a quick overview of how to use the main LCEL primitives.\\n\\n[**Migration guide**](/docs/versions/migrating_chains): For migrating legacy chain abstractions to LCEL.\\n\\n- [How to: chain runnables](/docs/how_to/sequence)\\n- [How to: stream runnables](/docs/how_to/streaming)\\n- [How to: invoke runnables in parallel](/docs/how_to/parallel/)\\n- [How to: add default invocation args to runnables](/docs/how_to/binding/)\\n- [How to: turn any function into a runnable](/docs/how_to/functions)\\n- [How to: pass through inputs from one chain step to the next](/docs/how_to/passthrough)\\n- [How to: configure runnable behavior at runtime](/docs/how_to/configure)\\n- [How to: add message history (memory) to a chain](/docs/how_to/message_history)\\n- [How to: route between sub-chains](/docs/how_to/routing)\\n- [How to: create a dynamic (self-constructing) chain](/docs/how_to/dynamic_chain/)\\n- [How to: inspect runnables](/docs/how_to/inspect)\\n- [How to: add fallbacks to a runnable](/docs/how_to/fallbacks)\\n- [How to: pass runtime secrets to a runnable](/docs/how_to/runnable_runtime_secrets)\\n\\n## [LangGraph](https://langchain-ai.github.io/langgraph)\\n\\nLangGraph is an extension of LangChain aimed at\\nbuilding robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.\\n\\nLangGraph documentation is currently hosted on a separate site.\\nYou can peruse [LangGraph how-to guides here](https://langchain-ai.github.io/langgraph/how-tos/).\\n\\n## [LangSmith](https://docs.smith.langchain.com/)\\n\\nLangSmith allows you to closely trace, monitor and evaluate your LLM application.\\nIt seamlessly integrates with LangChain and LangGraph, and you can use it to inspect and debug individual steps of your chains and agents as you build.\\n\\nLangSmith documentation is hosted on a separate site.\\nYou can peruse [LangSmith how-to guides here](https://docs.smith.langchain.com/), but we\\'ll highlight a few sections that are particularly\\nrelevant to LangChain below:\\n\\n### Evaluation\\n<span data-heading-keywords=\"evaluation,evaluate\"></span>\\n\\nEvaluating performance is a vital part of building LLM-powered applications.\\nLangSmith helps with every step of the process from creating a dataset to defining metrics to running evaluators.\\n\\nTo learn more, check out the [LangSmith evaluation how-to guides](https://docs.smith.langchain.com/how_to_guides#evaluation).\\n\\n### Tracing\\n<span data-heading-keywords=\"trace,tracing\"></span>\\n\\nTracing gives you observability inside your chains and agents, and is vital in diagnosing issues.\\n\\n- [How to: trace with LangChain](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain)\\n- [How to: add metadata and tags to traces](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#add-metadata-and-tags-to-traces)\\n\\nYou can see general tracing-related how-tos [in this section of the LangSmith docs](https://docs.smith.langchain.com/how_to_guides/tracing).\\n'), Document(metadata={'file_name': 'why_langchain.mdx', 'file_path': 'docs\\\\docs\\\\concepts\\\\why_langchain.mdx', 'file_type': '.mdx', 'source': 'docs\\\\docs\\\\concepts\\\\why_langchain.mdx'}, page_content='# Why LangChain?\\n\\nThe goal of `langchain` the Python package and LangChain the company is to make it as easy as possible for developers to build applications that reason.\\nWhile LangChain originally started as a single open source package, it has evolved into a company and a whole ecosystem.\\nThis page will talk about the LangChain ecosystem as a whole.\\nMost of the components within the LangChain ecosystem can be used by themselves - so if you feel particularly drawn to certain components but not others, that is totally fine! Pick and choose whichever components you like best for your own use case!\\n\\n## Features\\n\\nThere are several primary needs that LangChain aims to address:\\n\\n1. **Standardized component interfaces:** The growing number of [models](/docs/integrations/chat/) and [related components](/docs/integrations/vectorstores/) for AI applications has resulted in a wide variety of different APIs that developers need to learn and use.\\nThis diversity can make it challenging for developers to switch between providers or combine components when building applications.\\nLangChain exposes a standard interface for key components, making it easy to switch between providers.\\n\\n2. **Orchestration:** As applications become more complex, combining multiple components and models, there\\'s [a growing need to efficiently connect these elements into control flows](https://lilianweng.github.io/posts/2023-06-23-agent/) that can [accomplish diverse tasks](https://www.sequoiacap.com/article/generative-ais-act-o1/).\\n[Orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)) is crucial for building such applications.\\n\\n3. **Observability and evaluation:** As applications become more complex, it becomes increasingly difficult to understand what is happening within them.\\nFurthermore, the pace of development can become rate-limited by the [paradox of choice](https://en.wikipedia.org/wiki/Paradox_of_choice).\\nFor example, developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\n[Observability](https://en.wikipedia.org/wiki/Observability) and evaluations can help developers monitor their applications and rapidly answer these types of questions with confidence.\\n\\n\\n## Standardized component interfaces\\n\\nLangChain provides common interfaces for components that are central to many AI applications.\\nAs an example, all [chat models](/docs/concepts/chat_models/) implement the [BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) interface.\\nThis provides a standard way to interact with chat models, supporting important but often provider-specific features like [tool calling](/docs/concepts/tool_calling/) and [structured outputs](/docs/concepts/structured_outputs/).\\n\\n\\n### Example: chat models \\n\\nMany [model providers](/docs/concepts/chat_models/) support [tool calling](/docs/concepts/tool_calling/), a critical feature for many applications (e.g., [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/)), that allows a developer to request model responses that match a particular schema.\\nThe APIs for each provider differ. \\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to bind [tools](/docs/concepts/tools) to a model in order to support [tool calling](/docs/concepts/tool_calling/):\\n\\n```python\\n# Tool creation\\ntools = [my_tool]\\n# Tool binding\\nmodel_with_tools = model.bind_tools(tools)\\n```\\n\\nSimilarly, getting models to produce [structured outputs](/docs/concepts/structured_outputs/) is an extremely common use case. \\nProviders support different approaches for this, including [JSON mode or tool calling](https://platform.openai.com/docs/guides/structured-outputs), with different APIs.\\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to produce structured outputs using the `with_structured_output()` method:\\n\\n```python\\n# Define schema\\nschema = ...\\n# Bind schema to model\\nmodel_with_structure = model.with_structured_output(schema)\\n```\\n\\n### Example: retrievers\\n\\nIn the context of [RAG](/docs/concepts/rag/) and LLM application components, LangChain\\'s [retriever](/docs/concepts/retrievers/) interface provides a standard way to connect to many different types of data services or databases (e.g., [vector stores](/docs/concepts/vectorstores) or databases).\\nThe underlying implementation of the retriever depends on the type of data store or database you are connecting to, but all retrievers implement the [runnable interface](/docs/concepts/runnables/), meaning they can be invoked in a common manner.\\n\\n```python\\ndocuments = my_retriever.invoke(\"What is the meaning of life?\")\\n```\\n\\n## Orchestration \\n\\nWhile standardization for individual components is useful, we\\'ve increasingly seen that developers want to *combine* components into more complex applications. \\nThis motivates the need for [orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)).\\nThere are several common characteristics of LLM applications that this orchestration layer should support:\\n\\n* **Complex control flow:** The application requires complex patterns such as cycles (e.g., a loop that reiterates until a condition is met).\\n* **[Persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/):** The application needs to maintain [short-term and / or long-term memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n* **[Human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/):** The application needs human interaction, e.g., pausing, reviewing, editing, approving certain steps.\\n\\nThe recommended way to orchestrate components for complex applications is [LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/).\\nLangGraph is a library that gives developers a high degree of control by expressing the flow of the application as a set of nodes and edges.\\nLangGraph comes with built-in support for [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), [memory](https://langchain-ai.github.io/langgraph/concepts/memory/), and other features.\\nIt\\'s particularly well suited for building [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/) or [multi-agent](https://langchain-ai.github.io/langgraph/concepts/multi_agent/) applications. \\nImportantly, individual LangChain components can be used as LangGraph nodes, but you can also use LangGraph **without** using LangChain components.\\n\\n:::info[Further reading]\\n\\nHave a look at our free course, [Introduction to LangGraph](https://academy.langchain.com/courses/intro-to-langgraph), to learn more about how to use LangGraph to build complex applications.\\n\\n:::\\n\\n## Observability and evaluation\\n\\nThe pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. \\nDevelopers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\nHigh quality tracing and evaluations can help you rapidly answer these types of questions with confidence.\\n[LangSmith](https://docs.smith.langchain.com/) is our platform that supports observability and evaluation for AI applications.\\nSee our conceptual guides on [evaluations](https://docs.smith.langchain.com/concepts/evaluation) and [tracing](https://docs.smith.langchain.com/concepts/tracing) for more details.\\n\\n:::info[Further reading]\\n\\nSee our video playlist on [LangSmith tracing and evaluations](https://youtube.com/playlist?list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S&feature=shared) for more details.\\n\\n:::\\n\\n## Conclusion\\n\\nLangChain offers standard interfaces for components that are central to many AI applications, which offers a few specific advantages:\\n- **Ease of swapping providers:** It allows you to swap out different component providers without having to change the underlying code.\\n- **Advanced features:** It provides common methods for more advanced features, such as [streaming](/docs/concepts/streaming) and [tool calling](/docs/concepts/tool_calling/).\\n\\n[LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/) makes it possible to orchestrate complex applications (e.g., [agents](/docs/concepts/agents/)) and provide features like including [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), or [memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n\\n[LangSmith](https://docs.smith.langchain.com/) makes it possible to iterate with confidence on your applications, by providing LLM-specific observability and framework for testing and evaluating your application.\\n')]\n",
      "rank:0\n",
      "rank:1\n",
      "rank:2\n",
      "rank:3\n",
      "docs:[Document(metadata={'source': 'docs\\\\docs\\\\integrations\\\\retrievers\\\\self_query\\\\index.mdx', 'file_path': 'docs\\\\docs\\\\integrations\\\\retrievers\\\\self_query\\\\index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='---\\nsidebar-position: 0\\n---\\n\\n# Self-querying retrievers\\n\\nLearn about how the self-querying retriever works [here](/docs/how_to/self_query).\\n\\nimport DocCardList from \"@theme/DocCardList\";\\n\\n<DocCardList />\\n'), Document(metadata={'source': 'docs\\\\docs\\\\integrations\\\\providers\\\\helicone.mdx', 'file_path': 'docs\\\\docs\\\\integrations\\\\providers\\\\helicone.mdx', 'file_name': 'helicone.mdx', 'file_type': '.mdx'}, page_content='# Helicone\\n\\nThis page covers how to use the [Helicone](https://helicone.ai) ecosystem within LangChain.\\n\\n## What is Helicone?\\n\\nHelicone is an [open-source](https://github.com/Helicone/helicone) observability platform that proxies your OpenAI traffic and provides you key insights into your spend, latency and usage.\\n\\n![Screenshot of the Helicone dashboard showing average requests per day, response time, tokens per response, total cost, and a graph of requests over time.](/img/HeliconeDashboard.png \"Helicone Dashboard\")\\n\\n## Quick start\\n\\nWith your LangChain environment you can just add the following parameter.\\n\\n```bash\\nexport OPENAI_API_BASE=\"https://oai.hconeai.com/v1\"\\n```\\n\\nNow head over to [helicone.ai](https://www.helicone.ai/signup) to create your account, and add your OpenAI API key within our dashboard to view your logs.\\n\\n![Interface for entering and managing OpenAI API keys in the Helicone dashboard.](/img/HeliconeKeys.png \"Helicone API Key Input\")\\n\\n## How to enable Helicone caching\\n\\n```python\\nfrom langchain_openai import OpenAI\\nimport openai\\nopenai.api_base = \"https://oai.hconeai.com/v1\"\\n\\nllm = OpenAI(temperature=0.9, headers={\"Helicone-Cache-Enabled\": \"true\"})\\ntext = \"What is a helicone?\"\\nprint(llm.invoke(text))\\n```\\n\\n[Helicone caching docs](https://docs.helicone.ai/advanced-usage/caching)\\n\\n## How to use Helicone custom properties\\n\\n```python\\nfrom langchain_openai import OpenAI\\nimport openai\\nopenai.api_base = \"https://oai.hconeai.com/v1\"\\n\\nllm = OpenAI(temperature=0.9, headers={\\n        \"Helicone-Property-Session\": \"24\",\\n        \"Helicone-Property-Conversation\": \"support_issue_2\",\\n        \"Helicone-Property-App\": \"mobile\",\\n      })\\ntext = \"What is a helicone?\"\\nprint(llm.invoke(text))\\n```\\n\\n[Helicone property docs](https://docs.helicone.ai/advanced-usage/custom-properties)\\n'), Document(metadata={'source': 'docs\\\\docs\\\\integrations\\\\providers\\\\cohere.mdx', 'file_path': 'docs\\\\docs\\\\integrations\\\\providers\\\\cohere.mdx', 'file_name': 'cohere.mdx', 'file_type': '.mdx'}, page_content='# Cohere\\n\\n>[Cohere](https://cohere.ai/about) is a Canadian startup that provides natural language processing models\\n> that help companies improve human-machine interactions.\\n\\n## Installation and Setup\\n- Install the Python SDK :\\n```bash\\npip install langchain-cohere\\n```\\n\\nGet a [Cohere api key](https://dashboard.cohere.ai/) and set it as an environment variable (`COHERE_API_KEY`)\\n\\n## Cohere langchain integrations\\n\\n|API|description|Endpoint docs|Import|Example usage|\\n|---|---|---|---|---|\\n|Chat|Build chat bots|[chat](https://docs.cohere.com/reference/chat)|`from langchain_cohere import ChatCohere`|[cohere.ipynb](/docs/integrations/chat/cohere)|\\n|LLM|Generate text|[generate](https://docs.cohere.com/reference/generate)|`from langchain_cohere.llms import Cohere`|[cohere.ipynb](/docs/integrations/llms/cohere)|\\n|RAG Retriever|Connect to external data sources|[chat + rag](https://docs.cohere.com/reference/chat)|`from langchain.retrievers import CohereRagRetriever`|[cohere.ipynb](/docs/integrations/retrievers/cohere)|\\n|Text Embedding|Embed strings to vectors|[embed](https://docs.cohere.com/reference/embed)|`from langchain_cohere import CohereEmbeddings`|[cohere.ipynb](/docs/integrations/text_embedding/cohere)|\\n|Rerank Retriever|Rank strings based on relevance|[rerank](https://docs.cohere.com/reference/rerank)|`from langchain.retrievers.document_compressors import CohereRerank`|[cohere.ipynb](/docs/integrations/retrievers/cohere-reranker)|\\n\\n## Quick copy examples\\n\\n### Chat\\n\\n```python\\nfrom langchain_cohere import ChatCohere\\nfrom langchain_core.messages import HumanMessage\\nchat = ChatCohere()\\nmessages = [HumanMessage(content=\"knock knock\")]\\nprint(chat.invoke(messages))\\n```\\n\\nUsage of the Cohere [chat model](/docs/integrations/chat/cohere)\\n\\n### LLM\\n\\n\\n```python\\nfrom langchain_cohere.llms import Cohere\\n\\nllm = Cohere()\\nprint(llm.invoke(\"Come up with a pet name\"))\\n```\\n\\nUsage of the Cohere (legacy) [LLM model](/docs/integrations/llms/cohere) \\n\\n### Tool calling\\n```python\\nfrom langchain_cohere import ChatCohere\\nfrom langchain_core.messages import (\\n    HumanMessage,\\n    ToolMessage,\\n)\\nfrom langchain_core.tools import tool\\n\\n@tool\\ndef magic_function(number: int) -> int:\\n    \"\"\"Applies a magic operation to an integer\\n\\n    Args:\\n        number: Number to have magic operation performed on\\n    \"\"\"\\n    return number + 10\\n\\ndef invoke_tools(tool_calls, messages):\\n    for tool_call in tool_calls:\\n        selected_tool = {\"magic_function\":magic_function}[\\n            tool_call[\"name\"].lower()\\n        ]\\n        tool_output = selected_tool.invoke(tool_call[\"args\"])\\n        messages.append(ToolMessage(tool_output, tool_call_id=tool_call[\"id\"]))\\n    return messages\\n\\ntools = [magic_function]\\n\\nllm = ChatCohere()\\nllm_with_tools = llm.bind_tools(tools=tools)\\nmessages = [\\n    HumanMessage(\\n        content=\"What is the value of magic_function(2)?\"\\n    )\\n]\\n\\nres = llm_with_tools.invoke(messages)\\nwhile res.tool_calls:\\n    messages.append(res)\\n    messages = invoke_tools(res.tool_calls, messages)\\n    res = llm_with_tools.invoke(messages)\\n\\nprint(res.content)\\n```\\nTool calling with Cohere LLM can be done by binding the necessary tools to the llm as seen above. \\nAn alternative, is to support multi hop tool calling with the ReAct agent as seen below.\\n\\n### ReAct Agent\\n\\nThe agent is based on the paper\\n[ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629).\\n\\n```python\\nfrom langchain_community.tools.tavily_search import TavilySearchResults\\nfrom langchain_cohere import ChatCohere, create_cohere_react_agent\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain.agents import AgentExecutor\\n\\nllm = ChatCohere()\\n\\ninternet_search = TavilySearchResults(max_results=4)\\ninternet_search.name = \"internet_search\"\\ninternet_search.description = \"Route a user query to the internet\"\\n\\nprompt = ChatPromptTemplate.from_template(\"{input}\")\\n\\nagent = create_cohere_react_agent(\\n    llm,\\n    [internet_search],\\n    prompt\\n)\\n\\nagent_executor = AgentExecutor(agent=agent, tools=[internet_search], verbose=True)\\n\\nagent_executor.invoke({\\n    \"input\": \"In what year was the company that was founded as Sound of Music added to the S&P 500?\",\\n})\\n```\\nThe ReAct agent can be used to call multiple tools in sequence.\\n\\n### RAG Retriever\\n\\n```python\\nfrom langchain_cohere import ChatCohere\\nfrom langchain.retrievers import CohereRagRetriever\\nfrom langchain_core.documents import Document\\n\\nrag = CohereRagRetriever(llm=ChatCohere())\\nprint(rag.invoke(\"What is cohere ai?\"))\\n```\\n\\nUsage of the Cohere [RAG Retriever](/docs/integrations/retrievers/cohere)\\n\\n### Text Embedding\\n\\n```python\\nfrom langchain_cohere import CohereEmbeddings\\n\\nembeddings = CohereEmbeddings(model=\"embed-english-light-v3.0\")\\nprint(embeddings.embed_documents([\"This is a test document.\"]))\\n```\\n\\nUsage of the Cohere [Text Embeddings model](/docs/integrations/text_embedding/cohere)\\n\\n### Reranker\\n\\nUsage of the Cohere [Reranker](/docs/integrations/retrievers/cohere-reranker)'), Document(metadata={'source': 'docs\\\\docs\\\\integrations\\\\providers\\\\college_confidential.mdx', 'file_path': 'docs\\\\docs\\\\integrations\\\\providers\\\\college_confidential.mdx', 'file_name': 'college_confidential.mdx', 'file_type': '.mdx'}, page_content=\"# College Confidential\\n\\n>[College Confidential](https://www.collegeconfidential.com/) gives information on 3,800+ colleges and universities.\\n\\n## Installation and Setup\\n\\nThere isn't any special setup for it.\\n\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/college_confidential).\\n\\n```python\\nfrom langchain_community.document_loaders import CollegeConfidentialLoader\\n```\\n\")]\n",
      "rank:0\n",
      "rank:1\n",
      "rank:2\n",
      "rank:3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LangChain„ÅØ„ÄÅÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÔºàLLMÔºâ„ÇíÊ¥ªÁî®„Åó„Åü„Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥„ÇíÈñãÁô∫„Åô„Çã„Åü„ÇÅ„ÅÆ„Éï„É¨„Éº„É†„ÉØ„Éº„ÇØ„Åß„Åô„ÄÇ„Åì„ÅÆ„Éï„É¨„Éº„É†„ÉØ„Éº„ÇØ„ÅØ„ÄÅLLM„Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥„ÅÆ„É©„Ç§„Éï„Çµ„Ç§„ÇØ„É´„ÅÆÂêÑ„Çπ„ÉÜ„Éº„Ç∏„ÇíÁ∞°Á¥†Âåñ„Åó„Åæ„Åô„ÄÇÂÖ∑‰ΩìÁöÑ„Å´„ÅØ„ÄÅ‰ª•‰∏ã„ÅÆ„Çà„ÅÜ„Å™Ê©üËÉΩ„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ\\n\\n1. **ÈñãÁô∫**: LangChain„ÅÆ„Ç™„Éº„Éó„É≥„ÇΩ„Éº„Çπ„Ç≥„É≥„Éù„Éº„Éç„É≥„Éà„ÇÑ„Çµ„Éº„Éâ„Éë„Éº„ÉÜ„Ç£Áµ±Âêà„Çí‰ΩøÁî®„Åó„Å¶„Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥„ÇíÊßãÁØâ„Åß„Åç„Åæ„Åô„ÄÇLangGraph„ÇíÂà©Áî®„Åô„Çã„Åì„Å®„Åß„ÄÅÁä∂ÊÖã„ÇíÊåÅ„Å§„Ç®„Éº„Ç∏„Çß„É≥„Éà„ÇíÊßãÁØâ„Åó„ÄÅ„Çπ„Éà„É™„Éº„Éü„É≥„Ç∞„ÇÑ‰∫∫Èñì„ÅÆ‰ªãÂÖ•„Çí„Çµ„Éù„Éº„Éà„Åó„Åæ„Åô„ÄÇ\\n\\n2. **ÁîüÁî£Âåñ**: LangSmith„Çí‰ΩøÁî®„Åó„Å¶„Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥„ÇíÊ§úÊüª„ÄÅÁõ£Ë¶ñ„ÄÅË©ï‰æ°„Åó„ÄÅÁ∂ôÁ∂öÁöÑ„Å´ÊúÄÈÅ©Âåñ„Åó„Å¶Ëá™‰ø°„ÇíÊåÅ„Å£„Å¶„Éá„Éó„É≠„Ç§„Åß„Åç„Åæ„Åô„ÄÇ\\n\\n3. **„Éá„Éó„É≠„Ç§**: LangGraph„Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥„ÇíÁîüÁî£Ê∫ñÂÇô„ÅåÊï¥„Å£„ÅüAPI„ÇÑ„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„Å´Â§âÊèõ„Åß„Åç„Åæ„Åô„ÄÇ\\n\\nLangChain„ÅØ„ÄÅ„Åï„Åæ„Åñ„Åæ„Å™„Éó„É≠„Éê„Ç§„ÉÄ„Éº„Å®Áµ±Âêà„Åó„ÄÅLLM„ÇÑÈñ¢ÈÄ£ÊäÄË°ìÔºàÂüã„ÇÅËæº„Åø„É¢„Éá„É´„ÇÑ„Éô„ÇØ„Éà„É´„Çπ„Éà„Ç¢Ôºâ„Å´ÂØæ„Åô„ÇãÊ®ôÊ∫ñ„Ç§„É≥„Çø„Éº„Éï„Çß„Éº„Çπ„ÇíÂÆüË£Ö„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Åæ„Åü„ÄÅË§áÊï∞„ÅÆ„Ç™„Éº„Éó„É≥„ÇΩ„Éº„Çπ„É©„Ç§„Éñ„É©„É™„ÅßÊßãÊàê„Åï„Çå„Å¶„Åä„Çä„ÄÅ„É¶„Éº„Ç∂„Éº„ÅØÁâπÂÆö„ÅÆ„Éã„Éº„Ç∫„Å´Âøú„Åò„Å¶„Ç≥„É≥„Éù„Éº„Éç„É≥„Éà„ÇíÈÅ∏Êäû„Åó„Å¶‰ΩøÁî®„Åß„Åç„Åæ„Åô„ÄÇ'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_rag_chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"context\": hybrid_retriever,\n",
    "    }\n",
    "    | prompt | model | StrOutputParser()\n",
    ")\n",
    "\n",
    "hybrid_rag_chain.invoke(\"LangChain„ÅÆÊ¶ÇË¶Å„ÇíÊïô„Åà„Å¶\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
