{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-1. 環境設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LANGSMITH_TRACING_V2: None\n",
      "LANGSMITH_ENDPOINT: https://api.smith.langchain.com\n",
      "LANGSMITH_PROJECT: agent-book\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# .env ファイルを読み込む\n",
    "path_env=\"C:\\\\Users\\\\ktgu1\\\\.env\"\n",
    "load_dotenv(path_env)\n",
    "\n",
    "# 環境変数を取得\n",
    "langsmith_tracing_v2 = os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
    "langsmith_endpoint = os.getenv(\"LANGSMITH_ENDPOINT\")\n",
    "langsmith_api_key = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "langsmith_project = os.getenv(\"LANGSMITH_PROJECT\")\n",
    "\n",
    "# Langsmithがただしく読み込めているか確認。なぜかtracingがNoneだができてる\n",
    "print(f\"LANGSMITH_TRACING_V2: {langsmith_tracing_v2}\")\n",
    "print(f\"LANGSMITH_ENDPOINT: {langsmith_endpoint}\")\n",
    "print(f\"LANGSMITH_PROJECT: {langsmith_project}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-core==0.3.0\n",
      "  Using cached langchain_core-0.3.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting langchain-openai==0.2.0\n",
      "  Using cached langchain_openai-0.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting langchain-community==0.3.0\n",
      "  Using cached langchain_community-0.3.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: GitPython==3.1.43 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (3.1.43)\n",
      "Collecting langchain-chroma==0.1.4\n",
      "  Using cached langchain_chroma-0.1.4-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting tavily-python==0.5.0\n",
      "  Using cached tavily_python-0.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pydantic==2.10.6\n",
      "  Using cached pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from langchain-core==0.3.0) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from langchain-core==0.3.0) (1.33)\n",
      "Collecting langsmith<0.2.0,>=0.1.117 (from langchain-core==0.3.0)\n",
      "  Using cached langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from langchain-core==0.3.0) (24.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from langchain-core==0.3.0) (8.2.3)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from langchain-core==0.3.0) (4.11.0)\n",
      "Collecting openai<2.0.0,>=1.40.0 (from langchain-openai==0.2.0)\n",
      "  Using cached openai-1.86.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain-openai==0.2.0)\n",
      "  Using cached tiktoken-0.9.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from langchain-community==0.3.0) (2.0.34)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from langchain-community==0.3.0) (3.10.5)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community==0.3.0)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting langchain<0.4.0,>=0.3.0 (from langchain-community==0.3.0)\n",
      "  Using cached langchain-0.3.25-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from langchain-community==0.3.0) (1.26.4)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community==0.3.0)\n",
      "  Using cached pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from langchain-community==0.3.0) (2.32.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from GitPython==3.1.43) (4.0.7)\n",
      "Collecting chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0 (from langchain-chroma==0.1.4)\n",
      "  Using cached chromadb-0.5.23-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting fastapi<1,>=0.95.2 (from langchain-chroma==0.1.4)\n",
      "  Using cached fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: httpx in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from tavily-python==0.5.0) (0.27.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from pydantic==2.10.6) (0.6.0)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic==2.10.6)\n",
      "  Using cached pydantic_core-2.27.2-cp312-cp312-win_amd64.whl.metadata (6.7 kB)\n",
      "Collecting typing-extensions>=4.7 (from langchain-core==0.3.0)\n",
      "  Using cached typing_extensions-4.14.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0) (1.11.0)\n",
      "Collecting build>=1.0.3 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting chroma-hnswlib==0.7.6 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached chroma_hnswlib-0.7.6.tar.gz (32 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached uvicorn-0.34.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting posthog>=2.4.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached posthog-4.10.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached onnxruntime-1.22.0-cp312-cp312-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached opentelemetry_api-1.34.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached opentelemetry_instrumentation_fastapi-0.55b1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached opentelemetry_sdk-1.34.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting tokenizers<=0.20.3,>=0.13.2 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached tokenizers-0.20.3-cp312-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting pypika>=0.48.9 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached pypika-0.48.9-py2.py3-none-any.whl\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (4.66.5)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (7.4.0)\n",
      "Collecting importlib-resources (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting grpcio>=1.58.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached grpcio-1.73.0-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached bcrypt-4.3.0-cp39-abi3-win_amd64.whl.metadata (10 kB)\n",
      "Collecting typer>=0.9.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached typer-0.16.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting mmh3>=4.0.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached mmh3-5.1.0-cp312-cp312-win_amd64.whl.metadata (16 kB)\n",
      "Collecting orjson>=3.9.12 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached orjson-3.10.18-cp312-cp312-win_amd64.whl.metadata (43 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (13.7.1)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.0)\n",
      "  Using cached marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.0)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting starlette<0.47.0,>=0.40.0 (from fastapi<1,>=0.95.2->langchain-chroma==0.1.4)\n",
      "  Using cached starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython==3.1.43) (4.0.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from httpx->tavily-python==0.5.0) (4.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from httpx->tavily-python==0.5.0) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from httpx->tavily-python==0.5.0) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from httpx->tavily-python==0.5.0) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from httpx->tavily-python==0.5.0) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx->tavily-python==0.5.0) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core==0.3.0) (2.1)\n",
      "INFO: pip is looking at multiple versions of langchain to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain<0.4.0,>=0.3.0 (from langchain-community==0.3.0)\n",
      "  Using cached langchain-0.3.24-py3-none-any.whl.metadata (7.8 kB)\n",
      "  Using cached langchain-0.3.23-py3-none-any.whl.metadata (7.8 kB)\n",
      "  Using cached langchain-0.3.22-py3-none-any.whl.metadata (7.8 kB)\n",
      "  Using cached langchain-0.3.21-py3-none-any.whl.metadata (7.8 kB)\n",
      "  Using cached langchain-0.3.20-py3-none-any.whl.metadata (7.7 kB)\n",
      "  Using cached langchain-0.3.19-py3-none-any.whl.metadata (7.9 kB)\n",
      "  Using cached langchain-0.3.18-py3-none-any.whl.metadata (7.8 kB)\n",
      "INFO: pip is still looking at multiple versions of langchain to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached langchain-0.3.17-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Using cached langchain-0.3.16-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Using cached langchain-0.3.15-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Using cached langchain-0.3.14-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Using cached langchain-0.3.13-py3-none-any.whl.metadata (7.1 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached langchain-0.3.12-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Using cached langchain-0.3.11-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Using cached langchain-0.3.10-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Using cached langchain-0.3.9-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Using cached langchain-0.3.8-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Using cached langchain-0.3.7-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Using cached langchain-0.3.6-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Using cached langchain-0.3.5-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Using cached langchain-0.3.4-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Using cached langchain-0.3.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Using cached langchain-0.3.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Using cached langchain-0.3.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Using cached langchain-0.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain<0.4.0,>=0.3.0->langchain-community==0.3.0)\n",
      "  Using cached langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.117->langchain-core==0.3.0) (1.0.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.40.0->langchain-openai==0.2.0) (1.9.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.40.0->langchain-openai==0.2.0)\n",
      "  Using cached jiter-0.10.0-cp312-cp312-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community==0.3.0) (0.21.0)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community==0.3.0)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain-community==0.3.0) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain-community==0.3.0) (2.2.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai==0.2.0) (2024.9.11)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.4.6)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.9.0.post0)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.8.0)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
      "INFO: pip is looking at multiple versions of langchain-text-splitters to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain<0.4.0,>=0.3.0->langchain-community==0.3.0)\n",
      "  Using cached langchain_text_splitters-0.3.7-py3-none-any.whl.metadata (1.9 kB)\n",
      "  Using cached langchain_text_splitters-0.3.6-py3-none-any.whl.metadata (1.9 kB)\n",
      "  Using cached langchain_text_splitters-0.3.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "  Using cached langchain_text_splitters-0.3.4-py3-none-any.whl.metadata (2.3 kB)\n",
      "  Using cached langchain_text_splitters-0.3.3-py3-none-any.whl.metadata (2.3 kB)\n",
      "  Using cached langchain_text_splitters-0.3.2-py3-none-any.whl.metadata (2.3 kB)\n",
      "  Using cached langchain_text_splitters-0.3.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "INFO: pip is still looking at multiple versions of langchain-text-splitters to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Requirement already satisfied: protobuf in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (4.25.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.13.2)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (7.0.1)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-proto==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached opentelemetry_proto-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached protobuf-5.29.5-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.55b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached opentelemetry_instrumentation_asgi-0.55b1-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting opentelemetry-instrumentation==0.55b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached opentelemetry_instrumentation-0.55b1-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.55b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-util-http==0.55b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached opentelemetry_util_http-0.55b1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation==0.55b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.14.1)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.55b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.15.1)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached huggingface_hub-0.33.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.0) (1.0.0)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached httptools-0.6.4-cp312-cp312-win_amd64.whl.metadata (3.7 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached watchfiles-1.0.5-cp312-cp312-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached websockets-15.0.1-cp312-cp312-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2024.6.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.17.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.1.0)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.3.0)\n",
      "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached pyreadline3-3.5.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\ktgu1\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.4.8)\n",
      "Using cached langchain_core-0.3.0-py3-none-any.whl (405 kB)\n",
      "Using cached langchain_openai-0.2.0-py3-none-any.whl (51 kB)\n",
      "Using cached langchain_community-0.3.0-py3-none-any.whl (2.3 MB)\n",
      "Using cached langchain_chroma-0.1.4-py3-none-any.whl (10 kB)\n",
      "Using cached tavily_python-0.5.0-py3-none-any.whl (14 kB)\n",
      "Using cached pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Using cached pydantic_core-2.27.2-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "Using cached chromadb-0.5.23-py3-none-any.whl (628 kB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
      "Using cached langchain-0.3.0-py3-none-any.whl (1.0 MB)\n",
      "Using cached langsmith-0.1.147-py3-none-any.whl (311 kB)\n",
      "Using cached openai-1.86.0-py3-none-any.whl (730 kB)\n",
      "Using cached pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
      "Using cached tiktoken-0.9.0-cp312-cp312-win_amd64.whl (894 kB)\n",
      "Using cached typing_extensions-4.14.0-py3-none-any.whl (43 kB)\n",
      "Using cached bcrypt-4.3.0-cp39-abi3-win_amd64.whl (152 kB)\n",
      "Using cached build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Using cached grpcio-1.73.0-cp312-cp312-win_amd64.whl (4.3 MB)\n",
      "Using cached jiter-0.10.0-cp312-cp312-win_amd64.whl (206 kB)\n",
      "Using cached kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
      "Using cached langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
      "Using cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Using cached mmh3-5.1.0-cp312-cp312-win_amd64.whl (41 kB)\n",
      "Using cached onnxruntime-1.22.0-cp312-cp312-win_amd64.whl (12.7 MB)\n",
      "Using cached opentelemetry_api-1.34.1-py3-none-any.whl (65 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_proto-1.34.1-py3-none-any.whl (55 kB)\n",
      "Using cached opentelemetry_instrumentation_fastapi-0.55b1-py3-none-any.whl (12 kB)\n",
      "Using cached opentelemetry_instrumentation-0.55b1-py3-none-any.whl (31 kB)\n",
      "Using cached opentelemetry_instrumentation_asgi-0.55b1-py3-none-any.whl (16 kB)\n",
      "Using cached opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl (196 kB)\n",
      "Using cached opentelemetry_util_http-0.55b1-py3-none-any.whl (7.3 kB)\n",
      "Using cached opentelemetry_sdk-1.34.1-py3-none-any.whl (118 kB)\n",
      "Using cached orjson-3.10.18-cp312-cp312-win_amd64.whl (134 kB)\n",
      "Using cached posthog-4.10.0-py3-none-any.whl (102 kB)\n",
      "Using cached starlette-0.46.2-py3-none-any.whl (72 kB)\n",
      "Using cached tokenizers-0.20.3-cp312-none-win_amd64.whl (2.4 MB)\n",
      "Using cached typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Using cached uvicorn-0.34.3-py3-none-any.whl (62 kB)\n",
      "Using cached importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
      "Using cached google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
      "Using cached googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Using cached httptools-0.6.4-cp312-cp312-win_amd64.whl (88 kB)\n",
      "Using cached huggingface_hub-0.33.0-py3-none-any.whl (514 kB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Using cached protobuf-5.29.5-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached watchfiles-1.0.5-cp312-cp312-win_amd64.whl (291 kB)\n",
      "Using cached websockets-15.0.1-cp312-cp312-win_amd64.whl (176 kB)\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Using cached pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Using cached rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Using cached pyreadline3-3.5.4-py3-none-any.whl (83 kB)\n",
      "Building wheels for collected packages: chroma-hnswlib\n",
      "  Building wheel for chroma-hnswlib (pyproject.toml): started\n",
      "  Building wheel for chroma-hnswlib (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for chroma-hnswlib: filename=chroma_hnswlib-0.7.6-cp312-cp312-win_amd64.whl size=160723 sha256=3a99e1d4b5f48d05a93f2fd1ea6b9b514bdf95eba862edd043084de560232b52\n",
      "  Stored in directory: c:\\users\\ktgu1\\appdata\\local\\pip\\cache\\wheels\\28\\29\\0e\\934c768c2e673547ec6e947e821346f4ed691a089fe046743f\n",
      "Successfully built chroma-hnswlib\n",
      "Installing collected packages: pypika, flatbuffers, durationpy, websockets, typing-extensions, shellingham, rsa, pyreadline3, pyproject_hooks, protobuf, orjson, opentelemetry-util-http, oauthlib, mmh3, marshmallow, jiter, importlib-resources, httptools, grpcio, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, uvicorn, typing-inspection, typing-inspect, tiktoken, starlette, requests-oauthlib, pydantic-core, posthog, opentelemetry-proto, opentelemetry-api, humanfriendly, huggingface-hub, googleapis-common-protos, google-auth, build, typer, tokenizers, tavily-python, pydantic, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, kubernetes, dataclasses-json, coloredlogs, pydantic-settings, opentelemetry-sdk, opentelemetry-instrumentation, openai, onnxruntime, langsmith, fastapi, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, langchain-core, opentelemetry-instrumentation-fastapi, langchain-text-splitters, langchain-openai, langchain, chromadb, langchain-community, langchain-chroma\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.11.0\n",
      "    Uninstalling typing_extensions-4.11.0:\n",
      "      Successfully uninstalled typing_extensions-4.11.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.3\n",
      "    Uninstalling protobuf-4.25.3:\n",
      "      Successfully uninstalled protobuf-4.25.3\n",
      "  Attempting uninstall: bcrypt\n",
      "    Found existing installation: bcrypt 3.2.0\n",
      "    Uninstalling bcrypt-3.2.0:\n",
      "      Successfully uninstalled bcrypt-3.2.0\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.20.1\n",
      "    Uninstalling pydantic_core-2.20.1:\n",
      "      Successfully uninstalled pydantic_core-2.20.1\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.8.2\n",
      "    Uninstalling pydantic-2.8.2:\n",
      "      Successfully uninstalled pydantic-2.8.2\n",
      "Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.3.0 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-0.5.23 coloredlogs-15.0.1 dataclasses-json-0.6.7 durationpy-0.10 fastapi-0.115.12 flatbuffers-25.2.10 google-auth-2.40.3 googleapis-common-protos-1.70.0 grpcio-1.73.0 httptools-0.6.4 huggingface-hub-0.33.0 humanfriendly-10.0 importlib-resources-6.5.2 jiter-0.10.0 kubernetes-33.1.0 langchain-0.3.0 langchain-chroma-0.1.4 langchain-community-0.3.0 langchain-core-0.3.0 langchain-openai-0.2.0 langchain-text-splitters-0.3.0 langsmith-0.1.147 marshmallow-3.26.1 mmh3-5.1.0 oauthlib-3.2.2 onnxruntime-1.22.0 openai-1.86.0 opentelemetry-api-1.34.1 opentelemetry-exporter-otlp-proto-common-1.34.1 opentelemetry-exporter-otlp-proto-grpc-1.34.1 opentelemetry-instrumentation-0.55b1 opentelemetry-instrumentation-asgi-0.55b1 opentelemetry-instrumentation-fastapi-0.55b1 opentelemetry-proto-1.34.1 opentelemetry-sdk-1.34.1 opentelemetry-semantic-conventions-0.55b1 opentelemetry-util-http-0.55b1 orjson-3.10.18 posthog-4.10.0 protobuf-5.29.5 pydantic-2.10.6 pydantic-core-2.27.2 pydantic-settings-2.9.1 pypika-0.48.9 pyproject_hooks-1.2.0 pyreadline3-3.5.4 requests-oauthlib-2.0.0 rsa-4.9.1 shellingham-1.5.4 starlette-0.46.2 tavily-python-0.5.0 tiktoken-0.9.0 tokenizers-0.20.3 typer-0.16.0 typing-extensions-4.14.0 typing-inspect-0.9.0 typing-inspection-0.4.1 uvicorn-0.34.3 watchfiles-1.0.5 websockets-15.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-core==0.3.0 langchain-openai==0.2.0 \\\n",
    "    langchain-community==0.3.0 GitPython==3.1.43 \\\n",
    "    langchain-chroma==0.1.4 tavily-python==0.5.0 pydantic==2.10.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-2. RAG実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "417\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import GitLoader\n",
    "\n",
    "\n",
    "def file_filter(file_path: str) -> bool:\n",
    "    return file_path.endswith(\".mdx\")\n",
    "\n",
    "\n",
    "loader = GitLoader(\n",
    "    clone_url=\"https://github.com/langchain-ai/langchain\",\n",
    "    repo_path=\"./langchain\",\n",
    "    branch=\"master\",\n",
    "    file_filter=file_filter,\n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "db = Chroma.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainは、大規模言語モデル（LLM）を活用したアプリケーションを開発するためのフレームワークです。このフレームワークは、LLMアプリケーションのライフサイクルの各段階を簡素化します。具体的には、以下のような機能を提供しています。\\n\\n1. **開発**: LangChainのオープンソースコンポーネントやサードパーティの統合を使用してアプリケーションを構築できます。LangGraphを利用することで、状態を持つエージェントを構築し、ストリーミングや人間の介入をサポートします。\\n\\n2. **生産化**: LangSmithを使用してアプリケーションを検査、監視、評価し、継続的に最適化して自信を持ってデプロイできます。\\n\\n3. **デプロイ**: LangGraphアプリケーションを生産準備が整ったAPIやアシスタントに変換することができます。\\n\\nLangChainは、LLMや関連技術（埋め込みモデルやベクトルストア）に対する標準インターフェースを実装し、数百のプロバイダーと統合しています。また、複数のオープンソースライブラリで構成されており、ユーザーは必要に応じてコンポーネントを選択して使用できます。'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template('''\\\n",
    "以下の文脈だけを踏まえて質問に回答してください。\n",
    "\n",
    "文脈: \"\"\"\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "質問: {question}\n",
    "''')\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "chain = {\n",
    "    \"question\": RunnablePassthrough(),\n",
    "    \"context\": retriever,\n",
    "} | prompt | model | StrOutputParser()\n",
    "\n",
    "chain.invoke(\"LangChainの概要を教えて\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-3. 検索クエリの工夫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothetical_prompt = ChatPromptTemplate.from_template(\"\"\"\\\n",
    "次の質問に回答する一文を書いてください。\n",
    "\n",
    "質問: {question}\n",
    "\"\"\")\n",
    "\n",
    "hypothetical_chain = hypothetical_prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainは、大規模言語モデル（LLM）を活用したアプリケーションを開発するためのフレームワークです。LangChainは、開発、運用、デプロイの各段階を簡素化することを目的としています。具体的には、以下のような特徴があります。\\n\\n1. **開発**: LangChainのオープンソースコンポーネントやサードパーティの統合を使用してアプリケーションを構築できます。また、LangGraphを利用して、状態を持つエージェントを構築することができます。\\n\\n2. **運用**: LangSmithを使用してアプリケーションを監視、評価し、継続的に最適化して自信を持ってデプロイできます。\\n\\n3. **デプロイ**: LangGraphアプリケーションを本番環境向けのAPIやアシスタントに変換することができます。\\n\\nLangChainは、さまざまなプロバイダーと統合し、標準インターフェースを提供することで、開発者が異なるコンポーネントを簡単に切り替えたり、組み合わせたりできるようにしています。また、複雑なアプリケーションのオーケストレーションをサポートするためのLangGraphや、アプリケーションの可視化と評価を行うLangSmithといったツールも提供しています。'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyde_rag_chain = {\n",
    "    \"question\": RunnablePassthrough(),\n",
    "    \"context\": hypothetical_chain | retriever,\n",
    "} | prompt | model | StrOutputParser()\n",
    "\n",
    "hyde_rag_chain.invoke(\"LangChainの概要を教えて\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 複数の検索クエリの生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class QueryGenerationOutput(BaseModel):\n",
    "    queries: list[str] = Field(..., description=\"検索クエリのリスト\")\n",
    "\n",
    "\n",
    "query_generation_prompt = ChatPromptTemplate.from_template(\"\"\"\\\n",
    "質問に対してベクターデータベースから関連文書を検索するために、\n",
    "3つの異なる検索クエリを生成してください。\n",
    "距離ベースの類似性検索の限界を克服するために、\n",
    "ユーザーの質問に対して複数の視点を提供することが目標です。\n",
    "\n",
    "質問: {question}\n",
    "\"\"\")\n",
    "\n",
    "query_generation_chain = (\n",
    "    query_generation_prompt\n",
    "    | model.with_structured_output(QueryGenerationOutput)\n",
    "    | (lambda x: x.queries)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainは、大規模言語モデル（LLM）を活用したアプリケーションを開発するためのフレームワークです。LangChainは、開発、運用、デプロイの各段階を簡素化することを目的としています。具体的には、以下のような機能を提供しています。\\n\\n1. **開発**: LangChainのオープンソースコンポーネントやサードパーティの統合を使用してアプリケーションを構築できます。また、LangGraphを利用して、状態を持つエージェントを構築し、ストリーミングや人間の介入をサポートします。\\n\\n2. **運用**: LangSmithを使用してアプリケーションを検査、監視、評価し、継続的に最適化して自信を持ってデプロイできます。\\n\\n3. **デプロイ**: LangGraphアプリケーションを生産準備が整ったAPIやアシスタントに変換できます。\\n\\nLangChainは、さまざまなモデルや関連コンポーネントの標準インターフェースを提供し、開発者が異なるプロバイダー間で簡単に切り替えたり、コンポーネントを組み合わせたりできるようにします。また、アプリケーションの複雑さが増すにつれて、効率的に要素を接続するためのオーケストレーション機能や、アプリケーションの可観測性と評価をサポートする機能も備えています。\\n\\n全体として、LangChainは、開発者がAIアプリケーションを構築する際の利便性を高めるためのエコシステムを提供しています。'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_query_rag_chain = {\n",
    "    \"question\": RunnablePassthrough(),\n",
    "    \"context\": query_generation_chain | retriever.map(),\n",
    "} | prompt | model | StrOutputParser()\n",
    "\n",
    "multi_query_rag_chain.invoke(\"LangChainの概要を教えて\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-4. 検索後の工夫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "def reciprocal_rank_fusion(\n",
    "    retriever_outputs: list[list[Document]],\n",
    "    k: int = 60,\n",
    ") -> list[str]:\n",
    "    # 各ドキュメントのコンテンツ (文字列) とそのスコアの対応を保持する辞書を準備\n",
    "    content_score_mapping = {}\n",
    "\n",
    "    # 検索クエリごとにループ（RFFアルゴリズムを使用）\n",
    "    for docs in retriever_outputs:\n",
    "        print(f\"docs:{docs}\")\n",
    "        # 検索結果のドキュメントごとにループ\n",
    "        for rank, doc in enumerate(docs):\n",
    "            content = doc.page_content\n",
    "            print(f\"rank:{rank}\")\n",
    "            # 初めて登場したコンテンツの場合はスコアを0で初期化\n",
    "            if content not in content_score_mapping:\n",
    "                content_score_mapping[content] = 0\n",
    "\n",
    "            # (1 / (順位 + k)) のスコアを加算\n",
    "            content_score_mapping[content] += 1 / (rank + k)\n",
    "            #print(f\"content_score_mapping:{content_score_mapping}\")\n",
    "\n",
    "    # スコアの大きい順にソート\n",
    "    ranked = sorted(content_score_mapping.items(), key=lambda x: x[1], reverse=True)  # noqa\n",
    "    return [content for content, _ in ranked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs:[Document(metadata={'file_name': 'why_langchain.mdx', 'file_path': 'docs\\\\docs\\\\concepts\\\\why_langchain.mdx', 'file_type': '.mdx', 'source': 'docs\\\\docs\\\\concepts\\\\why_langchain.mdx'}, page_content='# Why LangChain?\\n\\nThe goal of `langchain` the Python package and LangChain the company is to make it as easy as possible for developers to build applications that reason.\\nWhile LangChain originally started as a single open source package, it has evolved into a company and a whole ecosystem.\\nThis page will talk about the LangChain ecosystem as a whole.\\nMost of the components within the LangChain ecosystem can be used by themselves - so if you feel particularly drawn to certain components but not others, that is totally fine! Pick and choose whichever components you like best for your own use case!\\n\\n## Features\\n\\nThere are several primary needs that LangChain aims to address:\\n\\n1. **Standardized component interfaces:** The growing number of [models](/docs/integrations/chat/) and [related components](/docs/integrations/vectorstores/) for AI applications has resulted in a wide variety of different APIs that developers need to learn and use.\\nThis diversity can make it challenging for developers to switch between providers or combine components when building applications.\\nLangChain exposes a standard interface for key components, making it easy to switch between providers.\\n\\n2. **Orchestration:** As applications become more complex, combining multiple components and models, there\\'s [a growing need to efficiently connect these elements into control flows](https://lilianweng.github.io/posts/2023-06-23-agent/) that can [accomplish diverse tasks](https://www.sequoiacap.com/article/generative-ais-act-o1/).\\n[Orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)) is crucial for building such applications.\\n\\n3. **Observability and evaluation:** As applications become more complex, it becomes increasingly difficult to understand what is happening within them.\\nFurthermore, the pace of development can become rate-limited by the [paradox of choice](https://en.wikipedia.org/wiki/Paradox_of_choice).\\nFor example, developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\n[Observability](https://en.wikipedia.org/wiki/Observability) and evaluations can help developers monitor their applications and rapidly answer these types of questions with confidence.\\n\\n\\n## Standardized component interfaces\\n\\nLangChain provides common interfaces for components that are central to many AI applications.\\nAs an example, all [chat models](/docs/concepts/chat_models/) implement the [BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) interface.\\nThis provides a standard way to interact with chat models, supporting important but often provider-specific features like [tool calling](/docs/concepts/tool_calling/) and [structured outputs](/docs/concepts/structured_outputs/).\\n\\n\\n### Example: chat models \\n\\nMany [model providers](/docs/concepts/chat_models/) support [tool calling](/docs/concepts/tool_calling/), a critical feature for many applications (e.g., [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/)), that allows a developer to request model responses that match a particular schema.\\nThe APIs for each provider differ. \\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to bind [tools](/docs/concepts/tools) to a model in order to support [tool calling](/docs/concepts/tool_calling/):\\n\\n```python\\n# Tool creation\\ntools = [my_tool]\\n# Tool binding\\nmodel_with_tools = model.bind_tools(tools)\\n```\\n\\nSimilarly, getting models to produce [structured outputs](/docs/concepts/structured_outputs/) is an extremely common use case. \\nProviders support different approaches for this, including [JSON mode or tool calling](https://platform.openai.com/docs/guides/structured-outputs), with different APIs.\\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to produce structured outputs using the `with_structured_output()` method:\\n\\n```python\\n# Define schema\\nschema = ...\\n# Bind schema to model\\nmodel_with_structure = model.with_structured_output(schema)\\n```\\n\\n### Example: retrievers\\n\\nIn the context of [RAG](/docs/concepts/rag/) and LLM application components, LangChain\\'s [retriever](/docs/concepts/retrievers/) interface provides a standard way to connect to many different types of data services or databases (e.g., [vector stores](/docs/concepts/vectorstores) or databases).\\nThe underlying implementation of the retriever depends on the type of data store or database you are connecting to, but all retrievers implement the [runnable interface](/docs/concepts/runnables/), meaning they can be invoked in a common manner.\\n\\n```python\\ndocuments = my_retriever.invoke(\"What is the meaning of life?\")\\n```\\n\\n## Orchestration \\n\\nWhile standardization for individual components is useful, we\\'ve increasingly seen that developers want to *combine* components into more complex applications. \\nThis motivates the need for [orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)).\\nThere are several common characteristics of LLM applications that this orchestration layer should support:\\n\\n* **Complex control flow:** The application requires complex patterns such as cycles (e.g., a loop that reiterates until a condition is met).\\n* **[Persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/):** The application needs to maintain [short-term and / or long-term memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n* **[Human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/):** The application needs human interaction, e.g., pausing, reviewing, editing, approving certain steps.\\n\\nThe recommended way to orchestrate components for complex applications is [LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/).\\nLangGraph is a library that gives developers a high degree of control by expressing the flow of the application as a set of nodes and edges.\\nLangGraph comes with built-in support for [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), [memory](https://langchain-ai.github.io/langgraph/concepts/memory/), and other features.\\nIt\\'s particularly well suited for building [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/) or [multi-agent](https://langchain-ai.github.io/langgraph/concepts/multi_agent/) applications. \\nImportantly, individual LangChain components can be used as LangGraph nodes, but you can also use LangGraph **without** using LangChain components.\\n\\n:::info[Further reading]\\n\\nHave a look at our free course, [Introduction to LangGraph](https://academy.langchain.com/courses/intro-to-langgraph), to learn more about how to use LangGraph to build complex applications.\\n\\n:::\\n\\n## Observability and evaluation\\n\\nThe pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. \\nDevelopers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\nHigh quality tracing and evaluations can help you rapidly answer these types of questions with confidence.\\n[LangSmith](https://docs.smith.langchain.com/) is our platform that supports observability and evaluation for AI applications.\\nSee our conceptual guides on [evaluations](https://docs.smith.langchain.com/concepts/evaluation) and [tracing](https://docs.smith.langchain.com/concepts/tracing) for more details.\\n\\n:::info[Further reading]\\n\\nSee our video playlist on [LangSmith tracing and evaluations](https://youtube.com/playlist?list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S&feature=shared) for more details.\\n\\n:::\\n\\n## Conclusion\\n\\nLangChain offers standard interfaces for components that are central to many AI applications, which offers a few specific advantages:\\n- **Ease of swapping providers:** It allows you to swap out different component providers without having to change the underlying code.\\n- **Advanced features:** It provides common methods for more advanced features, such as [streaming](/docs/concepts/streaming) and [tool calling](/docs/concepts/tool_calling/).\\n\\n[LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/) makes it possible to orchestrate complex applications (e.g., [agents](/docs/concepts/agents/)) and provide features like including [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), or [memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n\\n[LangSmith](https://docs.smith.langchain.com/) makes it possible to iterate with confidence on your applications, by providing LLM-specific observability and framework for testing and evaluating your application.\\n'), Document(metadata={'file_name': 'introduction.mdx', 'file_path': 'docs\\\\docs\\\\introduction.mdx', 'file_type': '.mdx', 'source': 'docs\\\\docs\\\\introduction.mdx'}, page_content='---\\nsidebar_position: 0\\nsidebar_class_name: hidden\\n---\\n\\n# Introduction\\n\\n**LangChain** is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:\\n- **Development**: Build your applications using LangChain\\'s open-source [components](/docs/concepts) and [third-party integrations](/docs/integrations/providers/).\\nUse [LangGraph](/docs/concepts/architecture/#langgraph) to build stateful agents with first-class streaming and human-in-the-loop support.\\n- **Productionization**: Use [LangSmith](https://docs.smith.langchain.com/) to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\n- **Deployment**: Turn your LangGraph applications into production-ready APIs and Assistants with [LangGraph Platform](https://langchain-ai.github.io/langgraph/cloud/).\\n\\nimport ThemedImage from \\'@theme/ThemedImage\\';\\nimport useBaseUrl from \\'@docusaurus/useBaseUrl\\';\\n\\n<ThemedImage\\n  alt=\"Diagram outlining the hierarchical organization of the LangChain framework, displaying the interconnected parts across multiple layers.\"\\n  sources={{\\n    light: useBaseUrl(\\'/svg/langchain_stack_112024.svg\\'),\\n    dark: useBaseUrl(\\'/svg/langchain_stack_112024_dark.svg\\'),\\n  }}\\n  style={{ width: \"100%\" }}\\n  title=\"LangChain Framework Overview\"\\n/>\\n\\nLangChain implements a standard interface for large language models and related\\ntechnologies, such as embedding models and vector stores, and integrates with\\nhundreds of providers. See the [integrations](/docs/integrations/providers/) page for\\nmore.\\n\\nimport ChatModelTabs from \"@theme/ChatModelTabs\";\\n\\n<ChatModelTabs/>\\n\\n```python\\nmodel.invoke(\"Hello, world!\")\\n```\\n\\n:::note\\n\\nThese docs focus on the Python LangChain library. [Head here](https://js.langchain.com) for docs on the JavaScript LangChain library.\\n\\n:::\\n\\n## Architecture\\n\\nThe LangChain framework consists of multiple open-source libraries. Read more in the\\n[Architecture](/docs/concepts/architecture/) page.\\n\\n- **`langchain-core`**: Base abstractions for chat models and other components.\\n- **Integration packages** (e.g. `langchain-openai`, `langchain-anthropic`, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\\n- **`langchain`**: Chains, agents, and retrieval strategies that make up an application\\'s cognitive architecture.\\n- **`langchain-community`**: Third-party integrations that are community maintained.\\n- **`langgraph`**: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See [LangGraph documentation](https://langchain-ai.github.io/langgraph/).\\n\\n## Guides\\n\\n### [Tutorials](/docs/tutorials)\\n\\nIf you\\'re looking to build something specific or are more of a hands-on learner, check out our [tutorials section](/docs/tutorials).\\nThis is the best place to get started.\\n\\nThese are the best ones to get started with:\\n\\n- [Build a Simple LLM Application](/docs/tutorials/llm_chain)\\n- [Build a Chatbot](/docs/tutorials/chatbot)\\n- [Build an Agent](/docs/tutorials/agents)\\n- [Introduction to LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/)\\n\\nExplore the full list of LangChain tutorials [here](/docs/tutorials), and check out other [LangGraph tutorials here](https://langchain-ai.github.io/langgraph/tutorials/). To learn more about LangGraph, check out our first LangChain Academy course, *Introduction to LangGraph*, available [here](https://academy.langchain.com/courses/intro-to-langgraph).\\n\\n\\n### [How-to guides](/docs/how_to)\\n\\n[Here](/docs/how_to) you’ll find short answers to “How do I….?” types of questions.\\nThese how-to guides don’t cover topics in depth – you’ll find that material in the [Tutorials](/docs/tutorials) and the [API Reference](https://python.langchain.com/api_reference/).\\nHowever, these guides will help you quickly accomplish common tasks using [chat models](/docs/how_to/#chat-models),\\n[vector stores](/docs/how_to/#vector-stores), and other common LangChain components.\\n\\nCheck out [LangGraph-specific how-tos here](https://langchain-ai.github.io/langgraph/how-tos/).\\n\\n### [Conceptual guide](/docs/concepts)\\n\\nIntroductions to all the key parts of LangChain you’ll need to know! [Here](/docs/concepts) you\\'ll find high level explanations of all LangChain concepts.\\n\\nFor a deeper dive into LangGraph concepts, check out [this page](https://langchain-ai.github.io/langgraph/concepts/).\\n\\n### [Integrations](integrations/providers/index.mdx)\\n\\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\\nIf you\\'re looking to get up and running quickly with [chat models](/docs/integrations/chat/), [vector stores](/docs/integrations/vectorstores/),\\nor other LangChain components from a specific provider, check out our growing list of [integrations](/docs/integrations/providers/).\\n\\n\\n### [API reference](https://python.langchain.com/api_reference/)\\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\\n\\n## Ecosystem\\n\\n### [🦜🛠️ LangSmith](https://docs.smith.langchain.com)\\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\\n\\n### [🦜🕸️ LangGraph](https://langchain-ai.github.io/langgraph)\\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by Linkedin, Uber, Klarna, GitLab, and many more.\\n\\n## Additional resources\\n\\n### [Versions](/docs/versions/v0_3/)\\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\\n\\n### [Security](/docs/security)\\nRead up on [security](/docs/security) best practices to make sure you\\'re developing safely with LangChain.\\n\\n### [Contributing](contributing/index.mdx)\\nCheck out the developer\\'s guide for guidelines on contributing and help getting your dev environment set up.\\n'), Document(metadata={'file_name': 'lcel.mdx', 'file_path': 'docs\\\\docs\\\\concepts\\\\lcel.mdx', 'file_type': '.mdx', 'source': 'docs\\\\docs\\\\concepts\\\\lcel.mdx'}, page_content='# LangChain Expression Language (LCEL)\\n\\n:::info Prerequisites\\n* [Runnable Interface](/docs/concepts/runnables)\\n:::\\n\\nThe **L**ang**C**hain **E**xpression **L**anguage (LCEL) takes a [declarative](https://en.wikipedia.org/wiki/Declarative_programming) approach to building new [Runnables](/docs/concepts/runnables) from existing Runnables.\\n\\nThis means that you describe what *should* happen, rather than *how* it should happen, allowing LangChain to optimize the run-time execution of the chains.\\n\\nWe often refer to a `Runnable` created using LCEL as a \"chain\". It\\'s important to remember that a \"chain\" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).\\n\\n:::note\\n* The [LCEL cheatsheet](/docs/how_to/lcel_cheatsheet/) shows common patterns that involve the Runnable interface and LCEL expressions.\\n* Please see the following list of [how-to guides](/docs/how_to/#langchain-expression-language-lcel) that cover common tasks with LCEL.\\n* A list of built-in `Runnables` can be found in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html). Many of these Runnables are useful when composing custom \"chains\" in LangChain using LCEL.\\n:::\\n\\n## Benefits of LCEL\\n\\nLangChain optimizes the run-time execution of chains built with LCEL in a number of ways:\\n\\n- **Optimized parallel execution**: Run Runnables in parallel using [RunnableParallel](#runnableparallel) or run multiple inputs through a given chain in parallel using the [Runnable Batch API](/docs/concepts/runnables/#optimized-parallel-execution-batch). Parallel execution can significantly reduce the latency as processing can be done in parallel instead of sequentially.\\n- **Guaranteed Async support**: Any chain built with LCEL can be run asynchronously using the [Runnable Async API](/docs/concepts/runnables/#asynchronous-support). This can be useful when running chains in a server environment where you want to handle large number of requests concurrently.\\n- **Simplify streaming**: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(time elapsed until the first chunk of output from a [chat model](/docs/concepts/chat_models) or [llm](/docs/concepts/text_llms) comes out).\\n\\nOther benefits include:\\n\\n- [**Seamless LangSmith tracing**](https://docs.smith.langchain.com)\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, **all** steps are automatically logged to [LangSmith](https://docs.smith.langchain.com/) for maximum observability and debuggability.\\n- **Standard API**: Because all chains are built using the Runnable interface, they can be used in the same way as any other Runnable.\\n- [**Deployable with LangServe**](/docs/concepts/architecture#langserve): Chains built with LCEL can be deployed using for production use.\\n\\n## Should I use LCEL?\\n\\nLCEL is an [orchestration solution](https://en.wikipedia.org/wiki/Orchestration_(computing)) -- it allows LangChain to handle run-time execution of chains in an optimized way.\\n\\nWhile we have seen users run chains with hundreds of steps in production, we generally recommend using LCEL for simpler orchestration tasks. When the application requires complex state management, branching, cycles or multiple agents, we recommend that users take advantage of [LangGraph](/docs/concepts/architecture#langgraph).\\n\\nIn LangGraph, users define graphs that specify the application\\'s flow. This allows users to keep using LCEL within individual nodes when LCEL is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.\\n\\nHere are some guidelines:\\n\\n* If you are making a single LLM call, you don\\'t need LCEL; instead call the underlying [chat model](/docs/concepts/chat_models) directly.\\n* If you have a simple chain (e.g., prompt + llm + parser, simple retrieval set up etc.), LCEL is a reasonable fit, if you\\'re taking advantage of the LCEL benefits.\\n* If you\\'re building a complex chain (e.g., with branching, cycles, multiple agents, etc.) use [LangGraph](/docs/concepts/architecture#langgraph) instead. Remember that you can always use LCEL within individual nodes in LangGraph.\\n\\n## Composition Primitives\\n\\n`LCEL` chains are built by composing existing `Runnables` together. The two main composition primitives are [RunnableSequence](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableSequence.html#langchain_core.runnables.base.RunnableSequence) and [RunnableParallel](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableParallel.html#langchain_core.runnables.base.RunnableParallel).\\n\\nMany other composition primitives (e.g., [RunnableAssign](\\nhttps://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnableAssign.html#langchain_core.runnables.passthrough.RunnableAssign\\n)) can be thought of as variations of these two primitives.\\n\\n:::note\\nYou can find a list of all composition primitives in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html).\\n:::\\n\\n### RunnableSequence\\n\\n`RunnableSequence` is a composition primitive that allows you \"chain\" multiple runnables sequentially, with the output of one runnable serving as the input to the next.\\n\\n```python\\nfrom langchain_core.runnables import RunnableSequence\\nchain = RunnableSequence([runnable1, runnable2])\\n```\\n\\nInvoking the `chain` with some input:\\n\\n```python\\nfinal_output = chain.invoke(some_input)\\n```\\n\\ncorresponds to the following:\\n\\n```python\\noutput1 = runnable1.invoke(some_input)\\nfinal_output = runnable2.invoke(output1)\\n```\\n\\n:::note\\n`runnable1` and `runnable2` are placeholders for any `Runnable` that you want to chain together.\\n:::\\n\\n### RunnableParallel\\n\\n`RunnableParallel` is a composition primitive that allows you to run multiple runnables concurrently, with the same input provided to each.\\n\\n```python\\nfrom langchain_core.runnables import RunnableParallel\\nchain = RunnableParallel({\\n    \"key1\": runnable1,\\n    \"key2\": runnable2,\\n})\\n```\\n\\nInvoking the `chain` with some input:\\n\\n```python\\nfinal_output = chain.invoke(some_input)\\n```\\n\\nWill yield a `final_output` dictionary with the same keys as the input dictionary, but with the values replaced by the output of the corresponding runnable.\\n\\n```python\\n{\\n    \"key1\": runnable1.invoke(some_input),\\n    \"key2\": runnable2.invoke(some_input),\\n}\\n```\\n\\nRecall, that the runnables are executed in parallel, so while the result is the same as\\ndictionary comprehension shown above, the execution time is much faster.\\n\\n:::note\\n`RunnableParallel`supports both synchronous and asynchronous execution (as all `Runnables` do).\\n\\n* For synchronous execution, `RunnableParallel` uses a [ThreadPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor) to run the runnables concurrently.\\n* For asynchronous execution, `RunnableParallel` uses [asyncio.gather](https://docs.python.org/3/library/asyncio.html#asyncio.gather) to run the runnables concurrently.\\n:::\\n\\n## Composition Syntax\\n\\nThe usage of `RunnableSequence` and `RunnableParallel` is so common that we created a shorthand syntax for using them. This helps\\nto make the code more readable and concise.\\n\\n### The `|` operator\\n\\nWe have [overloaded](https://docs.python.org/3/reference/datamodel.html#special-method-names) the `|` operator to create a `RunnableSequence` from two `Runnables`.\\n\\n```python\\nchain = runnable1 | runnable2\\n```\\n\\nis Equivalent to:\\n\\n```python\\nchain = RunnableSequence([runnable1, runnable2])\\n```\\n\\n### The `.pipe` method\\n\\nIf you have moral qualms with operator overloading, you can use the `.pipe` method instead. This is equivalent to the `|` operator.\\n\\n```python\\nchain = runnable1.pipe(runnable2)\\n```\\n\\n### Coercion\\n\\nLCEL applies automatic type coercion to make it easier to compose chains.\\n\\nIf you do not understand the type coercion, you can always use the `RunnableSequence` and `RunnableParallel` classes directly.\\n\\nThis will make the code more verbose, but it will also make it more explicit.\\n\\n#### Dictionary to RunnableParallel\\n\\nInside an LCEL expression, a dictionary is automatically converted to a `RunnableParallel`.\\n\\nFor example, the following code:\\n\\n```python\\nmapping = {\\n    \"key1\": runnable1,\\n    \"key2\": runnable2,\\n}\\n\\nchain = mapping | runnable3\\n```\\n\\nIt gets automatically converted to the following:\\n\\n```python\\nchain = RunnableSequence([RunnableParallel(mapping), runnable3])\\n```\\n\\n:::caution\\nYou have to be careful because the `mapping` dictionary is not a `RunnableParallel` object, it is just a dictionary. This means that the following code will raise an `AttributeError`:\\n\\n```python\\nmapping.invoke(some_input)\\n```\\n:::\\n\\n#### Function to RunnableLambda\\n\\nInside an LCEL expression, a function is automatically converted to a `RunnableLambda`.\\n\\n```\\ndef some_func(x):\\n    return x\\n\\nchain = some_func | runnable1\\n```\\n\\nIt gets automatically converted to the following:\\n\\n```python\\nchain = RunnableSequence([RunnableLambda(some_func), runnable1])\\n```\\n\\n:::caution\\nYou have to be careful because the lambda function is not a `RunnableLambda` object, it is just a function. This means that the following code will raise an `AttributeError`:\\n\\n```python\\nlambda x: x + 1.invoke(some_input)\\n```\\n:::\\n\\n## Legacy chains\\n\\nLCEL aims to provide consistency around behavior and customization over legacy subclassed chains such as `LLMChain` and\\n`ConversationalRetrievalChain`. Many of these legacy chains hide important details like prompts, and as a wider variety\\nof viable models emerge, customization has become more and more important.\\n\\nIf you are currently using one of these legacy chains, please see [this guide for guidance on how to migrate](/docs/versions/migrating_chains).\\n\\nFor guides on how to do specific tasks with LCEL, check out [the relevant how-to guides](/docs/how_to/#langchain-expression-language-lcel).\\n'), Document(metadata={'file_name': 'index.mdx', 'file_path': 'docs\\\\docs\\\\concepts\\\\index.mdx', 'file_type': '.mdx', 'source': 'docs\\\\docs\\\\concepts\\\\index.mdx'}, page_content='# Conceptual guide\\n\\nThis guide provides explanations of the key concepts behind the LangChain framework and AI applications more broadly.\\n\\nWe recommend that you go through at least one of the [Tutorials](/docs/tutorials) before diving into the conceptual guide. This will provide practical context that will make it easier to understand the concepts discussed here.\\n\\nThe conceptual guide does not cover step-by-step instructions or specific implementation examples — those are found in the [How-to guides](/docs/how_to/) and [Tutorials](/docs/tutorials). For detailed reference material, please see the [API reference](https://python.langchain.com/api_reference/).\\n\\n## High level\\n\\n- **[Why LangChain?](/docs/concepts/why_langchain)**: Overview of the value that LangChain provides.\\n- **[Architecture](/docs/concepts/architecture)**: How packages are organized in the LangChain ecosystem.\\n\\n## Concepts\\n\\n- **[Chat models](/docs/concepts/chat_models)**: LLMs exposed via a chat API that process sequences of messages as input and output a message.\\n- **[Messages](/docs/concepts/messages)**: The unit of communication in chat models, used to represent model input and output.\\n- **[Chat history](/docs/concepts/chat_history)**: A conversation represented as a sequence of messages, alternating between user messages and model responses.\\n- **[Tools](/docs/concepts/tools)**: A function with an associated schema defining the function\\'s name, description, and the arguments it accepts.\\n- **[Tool calling](/docs/concepts/tool_calling)**: A type of chat model API that accepts tool schemas, along with messages, as input and returns invocations of those tools as part of the output message.\\n- **[Structured output](/docs/concepts/structured_outputs)**: A technique to make a chat model respond in a structured format, such as JSON that matches a given schema.\\n- **[Memory](https://langchain-ai.github.io/langgraph/concepts/memory/)**: Information about a conversation that is persisted so that it can be used in future conversations.\\n- **[Multimodality](/docs/concepts/multimodality)**: The ability to work with data that comes in different forms, such as text, audio, images, and video.\\n- **[Runnable interface](/docs/concepts/runnables)**: The base abstraction that many LangChain components and the LangChain Expression Language are built on.\\n- **[Streaming](/docs/concepts/streaming)**: LangChain streaming APIs for surfacing results as they are generated.\\n- **[LangChain Expression Language (LCEL)](/docs/concepts/lcel)**: A syntax for orchestrating LangChain components. Most useful for simpler applications.\\n- **[Document loaders](/docs/concepts/document_loaders)**: Load a source as a list of documents.\\n- **[Retrieval](/docs/concepts/retrieval)**: Information retrieval systems can retrieve structured or unstructured data from a datasource in response to a query.\\n- **[Text splitters](/docs/concepts/text_splitters)**: Split long text into smaller chunks that can be individually indexed to enable granular retrieval.\\n- **[Embedding models](/docs/concepts/embedding_models)**: Models that represent data such as text or images in a vector space.\\n- **[Vector stores](/docs/concepts/vectorstores)**: Storage of and efficient search over vectors and associated metadata.\\n- **[Retriever](/docs/concepts/retrievers)**: A component that returns relevant documents from a knowledge base in response to a query.\\n- **[Retrieval Augmented Generation (RAG)](/docs/concepts/rag)**: A technique that enhances language models by combining them with external knowledge bases.\\n- **[Agents](/docs/concepts/agents)**: Use a [language model](/docs/concepts/chat_models) to choose a sequence of actions to take. Agents can interact with external resources via [tool](/docs/concepts/tools).\\n- **[Prompt templates](/docs/concepts/prompt_templates)**: Component for factoring out the static parts of a model \"prompt\" (usually a sequence of messages). Useful for serializing, versioning, and reusing these static parts.\\n- **[Output parsers](/docs/concepts/output_parsers)**: Responsible for taking the output of a model and transforming it into a more suitable format for downstream tasks. Output parsers were primarily useful prior to the general availability of [tool calling](/docs/concepts/tool_calling) and [structured outputs](/docs/concepts/structured_outputs).\\n- **[Few-shot prompting](/docs/concepts/few_shot_prompting)**: A technique for improving model performance by providing a few examples of the task to perform in the prompt.\\n- **[Example selectors](/docs/concepts/example_selectors)**: Used to select the most relevant examples from a dataset based on a given input. Example selectors are used in few-shot prompting to select examples for a prompt.\\n- **[Async programming](/docs/concepts/async)**: The basics that one should know to use LangChain in an asynchronous context.\\n- **[Callbacks](/docs/concepts/callbacks)**: Callbacks enable the execution of custom auxiliary code in built-in components. Callbacks are used to stream outputs from LLMs in LangChain, trace the intermediate steps of an application, and more.\\n- **[Tracing](/docs/concepts/tracing)**: The process of recording the steps that an application takes to go from input to output. Tracing is essential for debugging and diagnosing issues in complex applications.\\n- **[Evaluation](/docs/concepts/evaluation)**: The process of assessing the performance and effectiveness of AI applications. This involves testing the model\\'s responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose. This process is vital for building reliable applications.\\n- **[Testing](/docs/concepts/testing)**: The process of verifying that a component of an integration or application works as expected. Testing is essential for ensuring that the application behaves correctly and that changes to the codebase do not introduce new bugs.\\n\\n## Glossary\\n\\n- **[AIMessageChunk](/docs/concepts/messages#aimessagechunk)**: A partial response from an AI message. Used when streaming responses from a chat model.\\n- **[AIMessage](/docs/concepts/messages#aimessage)**: Represents a complete response from an AI model.\\n- **[astream_events](/docs/concepts/chat_models#key-methods)**: Stream granular information from [LCEL](/docs/concepts/lcel) chains.\\n- **[BaseTool](/docs/concepts/tools/#tool-interface)**: The base class for all tools in LangChain.\\n- **[batch](/docs/concepts/runnables)**: Use to execute a runnable with batch inputs.\\n- **[bind_tools](/docs/concepts/tool_calling/#tool-binding)**: Allows models to interact with tools.\\n- **[Caching](/docs/concepts/chat_models#caching)**: Storing results to avoid redundant calls to a chat model.\\n- **[Chat models](/docs/concepts/multimodality/#multimodality-in-chat-models)**: Chat models that handle multiple data modalities.\\n- **[Configurable runnables](/docs/concepts/runnables/#configurable-runnables)**: Creating configurable Runnables.\\n- **[Context window](/docs/concepts/chat_models#context-window)**: The maximum size of input a chat model can process.\\n- **[Conversation patterns](/docs/concepts/chat_history#conversation-patterns)**: Common patterns in chat interactions.\\n- **[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)**: LangChain\\'s representation of a document.\\n- **[Embedding models](/docs/concepts/multimodality/#multimodality-in-embedding-models)**: Models that generate vector embeddings for various data types.\\n- **[HumanMessage](/docs/concepts/messages#humanmessage)**: Represents a message from a human user.\\n- **[InjectedState](/docs/concepts/tools#injectedstate)**: A state injected into a tool function.\\n- **[InjectedStore](/docs/concepts/tools#injectedstore)**: A store that can be injected into a tool for data persistence.\\n- **[InjectedToolArg](/docs/concepts/tools#injectedtoolarg)**: Mechanism to inject arguments into tool functions.\\n- **[input and output types](/docs/concepts/runnables#input-and-output-types)**: Types used for input and output in Runnables.\\n- **[Integration packages](/docs/concepts/architecture/#integration-packages)**: Third-party packages that integrate with LangChain.\\n- **[Integration tests](/docs/concepts/testing#integration-tests)**: Tests that verify the correctness of the interaction between components, usually run with access to the underlying API that powers an integration.\\n- **[invoke](/docs/concepts/runnables)**: A standard method to invoke a Runnable.\\n- **[JSON mode](/docs/concepts/structured_outputs#json-mode)**: Returning responses in JSON format.\\n- **[langchain-community](/docs/concepts/architecture#langchain-community)**: Community-driven components for LangChain.\\n- **[langchain-core](/docs/concepts/architecture#langchain-core)**: Core langchain package. Includes base interfaces and in-memory implementations.\\n- **[langchain](/docs/concepts/architecture#langchain)**: A package for higher level components (e.g., some pre-built chains).\\n- **[langgraph](/docs/concepts/architecture#langgraph)**: Powerful orchestration layer for LangChain. Use to build complex pipelines and workflows.\\n- **[langserve](/docs/concepts/architecture#langserve)**: Used to deploy LangChain Runnables as REST endpoints. Uses FastAPI. Works primarily for LangChain Runnables, does not currently integrate with LangGraph.\\n- **[LLMs (legacy)](/docs/concepts/text_llms)**: Older language models that take a string as input and return a string as output.\\n- **[Managing chat history](/docs/concepts/chat_history#managing-chat-history)**: Techniques to maintain and manage the chat history.\\n- **[OpenAI format](/docs/concepts/messages#openai-format)**: OpenAI\\'s message format for chat models.\\n- **[Propagation of RunnableConfig](/docs/concepts/runnables/#propagation-of-runnableconfig)**: Propagating configuration through Runnables. Read if working with python 3.9, 3.10 and async.\\n- **[rate-limiting](/docs/concepts/chat_models#rate-limiting)**: Client side rate limiting for chat models.\\n- **[RemoveMessage](/docs/concepts/messages/#removemessage)**: An abstraction used to remove a message from chat history, used primarily in LangGraph.\\n- **[role](/docs/concepts/messages#role)**: Represents the role (e.g., user, assistant) of a chat message.\\n- **[RunnableConfig](/docs/concepts/runnables/#runnableconfig)**: Use to pass run time information to Runnables (e.g., `run_name`, `run_id`, `tags`, `metadata`, `max_concurrency`, `recursion_limit`, `configurable`).\\n- **[Standard parameters for chat models](/docs/concepts/chat_models#standard-parameters)**: Parameters such as API key, `temperature`, and `max_tokens`.\\n- **[Standard tests](/docs/concepts/testing#standard-tests)**: A defined set of unit and integration tests that all integrations must pass.\\n- **[stream](/docs/concepts/streaming)**: Use to stream output from a Runnable or a graph.\\n- **[Tokenization](/docs/concepts/tokens)**: The process of converting data into tokens and vice versa.\\n- **[Tokens](/docs/concepts/tokens)**: The basic unit that a language model reads, processes, and generates under the hood.\\n- **[Tool artifacts](/docs/concepts/tools#tool-artifacts)**: Add artifacts to the output of a tool that will not be sent to the model, but will be available for downstream processing.\\n- **[Tool binding](/docs/concepts/tool_calling#tool-binding)**: Binding tools to models.\\n- **[@tool](/docs/concepts/tools/#create-tools-using-the-tool-decorator)**: Decorator for creating tools in LangChain.\\n- **[Toolkits](/docs/concepts/tools#toolkits)**: A collection of tools that can be used together.\\n- **[ToolMessage](/docs/concepts/messages#toolmessage)**: Represents a message that contains the results of a tool execution.\\n- **[Unit tests](/docs/concepts/testing#unit-tests)**: Tests that verify the correctness of individual components, run in isolation without access to the Internet.\\n- **[Vector stores](/docs/concepts/vectorstores)**: Datastores specialized for storing and efficiently searching vector embeddings.\\n- **[with_structured_output](/docs/concepts/structured_outputs/#structured-output-method)**: A helper method for chat models that natively support [tool calling](/docs/concepts/tool_calling) to get structured output matching a given schema specified via Pydantic, JSON schema or a function.\\n- **[with_types](/docs/concepts/runnables#with_types)**: Method to overwrite the input and output types of a runnable. Useful when working with complex LCEL chains and deploying with LangServe.\\n')]\n",
      "rank:0\n",
      "content_score_mapping:{'# Why LangChain?\\n\\nThe goal of `langchain` the Python package and LangChain the company is to make it as easy as possible for developers to build applications that reason.\\nWhile LangChain originally started as a single open source package, it has evolved into a company and a whole ecosystem.\\nThis page will talk about the LangChain ecosystem as a whole.\\nMost of the components within the LangChain ecosystem can be used by themselves - so if you feel particularly drawn to certain components but not others, that is totally fine! Pick and choose whichever components you like best for your own use case!\\n\\n## Features\\n\\nThere are several primary needs that LangChain aims to address:\\n\\n1. **Standardized component interfaces:** The growing number of [models](/docs/integrations/chat/) and [related components](/docs/integrations/vectorstores/) for AI applications has resulted in a wide variety of different APIs that developers need to learn and use.\\nThis diversity can make it challenging for developers to switch between providers or combine components when building applications.\\nLangChain exposes a standard interface for key components, making it easy to switch between providers.\\n\\n2. **Orchestration:** As applications become more complex, combining multiple components and models, there\\'s [a growing need to efficiently connect these elements into control flows](https://lilianweng.github.io/posts/2023-06-23-agent/) that can [accomplish diverse tasks](https://www.sequoiacap.com/article/generative-ais-act-o1/).\\n[Orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)) is crucial for building such applications.\\n\\n3. **Observability and evaluation:** As applications become more complex, it becomes increasingly difficult to understand what is happening within them.\\nFurthermore, the pace of development can become rate-limited by the [paradox of choice](https://en.wikipedia.org/wiki/Paradox_of_choice).\\nFor example, developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\n[Observability](https://en.wikipedia.org/wiki/Observability) and evaluations can help developers monitor their applications and rapidly answer these types of questions with confidence.\\n\\n\\n## Standardized component interfaces\\n\\nLangChain provides common interfaces for components that are central to many AI applications.\\nAs an example, all [chat models](/docs/concepts/chat_models/) implement the [BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) interface.\\nThis provides a standard way to interact with chat models, supporting important but often provider-specific features like [tool calling](/docs/concepts/tool_calling/) and [structured outputs](/docs/concepts/structured_outputs/).\\n\\n\\n### Example: chat models \\n\\nMany [model providers](/docs/concepts/chat_models/) support [tool calling](/docs/concepts/tool_calling/), a critical feature for many applications (e.g., [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/)), that allows a developer to request model responses that match a particular schema.\\nThe APIs for each provider differ. \\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to bind [tools](/docs/concepts/tools) to a model in order to support [tool calling](/docs/concepts/tool_calling/):\\n\\n```python\\n# Tool creation\\ntools = [my_tool]\\n# Tool binding\\nmodel_with_tools = model.bind_tools(tools)\\n```\\n\\nSimilarly, getting models to produce [structured outputs](/docs/concepts/structured_outputs/) is an extremely common use case. \\nProviders support different approaches for this, including [JSON mode or tool calling](https://platform.openai.com/docs/guides/structured-outputs), with different APIs.\\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to produce structured outputs using the `with_structured_output()` method:\\n\\n```python\\n# Define schema\\nschema = ...\\n# Bind schema to model\\nmodel_with_structure = model.with_structured_output(schema)\\n```\\n\\n### Example: retrievers\\n\\nIn the context of [RAG](/docs/concepts/rag/) and LLM application components, LangChain\\'s [retriever](/docs/concepts/retrievers/) interface provides a standard way to connect to many different types of data services or databases (e.g., [vector stores](/docs/concepts/vectorstores) or databases).\\nThe underlying implementation of the retriever depends on the type of data store or database you are connecting to, but all retrievers implement the [runnable interface](/docs/concepts/runnables/), meaning they can be invoked in a common manner.\\n\\n```python\\ndocuments = my_retriever.invoke(\"What is the meaning of life?\")\\n```\\n\\n## Orchestration \\n\\nWhile standardization for individual components is useful, we\\'ve increasingly seen that developers want to *combine* components into more complex applications. \\nThis motivates the need for [orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)).\\nThere are several common characteristics of LLM applications that this orchestration layer should support:\\n\\n* **Complex control flow:** The application requires complex patterns such as cycles (e.g., a loop that reiterates until a condition is met).\\n* **[Persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/):** The application needs to maintain [short-term and / or long-term memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n* **[Human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/):** The application needs human interaction, e.g., pausing, reviewing, editing, approving certain steps.\\n\\nThe recommended way to orchestrate components for complex applications is [LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/).\\nLangGraph is a library that gives developers a high degree of control by expressing the flow of the application as a set of nodes and edges.\\nLangGraph comes with built-in support for [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), [memory](https://langchain-ai.github.io/langgraph/concepts/memory/), and other features.\\nIt\\'s particularly well suited for building [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/) or [multi-agent](https://langchain-ai.github.io/langgraph/concepts/multi_agent/) applications. \\nImportantly, individual LangChain components can be used as LangGraph nodes, but you can also use LangGraph **without** using LangChain components.\\n\\n:::info[Further reading]\\n\\nHave a look at our free course, [Introduction to LangGraph](https://academy.langchain.com/courses/intro-to-langgraph), to learn more about how to use LangGraph to build complex applications.\\n\\n:::\\n\\n## Observability and evaluation\\n\\nThe pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. \\nDevelopers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\nHigh quality tracing and evaluations can help you rapidly answer these types of questions with confidence.\\n[LangSmith](https://docs.smith.langchain.com/) is our platform that supports observability and evaluation for AI applications.\\nSee our conceptual guides on [evaluations](https://docs.smith.langchain.com/concepts/evaluation) and [tracing](https://docs.smith.langchain.com/concepts/tracing) for more details.\\n\\n:::info[Further reading]\\n\\nSee our video playlist on [LangSmith tracing and evaluations](https://youtube.com/playlist?list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S&feature=shared) for more details.\\n\\n:::\\n\\n## Conclusion\\n\\nLangChain offers standard interfaces for components that are central to many AI applications, which offers a few specific advantages:\\n- **Ease of swapping providers:** It allows you to swap out different component providers without having to change the underlying code.\\n- **Advanced features:** It provides common methods for more advanced features, such as [streaming](/docs/concepts/streaming) and [tool calling](/docs/concepts/tool_calling/).\\n\\n[LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/) makes it possible to orchestrate complex applications (e.g., [agents](/docs/concepts/agents/)) and provide features like including [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), or [memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n\\n[LangSmith](https://docs.smith.langchain.com/) makes it possible to iterate with confidence on your applications, by providing LLM-specific observability and framework for testing and evaluating your application.\\n': 0.016666666666666666}\n",
      "rank:1\n",
      "content_score_mapping:{'# Why LangChain?\\n\\nThe goal of `langchain` the Python package and LangChain the company is to make it as easy as possible for developers to build applications that reason.\\nWhile LangChain originally started as a single open source package, it has evolved into a company and a whole ecosystem.\\nThis page will talk about the LangChain ecosystem as a whole.\\nMost of the components within the LangChain ecosystem can be used by themselves - so if you feel particularly drawn to certain components but not others, that is totally fine! Pick and choose whichever components you like best for your own use case!\\n\\n## Features\\n\\nThere are several primary needs that LangChain aims to address:\\n\\n1. **Standardized component interfaces:** The growing number of [models](/docs/integrations/chat/) and [related components](/docs/integrations/vectorstores/) for AI applications has resulted in a wide variety of different APIs that developers need to learn and use.\\nThis diversity can make it challenging for developers to switch between providers or combine components when building applications.\\nLangChain exposes a standard interface for key components, making it easy to switch between providers.\\n\\n2. **Orchestration:** As applications become more complex, combining multiple components and models, there\\'s [a growing need to efficiently connect these elements into control flows](https://lilianweng.github.io/posts/2023-06-23-agent/) that can [accomplish diverse tasks](https://www.sequoiacap.com/article/generative-ais-act-o1/).\\n[Orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)) is crucial for building such applications.\\n\\n3. **Observability and evaluation:** As applications become more complex, it becomes increasingly difficult to understand what is happening within them.\\nFurthermore, the pace of development can become rate-limited by the [paradox of choice](https://en.wikipedia.org/wiki/Paradox_of_choice).\\nFor example, developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\n[Observability](https://en.wikipedia.org/wiki/Observability) and evaluations can help developers monitor their applications and rapidly answer these types of questions with confidence.\\n\\n\\n## Standardized component interfaces\\n\\nLangChain provides common interfaces for components that are central to many AI applications.\\nAs an example, all [chat models](/docs/concepts/chat_models/) implement the [BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) interface.\\nThis provides a standard way to interact with chat models, supporting important but often provider-specific features like [tool calling](/docs/concepts/tool_calling/) and [structured outputs](/docs/concepts/structured_outputs/).\\n\\n\\n### Example: chat models \\n\\nMany [model providers](/docs/concepts/chat_models/) support [tool calling](/docs/concepts/tool_calling/), a critical feature for many applications (e.g., [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/)), that allows a developer to request model responses that match a particular schema.\\nThe APIs for each provider differ. \\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to bind [tools](/docs/concepts/tools) to a model in order to support [tool calling](/docs/concepts/tool_calling/):\\n\\n```python\\n# Tool creation\\ntools = [my_tool]\\n# Tool binding\\nmodel_with_tools = model.bind_tools(tools)\\n```\\n\\nSimilarly, getting models to produce [structured outputs](/docs/concepts/structured_outputs/) is an extremely common use case. \\nProviders support different approaches for this, including [JSON mode or tool calling](https://platform.openai.com/docs/guides/structured-outputs), with different APIs.\\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to produce structured outputs using the `with_structured_output()` method:\\n\\n```python\\n# Define schema\\nschema = ...\\n# Bind schema to model\\nmodel_with_structure = model.with_structured_output(schema)\\n```\\n\\n### Example: retrievers\\n\\nIn the context of [RAG](/docs/concepts/rag/) and LLM application components, LangChain\\'s [retriever](/docs/concepts/retrievers/) interface provides a standard way to connect to many different types of data services or databases (e.g., [vector stores](/docs/concepts/vectorstores) or databases).\\nThe underlying implementation of the retriever depends on the type of data store or database you are connecting to, but all retrievers implement the [runnable interface](/docs/concepts/runnables/), meaning they can be invoked in a common manner.\\n\\n```python\\ndocuments = my_retriever.invoke(\"What is the meaning of life?\")\\n```\\n\\n## Orchestration \\n\\nWhile standardization for individual components is useful, we\\'ve increasingly seen that developers want to *combine* components into more complex applications. \\nThis motivates the need for [orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)).\\nThere are several common characteristics of LLM applications that this orchestration layer should support:\\n\\n* **Complex control flow:** The application requires complex patterns such as cycles (e.g., a loop that reiterates until a condition is met).\\n* **[Persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/):** The application needs to maintain [short-term and / or long-term memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n* **[Human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/):** The application needs human interaction, e.g., pausing, reviewing, editing, approving certain steps.\\n\\nThe recommended way to orchestrate components for complex applications is [LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/).\\nLangGraph is a library that gives developers a high degree of control by expressing the flow of the application as a set of nodes and edges.\\nLangGraph comes with built-in support for [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), [memory](https://langchain-ai.github.io/langgraph/concepts/memory/), and other features.\\nIt\\'s particularly well suited for building [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/) or [multi-agent](https://langchain-ai.github.io/langgraph/concepts/multi_agent/) applications. \\nImportantly, individual LangChain components can be used as LangGraph nodes, but you can also use LangGraph **without** using LangChain components.\\n\\n:::info[Further reading]\\n\\nHave a look at our free course, [Introduction to LangGraph](https://academy.langchain.com/courses/intro-to-langgraph), to learn more about how to use LangGraph to build complex applications.\\n\\n:::\\n\\n## Observability and evaluation\\n\\nThe pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. \\nDevelopers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\nHigh quality tracing and evaluations can help you rapidly answer these types of questions with confidence.\\n[LangSmith](https://docs.smith.langchain.com/) is our platform that supports observability and evaluation for AI applications.\\nSee our conceptual guides on [evaluations](https://docs.smith.langchain.com/concepts/evaluation) and [tracing](https://docs.smith.langchain.com/concepts/tracing) for more details.\\n\\n:::info[Further reading]\\n\\nSee our video playlist on [LangSmith tracing and evaluations](https://youtube.com/playlist?list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S&feature=shared) for more details.\\n\\n:::\\n\\n## Conclusion\\n\\nLangChain offers standard interfaces for components that are central to many AI applications, which offers a few specific advantages:\\n- **Ease of swapping providers:** It allows you to swap out different component providers without having to change the underlying code.\\n- **Advanced features:** It provides common methods for more advanced features, such as [streaming](/docs/concepts/streaming) and [tool calling](/docs/concepts/tool_calling/).\\n\\n[LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/) makes it possible to orchestrate complex applications (e.g., [agents](/docs/concepts/agents/)) and provide features like including [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), or [memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n\\n[LangSmith](https://docs.smith.langchain.com/) makes it possible to iterate with confidence on your applications, by providing LLM-specific observability and framework for testing and evaluating your application.\\n': 0.016666666666666666, '---\\nsidebar_position: 0\\nsidebar_class_name: hidden\\n---\\n\\n# Introduction\\n\\n**LangChain** is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:\\n- **Development**: Build your applications using LangChain\\'s open-source [components](/docs/concepts) and [third-party integrations](/docs/integrations/providers/).\\nUse [LangGraph](/docs/concepts/architecture/#langgraph) to build stateful agents with first-class streaming and human-in-the-loop support.\\n- **Productionization**: Use [LangSmith](https://docs.smith.langchain.com/) to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\n- **Deployment**: Turn your LangGraph applications into production-ready APIs and Assistants with [LangGraph Platform](https://langchain-ai.github.io/langgraph/cloud/).\\n\\nimport ThemedImage from \\'@theme/ThemedImage\\';\\nimport useBaseUrl from \\'@docusaurus/useBaseUrl\\';\\n\\n<ThemedImage\\n  alt=\"Diagram outlining the hierarchical organization of the LangChain framework, displaying the interconnected parts across multiple layers.\"\\n  sources={{\\n    light: useBaseUrl(\\'/svg/langchain_stack_112024.svg\\'),\\n    dark: useBaseUrl(\\'/svg/langchain_stack_112024_dark.svg\\'),\\n  }}\\n  style={{ width: \"100%\" }}\\n  title=\"LangChain Framework Overview\"\\n/>\\n\\nLangChain implements a standard interface for large language models and related\\ntechnologies, such as embedding models and vector stores, and integrates with\\nhundreds of providers. See the [integrations](/docs/integrations/providers/) page for\\nmore.\\n\\nimport ChatModelTabs from \"@theme/ChatModelTabs\";\\n\\n<ChatModelTabs/>\\n\\n```python\\nmodel.invoke(\"Hello, world!\")\\n```\\n\\n:::note\\n\\nThese docs focus on the Python LangChain library. [Head here](https://js.langchain.com) for docs on the JavaScript LangChain library.\\n\\n:::\\n\\n## Architecture\\n\\nThe LangChain framework consists of multiple open-source libraries. Read more in the\\n[Architecture](/docs/concepts/architecture/) page.\\n\\n- **`langchain-core`**: Base abstractions for chat models and other components.\\n- **Integration packages** (e.g. `langchain-openai`, `langchain-anthropic`, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\\n- **`langchain`**: Chains, agents, and retrieval strategies that make up an application\\'s cognitive architecture.\\n- **`langchain-community`**: Third-party integrations that are community maintained.\\n- **`langgraph`**: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See [LangGraph documentation](https://langchain-ai.github.io/langgraph/).\\n\\n## Guides\\n\\n### [Tutorials](/docs/tutorials)\\n\\nIf you\\'re looking to build something specific or are more of a hands-on learner, check out our [tutorials section](/docs/tutorials).\\nThis is the best place to get started.\\n\\nThese are the best ones to get started with:\\n\\n- [Build a Simple LLM Application](/docs/tutorials/llm_chain)\\n- [Build a Chatbot](/docs/tutorials/chatbot)\\n- [Build an Agent](/docs/tutorials/agents)\\n- [Introduction to LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/)\\n\\nExplore the full list of LangChain tutorials [here](/docs/tutorials), and check out other [LangGraph tutorials here](https://langchain-ai.github.io/langgraph/tutorials/). To learn more about LangGraph, check out our first LangChain Academy course, *Introduction to LangGraph*, available [here](https://academy.langchain.com/courses/intro-to-langgraph).\\n\\n\\n### [How-to guides](/docs/how_to)\\n\\n[Here](/docs/how_to) you’ll find short answers to “How do I….?” types of questions.\\nThese how-to guides don’t cover topics in depth – you’ll find that material in the [Tutorials](/docs/tutorials) and the [API Reference](https://python.langchain.com/api_reference/).\\nHowever, these guides will help you quickly accomplish common tasks using [chat models](/docs/how_to/#chat-models),\\n[vector stores](/docs/how_to/#vector-stores), and other common LangChain components.\\n\\nCheck out [LangGraph-specific how-tos here](https://langchain-ai.github.io/langgraph/how-tos/).\\n\\n### [Conceptual guide](/docs/concepts)\\n\\nIntroductions to all the key parts of LangChain you’ll need to know! [Here](/docs/concepts) you\\'ll find high level explanations of all LangChain concepts.\\n\\nFor a deeper dive into LangGraph concepts, check out [this page](https://langchain-ai.github.io/langgraph/concepts/).\\n\\n### [Integrations](integrations/providers/index.mdx)\\n\\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\\nIf you\\'re looking to get up and running quickly with [chat models](/docs/integrations/chat/), [vector stores](/docs/integrations/vectorstores/),\\nor other LangChain components from a specific provider, check out our growing list of [integrations](/docs/integrations/providers/).\\n\\n\\n### [API reference](https://python.langchain.com/api_reference/)\\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\\n\\n## Ecosystem\\n\\n### [🦜🛠️ LangSmith](https://docs.smith.langchain.com)\\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\\n\\n### [🦜🕸️ LangGraph](https://langchain-ai.github.io/langgraph)\\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by Linkedin, Uber, Klarna, GitLab, and many more.\\n\\n## Additional resources\\n\\n### [Versions](/docs/versions/v0_3/)\\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\\n\\n### [Security](/docs/security)\\nRead up on [security](/docs/security) best practices to make sure you\\'re developing safely with LangChain.\\n\\n### [Contributing](contributing/index.mdx)\\nCheck out the developer\\'s guide for guidelines on contributing and help getting your dev environment set up.\\n': 0.01639344262295082}\n",
      "rank:2\n",
      "content_score_mapping:{'# Why LangChain?\\n\\nThe goal of `langchain` the Python package and LangChain the company is to make it as easy as possible for developers to build applications that reason.\\nWhile LangChain originally started as a single open source package, it has evolved into a company and a whole ecosystem.\\nThis page will talk about the LangChain ecosystem as a whole.\\nMost of the components within the LangChain ecosystem can be used by themselves - so if you feel particularly drawn to certain components but not others, that is totally fine! Pick and choose whichever components you like best for your own use case!\\n\\n## Features\\n\\nThere are several primary needs that LangChain aims to address:\\n\\n1. **Standardized component interfaces:** The growing number of [models](/docs/integrations/chat/) and [related components](/docs/integrations/vectorstores/) for AI applications has resulted in a wide variety of different APIs that developers need to learn and use.\\nThis diversity can make it challenging for developers to switch between providers or combine components when building applications.\\nLangChain exposes a standard interface for key components, making it easy to switch between providers.\\n\\n2. **Orchestration:** As applications become more complex, combining multiple components and models, there\\'s [a growing need to efficiently connect these elements into control flows](https://lilianweng.github.io/posts/2023-06-23-agent/) that can [accomplish diverse tasks](https://www.sequoiacap.com/article/generative-ais-act-o1/).\\n[Orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)) is crucial for building such applications.\\n\\n3. **Observability and evaluation:** As applications become more complex, it becomes increasingly difficult to understand what is happening within them.\\nFurthermore, the pace of development can become rate-limited by the [paradox of choice](https://en.wikipedia.org/wiki/Paradox_of_choice).\\nFor example, developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\n[Observability](https://en.wikipedia.org/wiki/Observability) and evaluations can help developers monitor their applications and rapidly answer these types of questions with confidence.\\n\\n\\n## Standardized component interfaces\\n\\nLangChain provides common interfaces for components that are central to many AI applications.\\nAs an example, all [chat models](/docs/concepts/chat_models/) implement the [BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) interface.\\nThis provides a standard way to interact with chat models, supporting important but often provider-specific features like [tool calling](/docs/concepts/tool_calling/) and [structured outputs](/docs/concepts/structured_outputs/).\\n\\n\\n### Example: chat models \\n\\nMany [model providers](/docs/concepts/chat_models/) support [tool calling](/docs/concepts/tool_calling/), a critical feature for many applications (e.g., [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/)), that allows a developer to request model responses that match a particular schema.\\nThe APIs for each provider differ. \\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to bind [tools](/docs/concepts/tools) to a model in order to support [tool calling](/docs/concepts/tool_calling/):\\n\\n```python\\n# Tool creation\\ntools = [my_tool]\\n# Tool binding\\nmodel_with_tools = model.bind_tools(tools)\\n```\\n\\nSimilarly, getting models to produce [structured outputs](/docs/concepts/structured_outputs/) is an extremely common use case. \\nProviders support different approaches for this, including [JSON mode or tool calling](https://platform.openai.com/docs/guides/structured-outputs), with different APIs.\\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to produce structured outputs using the `with_structured_output()` method:\\n\\n```python\\n# Define schema\\nschema = ...\\n# Bind schema to model\\nmodel_with_structure = model.with_structured_output(schema)\\n```\\n\\n### Example: retrievers\\n\\nIn the context of [RAG](/docs/concepts/rag/) and LLM application components, LangChain\\'s [retriever](/docs/concepts/retrievers/) interface provides a standard way to connect to many different types of data services or databases (e.g., [vector stores](/docs/concepts/vectorstores) or databases).\\nThe underlying implementation of the retriever depends on the type of data store or database you are connecting to, but all retrievers implement the [runnable interface](/docs/concepts/runnables/), meaning they can be invoked in a common manner.\\n\\n```python\\ndocuments = my_retriever.invoke(\"What is the meaning of life?\")\\n```\\n\\n## Orchestration \\n\\nWhile standardization for individual components is useful, we\\'ve increasingly seen that developers want to *combine* components into more complex applications. \\nThis motivates the need for [orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)).\\nThere are several common characteristics of LLM applications that this orchestration layer should support:\\n\\n* **Complex control flow:** The application requires complex patterns such as cycles (e.g., a loop that reiterates until a condition is met).\\n* **[Persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/):** The application needs to maintain [short-term and / or long-term memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n* **[Human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/):** The application needs human interaction, e.g., pausing, reviewing, editing, approving certain steps.\\n\\nThe recommended way to orchestrate components for complex applications is [LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/).\\nLangGraph is a library that gives developers a high degree of control by expressing the flow of the application as a set of nodes and edges.\\nLangGraph comes with built-in support for [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), [memory](https://langchain-ai.github.io/langgraph/concepts/memory/), and other features.\\nIt\\'s particularly well suited for building [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/) or [multi-agent](https://langchain-ai.github.io/langgraph/concepts/multi_agent/) applications. \\nImportantly, individual LangChain components can be used as LangGraph nodes, but you can also use LangGraph **without** using LangChain components.\\n\\n:::info[Further reading]\\n\\nHave a look at our free course, [Introduction to LangGraph](https://academy.langchain.com/courses/intro-to-langgraph), to learn more about how to use LangGraph to build complex applications.\\n\\n:::\\n\\n## Observability and evaluation\\n\\nThe pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. \\nDevelopers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\nHigh quality tracing and evaluations can help you rapidly answer these types of questions with confidence.\\n[LangSmith](https://docs.smith.langchain.com/) is our platform that supports observability and evaluation for AI applications.\\nSee our conceptual guides on [evaluations](https://docs.smith.langchain.com/concepts/evaluation) and [tracing](https://docs.smith.langchain.com/concepts/tracing) for more details.\\n\\n:::info[Further reading]\\n\\nSee our video playlist on [LangSmith tracing and evaluations](https://youtube.com/playlist?list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S&feature=shared) for more details.\\n\\n:::\\n\\n## Conclusion\\n\\nLangChain offers standard interfaces for components that are central to many AI applications, which offers a few specific advantages:\\n- **Ease of swapping providers:** It allows you to swap out different component providers without having to change the underlying code.\\n- **Advanced features:** It provides common methods for more advanced features, such as [streaming](/docs/concepts/streaming) and [tool calling](/docs/concepts/tool_calling/).\\n\\n[LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/) makes it possible to orchestrate complex applications (e.g., [agents](/docs/concepts/agents/)) and provide features like including [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), or [memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n\\n[LangSmith](https://docs.smith.langchain.com/) makes it possible to iterate with confidence on your applications, by providing LLM-specific observability and framework for testing and evaluating your application.\\n': 0.016666666666666666, '---\\nsidebar_position: 0\\nsidebar_class_name: hidden\\n---\\n\\n# Introduction\\n\\n**LangChain** is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:\\n- **Development**: Build your applications using LangChain\\'s open-source [components](/docs/concepts) and [third-party integrations](/docs/integrations/providers/).\\nUse [LangGraph](/docs/concepts/architecture/#langgraph) to build stateful agents with first-class streaming and human-in-the-loop support.\\n- **Productionization**: Use [LangSmith](https://docs.smith.langchain.com/) to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\n- **Deployment**: Turn your LangGraph applications into production-ready APIs and Assistants with [LangGraph Platform](https://langchain-ai.github.io/langgraph/cloud/).\\n\\nimport ThemedImage from \\'@theme/ThemedImage\\';\\nimport useBaseUrl from \\'@docusaurus/useBaseUrl\\';\\n\\n<ThemedImage\\n  alt=\"Diagram outlining the hierarchical organization of the LangChain framework, displaying the interconnected parts across multiple layers.\"\\n  sources={{\\n    light: useBaseUrl(\\'/svg/langchain_stack_112024.svg\\'),\\n    dark: useBaseUrl(\\'/svg/langchain_stack_112024_dark.svg\\'),\\n  }}\\n  style={{ width: \"100%\" }}\\n  title=\"LangChain Framework Overview\"\\n/>\\n\\nLangChain implements a standard interface for large language models and related\\ntechnologies, such as embedding models and vector stores, and integrates with\\nhundreds of providers. See the [integrations](/docs/integrations/providers/) page for\\nmore.\\n\\nimport ChatModelTabs from \"@theme/ChatModelTabs\";\\n\\n<ChatModelTabs/>\\n\\n```python\\nmodel.invoke(\"Hello, world!\")\\n```\\n\\n:::note\\n\\nThese docs focus on the Python LangChain library. [Head here](https://js.langchain.com) for docs on the JavaScript LangChain library.\\n\\n:::\\n\\n## Architecture\\n\\nThe LangChain framework consists of multiple open-source libraries. Read more in the\\n[Architecture](/docs/concepts/architecture/) page.\\n\\n- **`langchain-core`**: Base abstractions for chat models and other components.\\n- **Integration packages** (e.g. `langchain-openai`, `langchain-anthropic`, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\\n- **`langchain`**: Chains, agents, and retrieval strategies that make up an application\\'s cognitive architecture.\\n- **`langchain-community`**: Third-party integrations that are community maintained.\\n- **`langgraph`**: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See [LangGraph documentation](https://langchain-ai.github.io/langgraph/).\\n\\n## Guides\\n\\n### [Tutorials](/docs/tutorials)\\n\\nIf you\\'re looking to build something specific or are more of a hands-on learner, check out our [tutorials section](/docs/tutorials).\\nThis is the best place to get started.\\n\\nThese are the best ones to get started with:\\n\\n- [Build a Simple LLM Application](/docs/tutorials/llm_chain)\\n- [Build a Chatbot](/docs/tutorials/chatbot)\\n- [Build an Agent](/docs/tutorials/agents)\\n- [Introduction to LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/)\\n\\nExplore the full list of LangChain tutorials [here](/docs/tutorials), and check out other [LangGraph tutorials here](https://langchain-ai.github.io/langgraph/tutorials/). To learn more about LangGraph, check out our first LangChain Academy course, *Introduction to LangGraph*, available [here](https://academy.langchain.com/courses/intro-to-langgraph).\\n\\n\\n### [How-to guides](/docs/how_to)\\n\\n[Here](/docs/how_to) you’ll find short answers to “How do I….?” types of questions.\\nThese how-to guides don’t cover topics in depth – you’ll find that material in the [Tutorials](/docs/tutorials) and the [API Reference](https://python.langchain.com/api_reference/).\\nHowever, these guides will help you quickly accomplish common tasks using [chat models](/docs/how_to/#chat-models),\\n[vector stores](/docs/how_to/#vector-stores), and other common LangChain components.\\n\\nCheck out [LangGraph-specific how-tos here](https://langchain-ai.github.io/langgraph/how-tos/).\\n\\n### [Conceptual guide](/docs/concepts)\\n\\nIntroductions to all the key parts of LangChain you’ll need to know! [Here](/docs/concepts) you\\'ll find high level explanations of all LangChain concepts.\\n\\nFor a deeper dive into LangGraph concepts, check out [this page](https://langchain-ai.github.io/langgraph/concepts/).\\n\\n### [Integrations](integrations/providers/index.mdx)\\n\\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\\nIf you\\'re looking to get up and running quickly with [chat models](/docs/integrations/chat/), [vector stores](/docs/integrations/vectorstores/),\\nor other LangChain components from a specific provider, check out our growing list of [integrations](/docs/integrations/providers/).\\n\\n\\n### [API reference](https://python.langchain.com/api_reference/)\\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\\n\\n## Ecosystem\\n\\n### [🦜🛠️ LangSmith](https://docs.smith.langchain.com)\\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\\n\\n### [🦜🕸️ LangGraph](https://langchain-ai.github.io/langgraph)\\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by Linkedin, Uber, Klarna, GitLab, and many more.\\n\\n## Additional resources\\n\\n### [Versions](/docs/versions/v0_3/)\\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\\n\\n### [Security](/docs/security)\\nRead up on [security](/docs/security) best practices to make sure you\\'re developing safely with LangChain.\\n\\n### [Contributing](contributing/index.mdx)\\nCheck out the developer\\'s guide for guidelines on contributing and help getting your dev environment set up.\\n': 0.01639344262295082, '# LangChain Expression Language (LCEL)\\n\\n:::info Prerequisites\\n* [Runnable Interface](/docs/concepts/runnables)\\n:::\\n\\nThe **L**ang**C**hain **E**xpression **L**anguage (LCEL) takes a [declarative](https://en.wikipedia.org/wiki/Declarative_programming) approach to building new [Runnables](/docs/concepts/runnables) from existing Runnables.\\n\\nThis means that you describe what *should* happen, rather than *how* it should happen, allowing LangChain to optimize the run-time execution of the chains.\\n\\nWe often refer to a `Runnable` created using LCEL as a \"chain\". It\\'s important to remember that a \"chain\" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).\\n\\n:::note\\n* The [LCEL cheatsheet](/docs/how_to/lcel_cheatsheet/) shows common patterns that involve the Runnable interface and LCEL expressions.\\n* Please see the following list of [how-to guides](/docs/how_to/#langchain-expression-language-lcel) that cover common tasks with LCEL.\\n* A list of built-in `Runnables` can be found in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html). Many of these Runnables are useful when composing custom \"chains\" in LangChain using LCEL.\\n:::\\n\\n## Benefits of LCEL\\n\\nLangChain optimizes the run-time execution of chains built with LCEL in a number of ways:\\n\\n- **Optimized parallel execution**: Run Runnables in parallel using [RunnableParallel](#runnableparallel) or run multiple inputs through a given chain in parallel using the [Runnable Batch API](/docs/concepts/runnables/#optimized-parallel-execution-batch). Parallel execution can significantly reduce the latency as processing can be done in parallel instead of sequentially.\\n- **Guaranteed Async support**: Any chain built with LCEL can be run asynchronously using the [Runnable Async API](/docs/concepts/runnables/#asynchronous-support). This can be useful when running chains in a server environment where you want to handle large number of requests concurrently.\\n- **Simplify streaming**: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(time elapsed until the first chunk of output from a [chat model](/docs/concepts/chat_models) or [llm](/docs/concepts/text_llms) comes out).\\n\\nOther benefits include:\\n\\n- [**Seamless LangSmith tracing**](https://docs.smith.langchain.com)\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, **all** steps are automatically logged to [LangSmith](https://docs.smith.langchain.com/) for maximum observability and debuggability.\\n- **Standard API**: Because all chains are built using the Runnable interface, they can be used in the same way as any other Runnable.\\n- [**Deployable with LangServe**](/docs/concepts/architecture#langserve): Chains built with LCEL can be deployed using for production use.\\n\\n## Should I use LCEL?\\n\\nLCEL is an [orchestration solution](https://en.wikipedia.org/wiki/Orchestration_(computing)) -- it allows LangChain to handle run-time execution of chains in an optimized way.\\n\\nWhile we have seen users run chains with hundreds of steps in production, we generally recommend using LCEL for simpler orchestration tasks. When the application requires complex state management, branching, cycles or multiple agents, we recommend that users take advantage of [LangGraph](/docs/concepts/architecture#langgraph).\\n\\nIn LangGraph, users define graphs that specify the application\\'s flow. This allows users to keep using LCEL within individual nodes when LCEL is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.\\n\\nHere are some guidelines:\\n\\n* If you are making a single LLM call, you don\\'t need LCEL; instead call the underlying [chat model](/docs/concepts/chat_models) directly.\\n* If you have a simple chain (e.g., prompt + llm + parser, simple retrieval set up etc.), LCEL is a reasonable fit, if you\\'re taking advantage of the LCEL benefits.\\n* If you\\'re building a complex chain (e.g., with branching, cycles, multiple agents, etc.) use [LangGraph](/docs/concepts/architecture#langgraph) instead. Remember that you can always use LCEL within individual nodes in LangGraph.\\n\\n## Composition Primitives\\n\\n`LCEL` chains are built by composing existing `Runnables` together. The two main composition primitives are [RunnableSequence](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableSequence.html#langchain_core.runnables.base.RunnableSequence) and [RunnableParallel](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableParallel.html#langchain_core.runnables.base.RunnableParallel).\\n\\nMany other composition primitives (e.g., [RunnableAssign](\\nhttps://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnableAssign.html#langchain_core.runnables.passthrough.RunnableAssign\\n)) can be thought of as variations of these two primitives.\\n\\n:::note\\nYou can find a list of all composition primitives in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html).\\n:::\\n\\n### RunnableSequence\\n\\n`RunnableSequence` is a composition primitive that allows you \"chain\" multiple runnables sequentially, with the output of one runnable serving as the input to the next.\\n\\n```python\\nfrom langchain_core.runnables import RunnableSequence\\nchain = RunnableSequence([runnable1, runnable2])\\n```\\n\\nInvoking the `chain` with some input:\\n\\n```python\\nfinal_output = chain.invoke(some_input)\\n```\\n\\ncorresponds to the following:\\n\\n```python\\noutput1 = runnable1.invoke(some_input)\\nfinal_output = runnable2.invoke(output1)\\n```\\n\\n:::note\\n`runnable1` and `runnable2` are placeholders for any `Runnable` that you want to chain together.\\n:::\\n\\n### RunnableParallel\\n\\n`RunnableParallel` is a composition primitive that allows you to run multiple runnables concurrently, with the same input provided to each.\\n\\n```python\\nfrom langchain_core.runnables import RunnableParallel\\nchain = RunnableParallel({\\n    \"key1\": runnable1,\\n    \"key2\": runnable2,\\n})\\n```\\n\\nInvoking the `chain` with some input:\\n\\n```python\\nfinal_output = chain.invoke(some_input)\\n```\\n\\nWill yield a `final_output` dictionary with the same keys as the input dictionary, but with the values replaced by the output of the corresponding runnable.\\n\\n```python\\n{\\n    \"key1\": runnable1.invoke(some_input),\\n    \"key2\": runnable2.invoke(some_input),\\n}\\n```\\n\\nRecall, that the runnables are executed in parallel, so while the result is the same as\\ndictionary comprehension shown above, the execution time is much faster.\\n\\n:::note\\n`RunnableParallel`supports both synchronous and asynchronous execution (as all `Runnables` do).\\n\\n* For synchronous execution, `RunnableParallel` uses a [ThreadPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor) to run the runnables concurrently.\\n* For asynchronous execution, `RunnableParallel` uses [asyncio.gather](https://docs.python.org/3/library/asyncio.html#asyncio.gather) to run the runnables concurrently.\\n:::\\n\\n## Composition Syntax\\n\\nThe usage of `RunnableSequence` and `RunnableParallel` is so common that we created a shorthand syntax for using them. This helps\\nto make the code more readable and concise.\\n\\n### The `|` operator\\n\\nWe have [overloaded](https://docs.python.org/3/reference/datamodel.html#special-method-names) the `|` operator to create a `RunnableSequence` from two `Runnables`.\\n\\n```python\\nchain = runnable1 | runnable2\\n```\\n\\nis Equivalent to:\\n\\n```python\\nchain = RunnableSequence([runnable1, runnable2])\\n```\\n\\n### The `.pipe` method\\n\\nIf you have moral qualms with operator overloading, you can use the `.pipe` method instead. This is equivalent to the `|` operator.\\n\\n```python\\nchain = runnable1.pipe(runnable2)\\n```\\n\\n### Coercion\\n\\nLCEL applies automatic type coercion to make it easier to compose chains.\\n\\nIf you do not understand the type coercion, you can always use the `RunnableSequence` and `RunnableParallel` classes directly.\\n\\nThis will make the code more verbose, but it will also make it more explicit.\\n\\n#### Dictionary to RunnableParallel\\n\\nInside an LCEL expression, a dictionary is automatically converted to a `RunnableParallel`.\\n\\nFor example, the following code:\\n\\n```python\\nmapping = {\\n    \"key1\": runnable1,\\n    \"key2\": runnable2,\\n}\\n\\nchain = mapping | runnable3\\n```\\n\\nIt gets automatically converted to the following:\\n\\n```python\\nchain = RunnableSequence([RunnableParallel(mapping), runnable3])\\n```\\n\\n:::caution\\nYou have to be careful because the `mapping` dictionary is not a `RunnableParallel` object, it is just a dictionary. This means that the following code will raise an `AttributeError`:\\n\\n```python\\nmapping.invoke(some_input)\\n```\\n:::\\n\\n#### Function to RunnableLambda\\n\\nInside an LCEL expression, a function is automatically converted to a `RunnableLambda`.\\n\\n```\\ndef some_func(x):\\n    return x\\n\\nchain = some_func | runnable1\\n```\\n\\nIt gets automatically converted to the following:\\n\\n```python\\nchain = RunnableSequence([RunnableLambda(some_func), runnable1])\\n```\\n\\n:::caution\\nYou have to be careful because the lambda function is not a `RunnableLambda` object, it is just a function. This means that the following code will raise an `AttributeError`:\\n\\n```python\\nlambda x: x + 1.invoke(some_input)\\n```\\n:::\\n\\n## Legacy chains\\n\\nLCEL aims to provide consistency around behavior and customization over legacy subclassed chains such as `LLMChain` and\\n`ConversationalRetrievalChain`. Many of these legacy chains hide important details like prompts, and as a wider variety\\nof viable models emerge, customization has become more and more important.\\n\\nIf you are currently using one of these legacy chains, please see [this guide for guidance on how to migrate](/docs/versions/migrating_chains).\\n\\nFor guides on how to do specific tasks with LCEL, check out [the relevant how-to guides](/docs/how_to/#langchain-expression-language-lcel).\\n': 0.016129032258064516}\n",
      "rank:3\n",
      "content_score_mapping:{'# Why LangChain?\\n\\nThe goal of `langchain` the Python package and LangChain the company is to make it as easy as possible for developers to build applications that reason.\\nWhile LangChain originally started as a single open source package, it has evolved into a company and a whole ecosystem.\\nThis page will talk about the LangChain ecosystem as a whole.\\nMost of the components within the LangChain ecosystem can be used by themselves - so if you feel particularly drawn to certain components but not others, that is totally fine! Pick and choose whichever components you like best for your own use case!\\n\\n## Features\\n\\nThere are several primary needs that LangChain aims to address:\\n\\n1. **Standardized component interfaces:** The growing number of [models](/docs/integrations/chat/) and [related components](/docs/integrations/vectorstores/) for AI applications has resulted in a wide variety of different APIs that developers need to learn and use.\\nThis diversity can make it challenging for developers to switch between providers or combine components when building applications.\\nLangChain exposes a standard interface for key components, making it easy to switch between providers.\\n\\n2. **Orchestration:** As applications become more complex, combining multiple components and models, there\\'s [a growing need to efficiently connect these elements into control flows](https://lilianweng.github.io/posts/2023-06-23-agent/) that can [accomplish diverse tasks](https://www.sequoiacap.com/article/generative-ais-act-o1/).\\n[Orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)) is crucial for building such applications.\\n\\n3. **Observability and evaluation:** As applications become more complex, it becomes increasingly difficult to understand what is happening within them.\\nFurthermore, the pace of development can become rate-limited by the [paradox of choice](https://en.wikipedia.org/wiki/Paradox_of_choice).\\nFor example, developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\n[Observability](https://en.wikipedia.org/wiki/Observability) and evaluations can help developers monitor their applications and rapidly answer these types of questions with confidence.\\n\\n\\n## Standardized component interfaces\\n\\nLangChain provides common interfaces for components that are central to many AI applications.\\nAs an example, all [chat models](/docs/concepts/chat_models/) implement the [BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) interface.\\nThis provides a standard way to interact with chat models, supporting important but often provider-specific features like [tool calling](/docs/concepts/tool_calling/) and [structured outputs](/docs/concepts/structured_outputs/).\\n\\n\\n### Example: chat models \\n\\nMany [model providers](/docs/concepts/chat_models/) support [tool calling](/docs/concepts/tool_calling/), a critical feature for many applications (e.g., [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/)), that allows a developer to request model responses that match a particular schema.\\nThe APIs for each provider differ. \\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to bind [tools](/docs/concepts/tools) to a model in order to support [tool calling](/docs/concepts/tool_calling/):\\n\\n```python\\n# Tool creation\\ntools = [my_tool]\\n# Tool binding\\nmodel_with_tools = model.bind_tools(tools)\\n```\\n\\nSimilarly, getting models to produce [structured outputs](/docs/concepts/structured_outputs/) is an extremely common use case. \\nProviders support different approaches for this, including [JSON mode or tool calling](https://platform.openai.com/docs/guides/structured-outputs), with different APIs.\\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to produce structured outputs using the `with_structured_output()` method:\\n\\n```python\\n# Define schema\\nschema = ...\\n# Bind schema to model\\nmodel_with_structure = model.with_structured_output(schema)\\n```\\n\\n### Example: retrievers\\n\\nIn the context of [RAG](/docs/concepts/rag/) and LLM application components, LangChain\\'s [retriever](/docs/concepts/retrievers/) interface provides a standard way to connect to many different types of data services or databases (e.g., [vector stores](/docs/concepts/vectorstores) or databases).\\nThe underlying implementation of the retriever depends on the type of data store or database you are connecting to, but all retrievers implement the [runnable interface](/docs/concepts/runnables/), meaning they can be invoked in a common manner.\\n\\n```python\\ndocuments = my_retriever.invoke(\"What is the meaning of life?\")\\n```\\n\\n## Orchestration \\n\\nWhile standardization for individual components is useful, we\\'ve increasingly seen that developers want to *combine* components into more complex applications. \\nThis motivates the need for [orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)).\\nThere are several common characteristics of LLM applications that this orchestration layer should support:\\n\\n* **Complex control flow:** The application requires complex patterns such as cycles (e.g., a loop that reiterates until a condition is met).\\n* **[Persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/):** The application needs to maintain [short-term and / or long-term memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n* **[Human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/):** The application needs human interaction, e.g., pausing, reviewing, editing, approving certain steps.\\n\\nThe recommended way to orchestrate components for complex applications is [LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/).\\nLangGraph is a library that gives developers a high degree of control by expressing the flow of the application as a set of nodes and edges.\\nLangGraph comes with built-in support for [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), [memory](https://langchain-ai.github.io/langgraph/concepts/memory/), and other features.\\nIt\\'s particularly well suited for building [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/) or [multi-agent](https://langchain-ai.github.io/langgraph/concepts/multi_agent/) applications. \\nImportantly, individual LangChain components can be used as LangGraph nodes, but you can also use LangGraph **without** using LangChain components.\\n\\n:::info[Further reading]\\n\\nHave a look at our free course, [Introduction to LangGraph](https://academy.langchain.com/courses/intro-to-langgraph), to learn more about how to use LangGraph to build complex applications.\\n\\n:::\\n\\n## Observability and evaluation\\n\\nThe pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. \\nDevelopers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\nHigh quality tracing and evaluations can help you rapidly answer these types of questions with confidence.\\n[LangSmith](https://docs.smith.langchain.com/) is our platform that supports observability and evaluation for AI applications.\\nSee our conceptual guides on [evaluations](https://docs.smith.langchain.com/concepts/evaluation) and [tracing](https://docs.smith.langchain.com/concepts/tracing) for more details.\\n\\n:::info[Further reading]\\n\\nSee our video playlist on [LangSmith tracing and evaluations](https://youtube.com/playlist?list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S&feature=shared) for more details.\\n\\n:::\\n\\n## Conclusion\\n\\nLangChain offers standard interfaces for components that are central to many AI applications, which offers a few specific advantages:\\n- **Ease of swapping providers:** It allows you to swap out different component providers without having to change the underlying code.\\n- **Advanced features:** It provides common methods for more advanced features, such as [streaming](/docs/concepts/streaming) and [tool calling](/docs/concepts/tool_calling/).\\n\\n[LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/) makes it possible to orchestrate complex applications (e.g., [agents](/docs/concepts/agents/)) and provide features like including [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), or [memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n\\n[LangSmith](https://docs.smith.langchain.com/) makes it possible to iterate with confidence on your applications, by providing LLM-specific observability and framework for testing and evaluating your application.\\n': 0.016666666666666666, '---\\nsidebar_position: 0\\nsidebar_class_name: hidden\\n---\\n\\n# Introduction\\n\\n**LangChain** is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:\\n- **Development**: Build your applications using LangChain\\'s open-source [components](/docs/concepts) and [third-party integrations](/docs/integrations/providers/).\\nUse [LangGraph](/docs/concepts/architecture/#langgraph) to build stateful agents with first-class streaming and human-in-the-loop support.\\n- **Productionization**: Use [LangSmith](https://docs.smith.langchain.com/) to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\n- **Deployment**: Turn your LangGraph applications into production-ready APIs and Assistants with [LangGraph Platform](https://langchain-ai.github.io/langgraph/cloud/).\\n\\nimport ThemedImage from \\'@theme/ThemedImage\\';\\nimport useBaseUrl from \\'@docusaurus/useBaseUrl\\';\\n\\n<ThemedImage\\n  alt=\"Diagram outlining the hierarchical organization of the LangChain framework, displaying the interconnected parts across multiple layers.\"\\n  sources={{\\n    light: useBaseUrl(\\'/svg/langchain_stack_112024.svg\\'),\\n    dark: useBaseUrl(\\'/svg/langchain_stack_112024_dark.svg\\'),\\n  }}\\n  style={{ width: \"100%\" }}\\n  title=\"LangChain Framework Overview\"\\n/>\\n\\nLangChain implements a standard interface for large language models and related\\ntechnologies, such as embedding models and vector stores, and integrates with\\nhundreds of providers. See the [integrations](/docs/integrations/providers/) page for\\nmore.\\n\\nimport ChatModelTabs from \"@theme/ChatModelTabs\";\\n\\n<ChatModelTabs/>\\n\\n```python\\nmodel.invoke(\"Hello, world!\")\\n```\\n\\n:::note\\n\\nThese docs focus on the Python LangChain library. [Head here](https://js.langchain.com) for docs on the JavaScript LangChain library.\\n\\n:::\\n\\n## Architecture\\n\\nThe LangChain framework consists of multiple open-source libraries. Read more in the\\n[Architecture](/docs/concepts/architecture/) page.\\n\\n- **`langchain-core`**: Base abstractions for chat models and other components.\\n- **Integration packages** (e.g. `langchain-openai`, `langchain-anthropic`, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\\n- **`langchain`**: Chains, agents, and retrieval strategies that make up an application\\'s cognitive architecture.\\n- **`langchain-community`**: Third-party integrations that are community maintained.\\n- **`langgraph`**: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See [LangGraph documentation](https://langchain-ai.github.io/langgraph/).\\n\\n## Guides\\n\\n### [Tutorials](/docs/tutorials)\\n\\nIf you\\'re looking to build something specific or are more of a hands-on learner, check out our [tutorials section](/docs/tutorials).\\nThis is the best place to get started.\\n\\nThese are the best ones to get started with:\\n\\n- [Build a Simple LLM Application](/docs/tutorials/llm_chain)\\n- [Build a Chatbot](/docs/tutorials/chatbot)\\n- [Build an Agent](/docs/tutorials/agents)\\n- [Introduction to LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/)\\n\\nExplore the full list of LangChain tutorials [here](/docs/tutorials), and check out other [LangGraph tutorials here](https://langchain-ai.github.io/langgraph/tutorials/). To learn more about LangGraph, check out our first LangChain Academy course, *Introduction to LangGraph*, available [here](https://academy.langchain.com/courses/intro-to-langgraph).\\n\\n\\n### [How-to guides](/docs/how_to)\\n\\n[Here](/docs/how_to) you’ll find short answers to “How do I….?” types of questions.\\nThese how-to guides don’t cover topics in depth – you’ll find that material in the [Tutorials](/docs/tutorials) and the [API Reference](https://python.langchain.com/api_reference/).\\nHowever, these guides will help you quickly accomplish common tasks using [chat models](/docs/how_to/#chat-models),\\n[vector stores](/docs/how_to/#vector-stores), and other common LangChain components.\\n\\nCheck out [LangGraph-specific how-tos here](https://langchain-ai.github.io/langgraph/how-tos/).\\n\\n### [Conceptual guide](/docs/concepts)\\n\\nIntroductions to all the key parts of LangChain you’ll need to know! [Here](/docs/concepts) you\\'ll find high level explanations of all LangChain concepts.\\n\\nFor a deeper dive into LangGraph concepts, check out [this page](https://langchain-ai.github.io/langgraph/concepts/).\\n\\n### [Integrations](integrations/providers/index.mdx)\\n\\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\\nIf you\\'re looking to get up and running quickly with [chat models](/docs/integrations/chat/), [vector stores](/docs/integrations/vectorstores/),\\nor other LangChain components from a specific provider, check out our growing list of [integrations](/docs/integrations/providers/).\\n\\n\\n### [API reference](https://python.langchain.com/api_reference/)\\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\\n\\n## Ecosystem\\n\\n### [🦜🛠️ LangSmith](https://docs.smith.langchain.com)\\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\\n\\n### [🦜🕸️ LangGraph](https://langchain-ai.github.io/langgraph)\\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by Linkedin, Uber, Klarna, GitLab, and many more.\\n\\n## Additional resources\\n\\n### [Versions](/docs/versions/v0_3/)\\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\\n\\n### [Security](/docs/security)\\nRead up on [security](/docs/security) best practices to make sure you\\'re developing safely with LangChain.\\n\\n### [Contributing](contributing/index.mdx)\\nCheck out the developer\\'s guide for guidelines on contributing and help getting your dev environment set up.\\n': 0.01639344262295082, '# LangChain Expression Language (LCEL)\\n\\n:::info Prerequisites\\n* [Runnable Interface](/docs/concepts/runnables)\\n:::\\n\\nThe **L**ang**C**hain **E**xpression **L**anguage (LCEL) takes a [declarative](https://en.wikipedia.org/wiki/Declarative_programming) approach to building new [Runnables](/docs/concepts/runnables) from existing Runnables.\\n\\nThis means that you describe what *should* happen, rather than *how* it should happen, allowing LangChain to optimize the run-time execution of the chains.\\n\\nWe often refer to a `Runnable` created using LCEL as a \"chain\". It\\'s important to remember that a \"chain\" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).\\n\\n:::note\\n* The [LCEL cheatsheet](/docs/how_to/lcel_cheatsheet/) shows common patterns that involve the Runnable interface and LCEL expressions.\\n* Please see the following list of [how-to guides](/docs/how_to/#langchain-expression-language-lcel) that cover common tasks with LCEL.\\n* A list of built-in `Runnables` can be found in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html). Many of these Runnables are useful when composing custom \"chains\" in LangChain using LCEL.\\n:::\\n\\n## Benefits of LCEL\\n\\nLangChain optimizes the run-time execution of chains built with LCEL in a number of ways:\\n\\n- **Optimized parallel execution**: Run Runnables in parallel using [RunnableParallel](#runnableparallel) or run multiple inputs through a given chain in parallel using the [Runnable Batch API](/docs/concepts/runnables/#optimized-parallel-execution-batch). Parallel execution can significantly reduce the latency as processing can be done in parallel instead of sequentially.\\n- **Guaranteed Async support**: Any chain built with LCEL can be run asynchronously using the [Runnable Async API](/docs/concepts/runnables/#asynchronous-support). This can be useful when running chains in a server environment where you want to handle large number of requests concurrently.\\n- **Simplify streaming**: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(time elapsed until the first chunk of output from a [chat model](/docs/concepts/chat_models) or [llm](/docs/concepts/text_llms) comes out).\\n\\nOther benefits include:\\n\\n- [**Seamless LangSmith tracing**](https://docs.smith.langchain.com)\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, **all** steps are automatically logged to [LangSmith](https://docs.smith.langchain.com/) for maximum observability and debuggability.\\n- **Standard API**: Because all chains are built using the Runnable interface, they can be used in the same way as any other Runnable.\\n- [**Deployable with LangServe**](/docs/concepts/architecture#langserve): Chains built with LCEL can be deployed using for production use.\\n\\n## Should I use LCEL?\\n\\nLCEL is an [orchestration solution](https://en.wikipedia.org/wiki/Orchestration_(computing)) -- it allows LangChain to handle run-time execution of chains in an optimized way.\\n\\nWhile we have seen users run chains with hundreds of steps in production, we generally recommend using LCEL for simpler orchestration tasks. When the application requires complex state management, branching, cycles or multiple agents, we recommend that users take advantage of [LangGraph](/docs/concepts/architecture#langgraph).\\n\\nIn LangGraph, users define graphs that specify the application\\'s flow. This allows users to keep using LCEL within individual nodes when LCEL is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.\\n\\nHere are some guidelines:\\n\\n* If you are making a single LLM call, you don\\'t need LCEL; instead call the underlying [chat model](/docs/concepts/chat_models) directly.\\n* If you have a simple chain (e.g., prompt + llm + parser, simple retrieval set up etc.), LCEL is a reasonable fit, if you\\'re taking advantage of the LCEL benefits.\\n* If you\\'re building a complex chain (e.g., with branching, cycles, multiple agents, etc.) use [LangGraph](/docs/concepts/architecture#langgraph) instead. Remember that you can always use LCEL within individual nodes in LangGraph.\\n\\n## Composition Primitives\\n\\n`LCEL` chains are built by composing existing `Runnables` together. The two main composition primitives are [RunnableSequence](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableSequence.html#langchain_core.runnables.base.RunnableSequence) and [RunnableParallel](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableParallel.html#langchain_core.runnables.base.RunnableParallel).\\n\\nMany other composition primitives (e.g., [RunnableAssign](\\nhttps://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnableAssign.html#langchain_core.runnables.passthrough.RunnableAssign\\n)) can be thought of as variations of these two primitives.\\n\\n:::note\\nYou can find a list of all composition primitives in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html).\\n:::\\n\\n### RunnableSequence\\n\\n`RunnableSequence` is a composition primitive that allows you \"chain\" multiple runnables sequentially, with the output of one runnable serving as the input to the next.\\n\\n```python\\nfrom langchain_core.runnables import RunnableSequence\\nchain = RunnableSequence([runnable1, runnable2])\\n```\\n\\nInvoking the `chain` with some input:\\n\\n```python\\nfinal_output = chain.invoke(some_input)\\n```\\n\\ncorresponds to the following:\\n\\n```python\\noutput1 = runnable1.invoke(some_input)\\nfinal_output = runnable2.invoke(output1)\\n```\\n\\n:::note\\n`runnable1` and `runnable2` are placeholders for any `Runnable` that you want to chain together.\\n:::\\n\\n### RunnableParallel\\n\\n`RunnableParallel` is a composition primitive that allows you to run multiple runnables concurrently, with the same input provided to each.\\n\\n```python\\nfrom langchain_core.runnables import RunnableParallel\\nchain = RunnableParallel({\\n    \"key1\": runnable1,\\n    \"key2\": runnable2,\\n})\\n```\\n\\nInvoking the `chain` with some input:\\n\\n```python\\nfinal_output = chain.invoke(some_input)\\n```\\n\\nWill yield a `final_output` dictionary with the same keys as the input dictionary, but with the values replaced by the output of the corresponding runnable.\\n\\n```python\\n{\\n    \"key1\": runnable1.invoke(some_input),\\n    \"key2\": runnable2.invoke(some_input),\\n}\\n```\\n\\nRecall, that the runnables are executed in parallel, so while the result is the same as\\ndictionary comprehension shown above, the execution time is much faster.\\n\\n:::note\\n`RunnableParallel`supports both synchronous and asynchronous execution (as all `Runnables` do).\\n\\n* For synchronous execution, `RunnableParallel` uses a [ThreadPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor) to run the runnables concurrently.\\n* For asynchronous execution, `RunnableParallel` uses [asyncio.gather](https://docs.python.org/3/library/asyncio.html#asyncio.gather) to run the runnables concurrently.\\n:::\\n\\n## Composition Syntax\\n\\nThe usage of `RunnableSequence` and `RunnableParallel` is so common that we created a shorthand syntax for using them. This helps\\nto make the code more readable and concise.\\n\\n### The `|` operator\\n\\nWe have [overloaded](https://docs.python.org/3/reference/datamodel.html#special-method-names) the `|` operator to create a `RunnableSequence` from two `Runnables`.\\n\\n```python\\nchain = runnable1 | runnable2\\n```\\n\\nis Equivalent to:\\n\\n```python\\nchain = RunnableSequence([runnable1, runnable2])\\n```\\n\\n### The `.pipe` method\\n\\nIf you have moral qualms with operator overloading, you can use the `.pipe` method instead. This is equivalent to the `|` operator.\\n\\n```python\\nchain = runnable1.pipe(runnable2)\\n```\\n\\n### Coercion\\n\\nLCEL applies automatic type coercion to make it easier to compose chains.\\n\\nIf you do not understand the type coercion, you can always use the `RunnableSequence` and `RunnableParallel` classes directly.\\n\\nThis will make the code more verbose, but it will also make it more explicit.\\n\\n#### Dictionary to RunnableParallel\\n\\nInside an LCEL expression, a dictionary is automatically converted to a `RunnableParallel`.\\n\\nFor example, the following code:\\n\\n```python\\nmapping = {\\n    \"key1\": runnable1,\\n    \"key2\": runnable2,\\n}\\n\\nchain = mapping | runnable3\\n```\\n\\nIt gets automatically converted to the following:\\n\\n```python\\nchain = RunnableSequence([RunnableParallel(mapping), runnable3])\\n```\\n\\n:::caution\\nYou have to be careful because the `mapping` dictionary is not a `RunnableParallel` object, it is just a dictionary. This means that the following code will raise an `AttributeError`:\\n\\n```python\\nmapping.invoke(some_input)\\n```\\n:::\\n\\n#### Function to RunnableLambda\\n\\nInside an LCEL expression, a function is automatically converted to a `RunnableLambda`.\\n\\n```\\ndef some_func(x):\\n    return x\\n\\nchain = some_func | runnable1\\n```\\n\\nIt gets automatically converted to the following:\\n\\n```python\\nchain = RunnableSequence([RunnableLambda(some_func), runnable1])\\n```\\n\\n:::caution\\nYou have to be careful because the lambda function is not a `RunnableLambda` object, it is just a function. This means that the following code will raise an `AttributeError`:\\n\\n```python\\nlambda x: x + 1.invoke(some_input)\\n```\\n:::\\n\\n## Legacy chains\\n\\nLCEL aims to provide consistency around behavior and customization over legacy subclassed chains such as `LLMChain` and\\n`ConversationalRetrievalChain`. Many of these legacy chains hide important details like prompts, and as a wider variety\\nof viable models emerge, customization has become more and more important.\\n\\nIf you are currently using one of these legacy chains, please see [this guide for guidance on how to migrate](/docs/versions/migrating_chains).\\n\\nFor guides on how to do specific tasks with LCEL, check out [the relevant how-to guides](/docs/how_to/#langchain-expression-language-lcel).\\n': 0.016129032258064516, '# Conceptual guide\\n\\nThis guide provides explanations of the key concepts behind the LangChain framework and AI applications more broadly.\\n\\nWe recommend that you go through at least one of the [Tutorials](/docs/tutorials) before diving into the conceptual guide. This will provide practical context that will make it easier to understand the concepts discussed here.\\n\\nThe conceptual guide does not cover step-by-step instructions or specific implementation examples — those are found in the [How-to guides](/docs/how_to/) and [Tutorials](/docs/tutorials). For detailed reference material, please see the [API reference](https://python.langchain.com/api_reference/).\\n\\n## High level\\n\\n- **[Why LangChain?](/docs/concepts/why_langchain)**: Overview of the value that LangChain provides.\\n- **[Architecture](/docs/concepts/architecture)**: How packages are organized in the LangChain ecosystem.\\n\\n## Concepts\\n\\n- **[Chat models](/docs/concepts/chat_models)**: LLMs exposed via a chat API that process sequences of messages as input and output a message.\\n- **[Messages](/docs/concepts/messages)**: The unit of communication in chat models, used to represent model input and output.\\n- **[Chat history](/docs/concepts/chat_history)**: A conversation represented as a sequence of messages, alternating between user messages and model responses.\\n- **[Tools](/docs/concepts/tools)**: A function with an associated schema defining the function\\'s name, description, and the arguments it accepts.\\n- **[Tool calling](/docs/concepts/tool_calling)**: A type of chat model API that accepts tool schemas, along with messages, as input and returns invocations of those tools as part of the output message.\\n- **[Structured output](/docs/concepts/structured_outputs)**: A technique to make a chat model respond in a structured format, such as JSON that matches a given schema.\\n- **[Memory](https://langchain-ai.github.io/langgraph/concepts/memory/)**: Information about a conversation that is persisted so that it can be used in future conversations.\\n- **[Multimodality](/docs/concepts/multimodality)**: The ability to work with data that comes in different forms, such as text, audio, images, and video.\\n- **[Runnable interface](/docs/concepts/runnables)**: The base abstraction that many LangChain components and the LangChain Expression Language are built on.\\n- **[Streaming](/docs/concepts/streaming)**: LangChain streaming APIs for surfacing results as they are generated.\\n- **[LangChain Expression Language (LCEL)](/docs/concepts/lcel)**: A syntax for orchestrating LangChain components. Most useful for simpler applications.\\n- **[Document loaders](/docs/concepts/document_loaders)**: Load a source as a list of documents.\\n- **[Retrieval](/docs/concepts/retrieval)**: Information retrieval systems can retrieve structured or unstructured data from a datasource in response to a query.\\n- **[Text splitters](/docs/concepts/text_splitters)**: Split long text into smaller chunks that can be individually indexed to enable granular retrieval.\\n- **[Embedding models](/docs/concepts/embedding_models)**: Models that represent data such as text or images in a vector space.\\n- **[Vector stores](/docs/concepts/vectorstores)**: Storage of and efficient search over vectors and associated metadata.\\n- **[Retriever](/docs/concepts/retrievers)**: A component that returns relevant documents from a knowledge base in response to a query.\\n- **[Retrieval Augmented Generation (RAG)](/docs/concepts/rag)**: A technique that enhances language models by combining them with external knowledge bases.\\n- **[Agents](/docs/concepts/agents)**: Use a [language model](/docs/concepts/chat_models) to choose a sequence of actions to take. Agents can interact with external resources via [tool](/docs/concepts/tools).\\n- **[Prompt templates](/docs/concepts/prompt_templates)**: Component for factoring out the static parts of a model \"prompt\" (usually a sequence of messages). Useful for serializing, versioning, and reusing these static parts.\\n- **[Output parsers](/docs/concepts/output_parsers)**: Responsible for taking the output of a model and transforming it into a more suitable format for downstream tasks. Output parsers were primarily useful prior to the general availability of [tool calling](/docs/concepts/tool_calling) and [structured outputs](/docs/concepts/structured_outputs).\\n- **[Few-shot prompting](/docs/concepts/few_shot_prompting)**: A technique for improving model performance by providing a few examples of the task to perform in the prompt.\\n- **[Example selectors](/docs/concepts/example_selectors)**: Used to select the most relevant examples from a dataset based on a given input. Example selectors are used in few-shot prompting to select examples for a prompt.\\n- **[Async programming](/docs/concepts/async)**: The basics that one should know to use LangChain in an asynchronous context.\\n- **[Callbacks](/docs/concepts/callbacks)**: Callbacks enable the execution of custom auxiliary code in built-in components. Callbacks are used to stream outputs from LLMs in LangChain, trace the intermediate steps of an application, and more.\\n- **[Tracing](/docs/concepts/tracing)**: The process of recording the steps that an application takes to go from input to output. Tracing is essential for debugging and diagnosing issues in complex applications.\\n- **[Evaluation](/docs/concepts/evaluation)**: The process of assessing the performance and effectiveness of AI applications. This involves testing the model\\'s responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose. This process is vital for building reliable applications.\\n- **[Testing](/docs/concepts/testing)**: The process of verifying that a component of an integration or application works as expected. Testing is essential for ensuring that the application behaves correctly and that changes to the codebase do not introduce new bugs.\\n\\n## Glossary\\n\\n- **[AIMessageChunk](/docs/concepts/messages#aimessagechunk)**: A partial response from an AI message. Used when streaming responses from a chat model.\\n- **[AIMessage](/docs/concepts/messages#aimessage)**: Represents a complete response from an AI model.\\n- **[astream_events](/docs/concepts/chat_models#key-methods)**: Stream granular information from [LCEL](/docs/concepts/lcel) chains.\\n- **[BaseTool](/docs/concepts/tools/#tool-interface)**: The base class for all tools in LangChain.\\n- **[batch](/docs/concepts/runnables)**: Use to execute a runnable with batch inputs.\\n- **[bind_tools](/docs/concepts/tool_calling/#tool-binding)**: Allows models to interact with tools.\\n- **[Caching](/docs/concepts/chat_models#caching)**: Storing results to avoid redundant calls to a chat model.\\n- **[Chat models](/docs/concepts/multimodality/#multimodality-in-chat-models)**: Chat models that handle multiple data modalities.\\n- **[Configurable runnables](/docs/concepts/runnables/#configurable-runnables)**: Creating configurable Runnables.\\n- **[Context window](/docs/concepts/chat_models#context-window)**: The maximum size of input a chat model can process.\\n- **[Conversation patterns](/docs/concepts/chat_history#conversation-patterns)**: Common patterns in chat interactions.\\n- **[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)**: LangChain\\'s representation of a document.\\n- **[Embedding models](/docs/concepts/multimodality/#multimodality-in-embedding-models)**: Models that generate vector embeddings for various data types.\\n- **[HumanMessage](/docs/concepts/messages#humanmessage)**: Represents a message from a human user.\\n- **[InjectedState](/docs/concepts/tools#injectedstate)**: A state injected into a tool function.\\n- **[InjectedStore](/docs/concepts/tools#injectedstore)**: A store that can be injected into a tool for data persistence.\\n- **[InjectedToolArg](/docs/concepts/tools#injectedtoolarg)**: Mechanism to inject arguments into tool functions.\\n- **[input and output types](/docs/concepts/runnables#input-and-output-types)**: Types used for input and output in Runnables.\\n- **[Integration packages](/docs/concepts/architecture/#integration-packages)**: Third-party packages that integrate with LangChain.\\n- **[Integration tests](/docs/concepts/testing#integration-tests)**: Tests that verify the correctness of the interaction between components, usually run with access to the underlying API that powers an integration.\\n- **[invoke](/docs/concepts/runnables)**: A standard method to invoke a Runnable.\\n- **[JSON mode](/docs/concepts/structured_outputs#json-mode)**: Returning responses in JSON format.\\n- **[langchain-community](/docs/concepts/architecture#langchain-community)**: Community-driven components for LangChain.\\n- **[langchain-core](/docs/concepts/architecture#langchain-core)**: Core langchain package. Includes base interfaces and in-memory implementations.\\n- **[langchain](/docs/concepts/architecture#langchain)**: A package for higher level components (e.g., some pre-built chains).\\n- **[langgraph](/docs/concepts/architecture#langgraph)**: Powerful orchestration layer for LangChain. Use to build complex pipelines and workflows.\\n- **[langserve](/docs/concepts/architecture#langserve)**: Used to deploy LangChain Runnables as REST endpoints. Uses FastAPI. Works primarily for LangChain Runnables, does not currently integrate with LangGraph.\\n- **[LLMs (legacy)](/docs/concepts/text_llms)**: Older language models that take a string as input and return a string as output.\\n- **[Managing chat history](/docs/concepts/chat_history#managing-chat-history)**: Techniques to maintain and manage the chat history.\\n- **[OpenAI format](/docs/concepts/messages#openai-format)**: OpenAI\\'s message format for chat models.\\n- **[Propagation of RunnableConfig](/docs/concepts/runnables/#propagation-of-runnableconfig)**: Propagating configuration through Runnables. Read if working with python 3.9, 3.10 and async.\\n- **[rate-limiting](/docs/concepts/chat_models#rate-limiting)**: Client side rate limiting for chat models.\\n- **[RemoveMessage](/docs/concepts/messages/#removemessage)**: An abstraction used to remove a message from chat history, used primarily in LangGraph.\\n- **[role](/docs/concepts/messages#role)**: Represents the role (e.g., user, assistant) of a chat message.\\n- **[RunnableConfig](/docs/concepts/runnables/#runnableconfig)**: Use to pass run time information to Runnables (e.g., `run_name`, `run_id`, `tags`, `metadata`, `max_concurrency`, `recursion_limit`, `configurable`).\\n- **[Standard parameters for chat models](/docs/concepts/chat_models#standard-parameters)**: Parameters such as API key, `temperature`, and `max_tokens`.\\n- **[Standard tests](/docs/concepts/testing#standard-tests)**: A defined set of unit and integration tests that all integrations must pass.\\n- **[stream](/docs/concepts/streaming)**: Use to stream output from a Runnable or a graph.\\n- **[Tokenization](/docs/concepts/tokens)**: The process of converting data into tokens and vice versa.\\n- **[Tokens](/docs/concepts/tokens)**: The basic unit that a language model reads, processes, and generates under the hood.\\n- **[Tool artifacts](/docs/concepts/tools#tool-artifacts)**: Add artifacts to the output of a tool that will not be sent to the model, but will be available for downstream processing.\\n- **[Tool binding](/docs/concepts/tool_calling#tool-binding)**: Binding tools to models.\\n- **[@tool](/docs/concepts/tools/#create-tools-using-the-tool-decorator)**: Decorator for creating tools in LangChain.\\n- **[Toolkits](/docs/concepts/tools#toolkits)**: A collection of tools that can be used together.\\n- **[ToolMessage](/docs/concepts/messages#toolmessage)**: Represents a message that contains the results of a tool execution.\\n- **[Unit tests](/docs/concepts/testing#unit-tests)**: Tests that verify the correctness of individual components, run in isolation without access to the Internet.\\n- **[Vector stores](/docs/concepts/vectorstores)**: Datastores specialized for storing and efficiently searching vector embeddings.\\n- **[with_structured_output](/docs/concepts/structured_outputs/#structured-output-method)**: A helper method for chat models that natively support [tool calling](/docs/concepts/tool_calling) to get structured output matching a given schema specified via Pydantic, JSON schema or a function.\\n- **[with_types](/docs/concepts/runnables#with_types)**: Method to overwrite the input and output types of a runnable. Useful when working with complex LCEL chains and deploying with LangServe.\\n': 0.015873015873015872}\n",
      "docs:[Document(metadata={'file_name': 'why_langchain.mdx', 'file_path': 'docs\\\\docs\\\\concepts\\\\why_langchain.mdx', 'file_type': '.mdx', 'source': 'docs\\\\docs\\\\concepts\\\\why_langchain.mdx'}, page_content='# Why LangChain?\\n\\nThe goal of `langchain` the Python package and LangChain the company is to make it as easy as possible for developers to build applications that reason.\\nWhile LangChain originally started as a single open source package, it has evolved into a company and a whole ecosystem.\\nThis page will talk about the LangChain ecosystem as a whole.\\nMost of the components within the LangChain ecosystem can be used by themselves - so if you feel particularly drawn to certain components but not others, that is totally fine! Pick and choose whichever components you like best for your own use case!\\n\\n## Features\\n\\nThere are several primary needs that LangChain aims to address:\\n\\n1. **Standardized component interfaces:** The growing number of [models](/docs/integrations/chat/) and [related components](/docs/integrations/vectorstores/) for AI applications has resulted in a wide variety of different APIs that developers need to learn and use.\\nThis diversity can make it challenging for developers to switch between providers or combine components when building applications.\\nLangChain exposes a standard interface for key components, making it easy to switch between providers.\\n\\n2. **Orchestration:** As applications become more complex, combining multiple components and models, there\\'s [a growing need to efficiently connect these elements into control flows](https://lilianweng.github.io/posts/2023-06-23-agent/) that can [accomplish diverse tasks](https://www.sequoiacap.com/article/generative-ais-act-o1/).\\n[Orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)) is crucial for building such applications.\\n\\n3. **Observability and evaluation:** As applications become more complex, it becomes increasingly difficult to understand what is happening within them.\\nFurthermore, the pace of development can become rate-limited by the [paradox of choice](https://en.wikipedia.org/wiki/Paradox_of_choice).\\nFor example, developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\n[Observability](https://en.wikipedia.org/wiki/Observability) and evaluations can help developers monitor their applications and rapidly answer these types of questions with confidence.\\n\\n\\n## Standardized component interfaces\\n\\nLangChain provides common interfaces for components that are central to many AI applications.\\nAs an example, all [chat models](/docs/concepts/chat_models/) implement the [BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) interface.\\nThis provides a standard way to interact with chat models, supporting important but often provider-specific features like [tool calling](/docs/concepts/tool_calling/) and [structured outputs](/docs/concepts/structured_outputs/).\\n\\n\\n### Example: chat models \\n\\nMany [model providers](/docs/concepts/chat_models/) support [tool calling](/docs/concepts/tool_calling/), a critical feature for many applications (e.g., [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/)), that allows a developer to request model responses that match a particular schema.\\nThe APIs for each provider differ. \\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to bind [tools](/docs/concepts/tools) to a model in order to support [tool calling](/docs/concepts/tool_calling/):\\n\\n```python\\n# Tool creation\\ntools = [my_tool]\\n# Tool binding\\nmodel_with_tools = model.bind_tools(tools)\\n```\\n\\nSimilarly, getting models to produce [structured outputs](/docs/concepts/structured_outputs/) is an extremely common use case. \\nProviders support different approaches for this, including [JSON mode or tool calling](https://platform.openai.com/docs/guides/structured-outputs), with different APIs.\\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to produce structured outputs using the `with_structured_output()` method:\\n\\n```python\\n# Define schema\\nschema = ...\\n# Bind schema to model\\nmodel_with_structure = model.with_structured_output(schema)\\n```\\n\\n### Example: retrievers\\n\\nIn the context of [RAG](/docs/concepts/rag/) and LLM application components, LangChain\\'s [retriever](/docs/concepts/retrievers/) interface provides a standard way to connect to many different types of data services or databases (e.g., [vector stores](/docs/concepts/vectorstores) or databases).\\nThe underlying implementation of the retriever depends on the type of data store or database you are connecting to, but all retrievers implement the [runnable interface](/docs/concepts/runnables/), meaning they can be invoked in a common manner.\\n\\n```python\\ndocuments = my_retriever.invoke(\"What is the meaning of life?\")\\n```\\n\\n## Orchestration \\n\\nWhile standardization for individual components is useful, we\\'ve increasingly seen that developers want to *combine* components into more complex applications. \\nThis motivates the need for [orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)).\\nThere are several common characteristics of LLM applications that this orchestration layer should support:\\n\\n* **Complex control flow:** The application requires complex patterns such as cycles (e.g., a loop that reiterates until a condition is met).\\n* **[Persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/):** The application needs to maintain [short-term and / or long-term memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n* **[Human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/):** The application needs human interaction, e.g., pausing, reviewing, editing, approving certain steps.\\n\\nThe recommended way to orchestrate components for complex applications is [LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/).\\nLangGraph is a library that gives developers a high degree of control by expressing the flow of the application as a set of nodes and edges.\\nLangGraph comes with built-in support for [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), [memory](https://langchain-ai.github.io/langgraph/concepts/memory/), and other features.\\nIt\\'s particularly well suited for building [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/) or [multi-agent](https://langchain-ai.github.io/langgraph/concepts/multi_agent/) applications. \\nImportantly, individual LangChain components can be used as LangGraph nodes, but you can also use LangGraph **without** using LangChain components.\\n\\n:::info[Further reading]\\n\\nHave a look at our free course, [Introduction to LangGraph](https://academy.langchain.com/courses/intro-to-langgraph), to learn more about how to use LangGraph to build complex applications.\\n\\n:::\\n\\n## Observability and evaluation\\n\\nThe pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. \\nDevelopers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\nHigh quality tracing and evaluations can help you rapidly answer these types of questions with confidence.\\n[LangSmith](https://docs.smith.langchain.com/) is our platform that supports observability and evaluation for AI applications.\\nSee our conceptual guides on [evaluations](https://docs.smith.langchain.com/concepts/evaluation) and [tracing](https://docs.smith.langchain.com/concepts/tracing) for more details.\\n\\n:::info[Further reading]\\n\\nSee our video playlist on [LangSmith tracing and evaluations](https://youtube.com/playlist?list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S&feature=shared) for more details.\\n\\n:::\\n\\n## Conclusion\\n\\nLangChain offers standard interfaces for components that are central to many AI applications, which offers a few specific advantages:\\n- **Ease of swapping providers:** It allows you to swap out different component providers without having to change the underlying code.\\n- **Advanced features:** It provides common methods for more advanced features, such as [streaming](/docs/concepts/streaming) and [tool calling](/docs/concepts/tool_calling/).\\n\\n[LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/) makes it possible to orchestrate complex applications (e.g., [agents](/docs/concepts/agents/)) and provide features like including [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), or [memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n\\n[LangSmith](https://docs.smith.langchain.com/) makes it possible to iterate with confidence on your applications, by providing LLM-specific observability and framework for testing and evaluating your application.\\n'), Document(metadata={'file_name': 'lcel.mdx', 'file_path': 'docs\\\\docs\\\\concepts\\\\lcel.mdx', 'file_type': '.mdx', 'source': 'docs\\\\docs\\\\concepts\\\\lcel.mdx'}, page_content='# LangChain Expression Language (LCEL)\\n\\n:::info Prerequisites\\n* [Runnable Interface](/docs/concepts/runnables)\\n:::\\n\\nThe **L**ang**C**hain **E**xpression **L**anguage (LCEL) takes a [declarative](https://en.wikipedia.org/wiki/Declarative_programming) approach to building new [Runnables](/docs/concepts/runnables) from existing Runnables.\\n\\nThis means that you describe what *should* happen, rather than *how* it should happen, allowing LangChain to optimize the run-time execution of the chains.\\n\\nWe often refer to a `Runnable` created using LCEL as a \"chain\". It\\'s important to remember that a \"chain\" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).\\n\\n:::note\\n* The [LCEL cheatsheet](/docs/how_to/lcel_cheatsheet/) shows common patterns that involve the Runnable interface and LCEL expressions.\\n* Please see the following list of [how-to guides](/docs/how_to/#langchain-expression-language-lcel) that cover common tasks with LCEL.\\n* A list of built-in `Runnables` can be found in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html). Many of these Runnables are useful when composing custom \"chains\" in LangChain using LCEL.\\n:::\\n\\n## Benefits of LCEL\\n\\nLangChain optimizes the run-time execution of chains built with LCEL in a number of ways:\\n\\n- **Optimized parallel execution**: Run Runnables in parallel using [RunnableParallel](#runnableparallel) or run multiple inputs through a given chain in parallel using the [Runnable Batch API](/docs/concepts/runnables/#optimized-parallel-execution-batch). Parallel execution can significantly reduce the latency as processing can be done in parallel instead of sequentially.\\n- **Guaranteed Async support**: Any chain built with LCEL can be run asynchronously using the [Runnable Async API](/docs/concepts/runnables/#asynchronous-support). This can be useful when running chains in a server environment where you want to handle large number of requests concurrently.\\n- **Simplify streaming**: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(time elapsed until the first chunk of output from a [chat model](/docs/concepts/chat_models) or [llm](/docs/concepts/text_llms) comes out).\\n\\nOther benefits include:\\n\\n- [**Seamless LangSmith tracing**](https://docs.smith.langchain.com)\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, **all** steps are automatically logged to [LangSmith](https://docs.smith.langchain.com/) for maximum observability and debuggability.\\n- **Standard API**: Because all chains are built using the Runnable interface, they can be used in the same way as any other Runnable.\\n- [**Deployable with LangServe**](/docs/concepts/architecture#langserve): Chains built with LCEL can be deployed using for production use.\\n\\n## Should I use LCEL?\\n\\nLCEL is an [orchestration solution](https://en.wikipedia.org/wiki/Orchestration_(computing)) -- it allows LangChain to handle run-time execution of chains in an optimized way.\\n\\nWhile we have seen users run chains with hundreds of steps in production, we generally recommend using LCEL for simpler orchestration tasks. When the application requires complex state management, branching, cycles or multiple agents, we recommend that users take advantage of [LangGraph](/docs/concepts/architecture#langgraph).\\n\\nIn LangGraph, users define graphs that specify the application\\'s flow. This allows users to keep using LCEL within individual nodes when LCEL is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.\\n\\nHere are some guidelines:\\n\\n* If you are making a single LLM call, you don\\'t need LCEL; instead call the underlying [chat model](/docs/concepts/chat_models) directly.\\n* If you have a simple chain (e.g., prompt + llm + parser, simple retrieval set up etc.), LCEL is a reasonable fit, if you\\'re taking advantage of the LCEL benefits.\\n* If you\\'re building a complex chain (e.g., with branching, cycles, multiple agents, etc.) use [LangGraph](/docs/concepts/architecture#langgraph) instead. Remember that you can always use LCEL within individual nodes in LangGraph.\\n\\n## Composition Primitives\\n\\n`LCEL` chains are built by composing existing `Runnables` together. The two main composition primitives are [RunnableSequence](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableSequence.html#langchain_core.runnables.base.RunnableSequence) and [RunnableParallel](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableParallel.html#langchain_core.runnables.base.RunnableParallel).\\n\\nMany other composition primitives (e.g., [RunnableAssign](\\nhttps://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnableAssign.html#langchain_core.runnables.passthrough.RunnableAssign\\n)) can be thought of as variations of these two primitives.\\n\\n:::note\\nYou can find a list of all composition primitives in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html).\\n:::\\n\\n### RunnableSequence\\n\\n`RunnableSequence` is a composition primitive that allows you \"chain\" multiple runnables sequentially, with the output of one runnable serving as the input to the next.\\n\\n```python\\nfrom langchain_core.runnables import RunnableSequence\\nchain = RunnableSequence([runnable1, runnable2])\\n```\\n\\nInvoking the `chain` with some input:\\n\\n```python\\nfinal_output = chain.invoke(some_input)\\n```\\n\\ncorresponds to the following:\\n\\n```python\\noutput1 = runnable1.invoke(some_input)\\nfinal_output = runnable2.invoke(output1)\\n```\\n\\n:::note\\n`runnable1` and `runnable2` are placeholders for any `Runnable` that you want to chain together.\\n:::\\n\\n### RunnableParallel\\n\\n`RunnableParallel` is a composition primitive that allows you to run multiple runnables concurrently, with the same input provided to each.\\n\\n```python\\nfrom langchain_core.runnables import RunnableParallel\\nchain = RunnableParallel({\\n    \"key1\": runnable1,\\n    \"key2\": runnable2,\\n})\\n```\\n\\nInvoking the `chain` with some input:\\n\\n```python\\nfinal_output = chain.invoke(some_input)\\n```\\n\\nWill yield a `final_output` dictionary with the same keys as the input dictionary, but with the values replaced by the output of the corresponding runnable.\\n\\n```python\\n{\\n    \"key1\": runnable1.invoke(some_input),\\n    \"key2\": runnable2.invoke(some_input),\\n}\\n```\\n\\nRecall, that the runnables are executed in parallel, so while the result is the same as\\ndictionary comprehension shown above, the execution time is much faster.\\n\\n:::note\\n`RunnableParallel`supports both synchronous and asynchronous execution (as all `Runnables` do).\\n\\n* For synchronous execution, `RunnableParallel` uses a [ThreadPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor) to run the runnables concurrently.\\n* For asynchronous execution, `RunnableParallel` uses [asyncio.gather](https://docs.python.org/3/library/asyncio.html#asyncio.gather) to run the runnables concurrently.\\n:::\\n\\n## Composition Syntax\\n\\nThe usage of `RunnableSequence` and `RunnableParallel` is so common that we created a shorthand syntax for using them. This helps\\nto make the code more readable and concise.\\n\\n### The `|` operator\\n\\nWe have [overloaded](https://docs.python.org/3/reference/datamodel.html#special-method-names) the `|` operator to create a `RunnableSequence` from two `Runnables`.\\n\\n```python\\nchain = runnable1 | runnable2\\n```\\n\\nis Equivalent to:\\n\\n```python\\nchain = RunnableSequence([runnable1, runnable2])\\n```\\n\\n### The `.pipe` method\\n\\nIf you have moral qualms with operator overloading, you can use the `.pipe` method instead. This is equivalent to the `|` operator.\\n\\n```python\\nchain = runnable1.pipe(runnable2)\\n```\\n\\n### Coercion\\n\\nLCEL applies automatic type coercion to make it easier to compose chains.\\n\\nIf you do not understand the type coercion, you can always use the `RunnableSequence` and `RunnableParallel` classes directly.\\n\\nThis will make the code more verbose, but it will also make it more explicit.\\n\\n#### Dictionary to RunnableParallel\\n\\nInside an LCEL expression, a dictionary is automatically converted to a `RunnableParallel`.\\n\\nFor example, the following code:\\n\\n```python\\nmapping = {\\n    \"key1\": runnable1,\\n    \"key2\": runnable2,\\n}\\n\\nchain = mapping | runnable3\\n```\\n\\nIt gets automatically converted to the following:\\n\\n```python\\nchain = RunnableSequence([RunnableParallel(mapping), runnable3])\\n```\\n\\n:::caution\\nYou have to be careful because the `mapping` dictionary is not a `RunnableParallel` object, it is just a dictionary. This means that the following code will raise an `AttributeError`:\\n\\n```python\\nmapping.invoke(some_input)\\n```\\n:::\\n\\n#### Function to RunnableLambda\\n\\nInside an LCEL expression, a function is automatically converted to a `RunnableLambda`.\\n\\n```\\ndef some_func(x):\\n    return x\\n\\nchain = some_func | runnable1\\n```\\n\\nIt gets automatically converted to the following:\\n\\n```python\\nchain = RunnableSequence([RunnableLambda(some_func), runnable1])\\n```\\n\\n:::caution\\nYou have to be careful because the lambda function is not a `RunnableLambda` object, it is just a function. This means that the following code will raise an `AttributeError`:\\n\\n```python\\nlambda x: x + 1.invoke(some_input)\\n```\\n:::\\n\\n## Legacy chains\\n\\nLCEL aims to provide consistency around behavior and customization over legacy subclassed chains such as `LLMChain` and\\n`ConversationalRetrievalChain`. Many of these legacy chains hide important details like prompts, and as a wider variety\\nof viable models emerge, customization has become more and more important.\\n\\nIf you are currently using one of these legacy chains, please see [this guide for guidance on how to migrate](/docs/versions/migrating_chains).\\n\\nFor guides on how to do specific tasks with LCEL, check out [the relevant how-to guides](/docs/how_to/#langchain-expression-language-lcel).\\n'), Document(metadata={'file_name': 'introduction.mdx', 'file_path': 'docs\\\\docs\\\\introduction.mdx', 'file_type': '.mdx', 'source': 'docs\\\\docs\\\\introduction.mdx'}, page_content='---\\nsidebar_position: 0\\nsidebar_class_name: hidden\\n---\\n\\n# Introduction\\n\\n**LangChain** is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:\\n- **Development**: Build your applications using LangChain\\'s open-source [components](/docs/concepts) and [third-party integrations](/docs/integrations/providers/).\\nUse [LangGraph](/docs/concepts/architecture/#langgraph) to build stateful agents with first-class streaming and human-in-the-loop support.\\n- **Productionization**: Use [LangSmith](https://docs.smith.langchain.com/) to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\n- **Deployment**: Turn your LangGraph applications into production-ready APIs and Assistants with [LangGraph Platform](https://langchain-ai.github.io/langgraph/cloud/).\\n\\nimport ThemedImage from \\'@theme/ThemedImage\\';\\nimport useBaseUrl from \\'@docusaurus/useBaseUrl\\';\\n\\n<ThemedImage\\n  alt=\"Diagram outlining the hierarchical organization of the LangChain framework, displaying the interconnected parts across multiple layers.\"\\n  sources={{\\n    light: useBaseUrl(\\'/svg/langchain_stack_112024.svg\\'),\\n    dark: useBaseUrl(\\'/svg/langchain_stack_112024_dark.svg\\'),\\n  }}\\n  style={{ width: \"100%\" }}\\n  title=\"LangChain Framework Overview\"\\n/>\\n\\nLangChain implements a standard interface for large language models and related\\ntechnologies, such as embedding models and vector stores, and integrates with\\nhundreds of providers. See the [integrations](/docs/integrations/providers/) page for\\nmore.\\n\\nimport ChatModelTabs from \"@theme/ChatModelTabs\";\\n\\n<ChatModelTabs/>\\n\\n```python\\nmodel.invoke(\"Hello, world!\")\\n```\\n\\n:::note\\n\\nThese docs focus on the Python LangChain library. [Head here](https://js.langchain.com) for docs on the JavaScript LangChain library.\\n\\n:::\\n\\n## Architecture\\n\\nThe LangChain framework consists of multiple open-source libraries. Read more in the\\n[Architecture](/docs/concepts/architecture/) page.\\n\\n- **`langchain-core`**: Base abstractions for chat models and other components.\\n- **Integration packages** (e.g. `langchain-openai`, `langchain-anthropic`, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\\n- **`langchain`**: Chains, agents, and retrieval strategies that make up an application\\'s cognitive architecture.\\n- **`langchain-community`**: Third-party integrations that are community maintained.\\n- **`langgraph`**: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See [LangGraph documentation](https://langchain-ai.github.io/langgraph/).\\n\\n## Guides\\n\\n### [Tutorials](/docs/tutorials)\\n\\nIf you\\'re looking to build something specific or are more of a hands-on learner, check out our [tutorials section](/docs/tutorials).\\nThis is the best place to get started.\\n\\nThese are the best ones to get started with:\\n\\n- [Build a Simple LLM Application](/docs/tutorials/llm_chain)\\n- [Build a Chatbot](/docs/tutorials/chatbot)\\n- [Build an Agent](/docs/tutorials/agents)\\n- [Introduction to LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/)\\n\\nExplore the full list of LangChain tutorials [here](/docs/tutorials), and check out other [LangGraph tutorials here](https://langchain-ai.github.io/langgraph/tutorials/). To learn more about LangGraph, check out our first LangChain Academy course, *Introduction to LangGraph*, available [here](https://academy.langchain.com/courses/intro-to-langgraph).\\n\\n\\n### [How-to guides](/docs/how_to)\\n\\n[Here](/docs/how_to) you’ll find short answers to “How do I….?” types of questions.\\nThese how-to guides don’t cover topics in depth – you’ll find that material in the [Tutorials](/docs/tutorials) and the [API Reference](https://python.langchain.com/api_reference/).\\nHowever, these guides will help you quickly accomplish common tasks using [chat models](/docs/how_to/#chat-models),\\n[vector stores](/docs/how_to/#vector-stores), and other common LangChain components.\\n\\nCheck out [LangGraph-specific how-tos here](https://langchain-ai.github.io/langgraph/how-tos/).\\n\\n### [Conceptual guide](/docs/concepts)\\n\\nIntroductions to all the key parts of LangChain you’ll need to know! [Here](/docs/concepts) you\\'ll find high level explanations of all LangChain concepts.\\n\\nFor a deeper dive into LangGraph concepts, check out [this page](https://langchain-ai.github.io/langgraph/concepts/).\\n\\n### [Integrations](integrations/providers/index.mdx)\\n\\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\\nIf you\\'re looking to get up and running quickly with [chat models](/docs/integrations/chat/), [vector stores](/docs/integrations/vectorstores/),\\nor other LangChain components from a specific provider, check out our growing list of [integrations](/docs/integrations/providers/).\\n\\n\\n### [API reference](https://python.langchain.com/api_reference/)\\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\\n\\n## Ecosystem\\n\\n### [🦜🛠️ LangSmith](https://docs.smith.langchain.com)\\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\\n\\n### [🦜🕸️ LangGraph](https://langchain-ai.github.io/langgraph)\\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by Linkedin, Uber, Klarna, GitLab, and many more.\\n\\n## Additional resources\\n\\n### [Versions](/docs/versions/v0_3/)\\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\\n\\n### [Security](/docs/security)\\nRead up on [security](/docs/security) best practices to make sure you\\'re developing safely with LangChain.\\n\\n### [Contributing](contributing/index.mdx)\\nCheck out the developer\\'s guide for guidelines on contributing and help getting your dev environment set up.\\n'), Document(metadata={'file_name': 'index.mdx', 'file_path': 'docs\\\\docs\\\\how_to\\\\index.mdx', 'file_type': '.mdx', 'source': 'docs\\\\docs\\\\how_to\\\\index.mdx'}, page_content='---\\nsidebar_position: 0\\nsidebar_class_name: hidden\\n---\\n\\n# How-to guides\\n\\nHere you’ll find answers to “How do I….?” types of questions.\\nThese guides are *goal-oriented* and *concrete*; they\\'re meant to help you complete a specific task.\\nFor conceptual explanations see the [Conceptual guide](/docs/concepts/).\\nFor end-to-end walkthroughs see [Tutorials](/docs/tutorials).\\nFor comprehensive descriptions of every class and function see the [API Reference](https://python.langchain.com/api_reference/).\\n\\n## Installation\\n\\n- [How to: install LangChain packages](/docs/how_to/installation/)\\n- [How to: use LangChain with different Pydantic versions](/docs/how_to/pydantic_compatibility)\\n\\n## Key features\\n\\nThis highlights functionality that is core to using LangChain.\\n\\n- [How to: return structured data from a model](/docs/how_to/structured_output/)\\n- [How to: use a model to call tools](/docs/how_to/tool_calling)\\n- [How to: stream runnables](/docs/how_to/streaming)\\n- [How to: debug your LLM apps](/docs/how_to/debugging/)\\n\\n## Components\\n\\nThese are the core building blocks you can use when building applications.\\n\\n### Chat models\\n\\n[Chat Models](/docs/concepts/chat_models) are newer forms of language models that take messages in and output a message.\\nSee [supported integrations](/docs/integrations/chat/) for details on getting started with chat models from a specific provider.\\n\\n- [How to: do function/tool calling](/docs/how_to/tool_calling)\\n- [How to: get models to return structured output](/docs/how_to/structured_output)\\n- [How to: cache model responses](/docs/how_to/chat_model_caching)\\n- [How to: get log probabilities](/docs/how_to/logprobs)\\n- [How to: create a custom chat model class](/docs/how_to/custom_chat_model)\\n- [How to: stream a response back](/docs/how_to/chat_streaming)\\n- [How to: track token usage](/docs/how_to/chat_token_usage_tracking)\\n- [How to: track response metadata across providers](/docs/how_to/response_metadata)\\n- [How to: use chat model to call tools](/docs/how_to/tool_calling)\\n- [How to: stream tool calls](/docs/how_to/tool_streaming)\\n- [How to: handle rate limits](/docs/how_to/chat_model_rate_limiting)\\n- [How to: few shot prompt tool behavior](/docs/how_to/tools_few_shot)\\n- [How to: bind model-specific formatted tools](/docs/how_to/tools_model_specific)\\n- [How to: force a specific tool call](/docs/how_to/tool_choice)\\n- [How to: work with local models](/docs/how_to/local_llms)\\n- [How to: init any model in one line](/docs/how_to/chat_models_universal_init/)\\n- [How to: pass multimodal data directly to models](/docs/how_to/multimodal_inputs/)\\n\\n### Messages\\n\\n[Messages](/docs/concepts/messages) are the input and output of chat models. They have some `content` and a `role`, which describes the source of the message.\\n\\n- [How to: trim messages](/docs/how_to/trim_messages/)\\n- [How to: filter messages](/docs/how_to/filter_messages/)\\n- [How to: merge consecutive messages of the same type](/docs/how_to/merge_message_runs/)\\n\\n### Prompt templates\\n\\n[Prompt Templates](/docs/concepts/prompt_templates) are responsible for formatting user input into a format that can be passed to a language model.\\n\\n- [How to: use few shot examples](/docs/how_to/few_shot_examples)\\n- [How to: use few shot examples in chat models](/docs/how_to/few_shot_examples_chat/)\\n- [How to: partially format prompt templates](/docs/how_to/prompts_partial)\\n- [How to: compose prompts together](/docs/how_to/prompts_composition)\\n- [How to: use multimodal prompts](/docs/how_to/multimodal_prompts/)\\n\\n### Example selectors\\n\\n[Example Selectors](/docs/concepts/example_selectors) are responsible for selecting the correct few shot examples to pass to the prompt.\\n\\n- [How to: use example selectors](/docs/how_to/example_selectors)\\n- [How to: select examples by length](/docs/how_to/example_selectors_length_based)\\n- [How to: select examples by semantic similarity](/docs/how_to/example_selectors_similarity)\\n- [How to: select examples by semantic ngram overlap](/docs/how_to/example_selectors_ngram)\\n- [How to: select examples by maximal marginal relevance](/docs/how_to/example_selectors_mmr)\\n- [How to: select examples from LangSmith few-shot datasets](/docs/how_to/example_selectors_langsmith/)\\n\\n### LLMs\\n\\nWhat LangChain calls [LLMs](/docs/concepts/text_llms) are older forms of language models that take a string in and output a string.\\n\\n- [How to: cache model responses](/docs/how_to/llm_caching)\\n- [How to: create a custom LLM class](/docs/how_to/custom_llm)\\n- [How to: stream a response back](/docs/how_to/streaming_llm)\\n- [How to: track token usage](/docs/how_to/llm_token_usage_tracking)\\n- [How to: work with local models](/docs/how_to/local_llms)\\n\\n### Output parsers\\n\\n[Output Parsers](/docs/concepts/output_parsers) are responsible for taking the output of an LLM and parsing into more structured format.\\n\\n- [How to: parse text from message objects](/docs/how_to/output_parser_string)\\n- [How to: use output parsers to parse an LLM response into structured format](/docs/how_to/output_parser_structured)\\n- [How to: parse JSON output](/docs/how_to/output_parser_json)\\n- [How to: parse XML output](/docs/how_to/output_parser_xml)\\n- [How to: parse YAML output](/docs/how_to/output_parser_yaml)\\n- [How to: retry when output parsing errors occur](/docs/how_to/output_parser_retry)\\n- [How to: try to fix errors in output parsing](/docs/how_to/output_parser_fixing)\\n- [How to: write a custom output parser class](/docs/how_to/output_parser_custom)\\n\\n### Document loaders\\n\\n[Document Loaders](/docs/concepts/document_loaders) are responsible for loading documents from a variety of sources.\\n\\n- [How to: load PDF files](/docs/how_to/document_loader_pdf)\\n- [How to: load web pages](/docs/how_to/document_loader_web)\\n- [How to: load CSV data](/docs/how_to/document_loader_csv)\\n- [How to: load data from a directory](/docs/how_to/document_loader_directory)\\n- [How to: load HTML data](/docs/how_to/document_loader_html)\\n- [How to: load JSON data](/docs/how_to/document_loader_json)\\n- [How to: load Markdown data](/docs/how_to/document_loader_markdown)\\n- [How to: load Microsoft Office data](/docs/how_to/document_loader_office_file)\\n- [How to: write a custom document loader](/docs/how_to/document_loader_custom)\\n\\n### Text splitters\\n\\n[Text Splitters](/docs/concepts/text_splitters) take a document and split into chunks that can be used for retrieval.\\n\\n- [How to: recursively split text](/docs/how_to/recursive_text_splitter)\\n- [How to: split HTML](/docs/how_to/split_html)\\n- [How to: split by character](/docs/how_to/character_text_splitter)\\n- [How to: split code](/docs/how_to/code_splitter)\\n- [How to: split Markdown by headers](/docs/how_to/markdown_header_metadata_splitter)\\n- [How to: recursively split JSON](/docs/how_to/recursive_json_splitter)\\n- [How to: split text into semantic chunks](/docs/how_to/semantic-chunker)\\n- [How to: split by tokens](/docs/how_to/split_by_token)\\n\\n### Embedding models\\n\\n[Embedding Models](/docs/concepts/embedding_models) take a piece of text and create a numerical representation of it.\\nSee [supported integrations](/docs/integrations/text_embedding/) for details on getting started with embedding models from a specific provider.\\n\\n- [How to: embed text data](/docs/how_to/embed_text)\\n- [How to: cache embedding results](/docs/how_to/caching_embeddings)\\n- [How to: create a custom embeddings class](/docs/how_to/custom_embeddings)\\n\\n### Vector stores\\n\\n[Vector stores](/docs/concepts/vectorstores) are databases that can efficiently store and retrieve embeddings.\\nSee [supported integrations](/docs/integrations/vectorstores/) for details on getting started with vector stores from a specific provider.\\n\\n- [How to: use a vector store to retrieve data](/docs/how_to/vectorstores)\\n\\n### Retrievers\\n\\n[Retrievers](/docs/concepts/retrievers) are responsible for taking a query and returning relevant documents.\\n\\n- [How to: use a vector store to retrieve data](/docs/how_to/vectorstore_retriever)\\n- [How to: generate multiple queries to retrieve data for](/docs/how_to/MultiQueryRetriever)\\n- [How to: use contextual compression to compress the data retrieved](/docs/how_to/contextual_compression)\\n- [How to: write a custom retriever class](/docs/how_to/custom_retriever)\\n- [How to: add similarity scores to retriever results](/docs/how_to/add_scores_retriever)\\n- [How to: combine the results from multiple retrievers](/docs/how_to/ensemble_retriever)\\n- [How to: reorder retrieved results to mitigate the \"lost in the middle\" effect](/docs/how_to/long_context_reorder)\\n- [How to: generate multiple embeddings per document](/docs/how_to/multi_vector)\\n- [How to: retrieve the whole document for a chunk](/docs/how_to/parent_document_retriever)\\n- [How to: generate metadata filters](/docs/how_to/self_query)\\n- [How to: create a time-weighted retriever](/docs/how_to/time_weighted_vectorstore)\\n- [How to: use hybrid vector and keyword retrieval](/docs/how_to/hybrid)\\n\\n### Indexing\\n\\nIndexing is the process of keeping your vectorstore in-sync with the underlying data source.\\n\\n- [How to: reindex data to keep your vectorstore in-sync with the underlying data source](/docs/how_to/indexing)\\n\\n### Tools\\n\\nLangChain [Tools](/docs/concepts/tools) contain a description of the tool (to pass to the language model) as well as the implementation of the function to call. Refer [here](/docs/integrations/tools/) for a list of pre-built tools. \\n\\n- [How to: create tools](/docs/how_to/custom_tools)\\n- [How to: use built-in tools and toolkits](/docs/how_to/tools_builtin)\\n- [How to: use chat models to call tools](/docs/how_to/tool_calling)\\n- [How to: pass tool outputs to chat models](/docs/how_to/tool_results_pass_to_model)\\n- [How to: pass run time values to tools](/docs/how_to/tool_runtime)\\n- [How to: add a human-in-the-loop for tools](/docs/how_to/tools_human)\\n- [How to: handle tool errors](/docs/how_to/tools_error)\\n- [How to: force models to call a tool](/docs/how_to/tool_choice)\\n- [How to: disable parallel tool calling](/docs/how_to/tool_calling_parallel)\\n- [How to: access the `RunnableConfig` from a tool](/docs/how_to/tool_configure)\\n- [How to: stream events from a tool](/docs/how_to/tool_stream_events)\\n- [How to: return artifacts from a tool](/docs/how_to/tool_artifacts/)\\n- [How to: convert Runnables to tools](/docs/how_to/convert_runnable_to_tool)\\n- [How to: add ad-hoc tool calling capability to models](/docs/how_to/tools_prompting)\\n- [How to: pass in runtime secrets](/docs/how_to/runnable_runtime_secrets)\\n\\n### Multimodal\\n\\n- [How to: pass multimodal data directly to models](/docs/how_to/multimodal_inputs/)\\n- [How to: use multimodal prompts](/docs/how_to/multimodal_prompts/)\\n\\n\\n### Agents\\n\\n:::note\\n\\nFor in depth how-to guides for agents, please check out [LangGraph](https://langchain-ai.github.io/langgraph/) documentation.\\n\\n:::\\n\\n- [How to: use legacy LangChain Agents (AgentExecutor)](/docs/how_to/agent_executor)\\n- [How to: migrate from legacy LangChain agents to LangGraph](/docs/how_to/migrate_agent)\\n\\n### Callbacks\\n\\n[Callbacks](/docs/concepts/callbacks) allow you to hook into the various stages of your LLM application\\'s execution.\\n\\n- [How to: pass in callbacks at runtime](/docs/how_to/callbacks_runtime)\\n- [How to: attach callbacks to a module](/docs/how_to/callbacks_attach)\\n- [How to: pass callbacks into a module constructor](/docs/how_to/callbacks_constructor)\\n- [How to: create custom callback handlers](/docs/how_to/custom_callbacks)\\n- [How to: use callbacks in async environments](/docs/how_to/callbacks_async)\\n- [How to: dispatch custom callback events](/docs/how_to/callbacks_custom_events)\\n\\n### Custom\\n\\nAll of LangChain components can easily be extended to support your own versions.\\n\\n- [How to: create a custom chat model class](/docs/how_to/custom_chat_model)\\n- [How to: create a custom LLM class](/docs/how_to/custom_llm)\\n- [How to: create a custom embeddings class](/docs/how_to/custom_embeddings)\\n- [How to: write a custom retriever class](/docs/how_to/custom_retriever)\\n- [How to: write a custom document loader](/docs/how_to/document_loader_custom)\\n- [How to: write a custom output parser class](/docs/how_to/output_parser_custom)\\n- [How to: create custom callback handlers](/docs/how_to/custom_callbacks)\\n- [How to: define a custom tool](/docs/how_to/custom_tools)\\n- [How to: dispatch custom callback events](/docs/how_to/callbacks_custom_events)\\n\\n### Serialization\\n- [How to: save and load LangChain objects](/docs/how_to/serialization)\\n\\n## Use cases\\n\\nThese guides cover use-case specific details.\\n\\n### Q&A with RAG\\n\\nRetrieval Augmented Generation (RAG) is a way to connect LLMs to external sources of data.\\nFor a high-level tutorial on RAG, check out [this guide](/docs/tutorials/rag/).\\n\\n- [How to: add chat history](/docs/how_to/qa_chat_history_how_to/)\\n- [How to: stream](/docs/how_to/qa_streaming/)\\n- [How to: return sources](/docs/how_to/qa_sources/)\\n- [How to: return citations](/docs/how_to/qa_citations/)\\n- [How to: do per-user retrieval](/docs/how_to/qa_per_user/)\\n\\n\\n### Extraction\\n\\nExtraction is when you use LLMs to extract structured information from unstructured text.\\nFor a high level tutorial on extraction, check out [this guide](/docs/tutorials/extraction/).\\n\\n- [How to: use reference examples](/docs/how_to/extraction_examples/)\\n- [How to: handle long text](/docs/how_to/extraction_long_text/)\\n- [How to: do extraction without using function calling](/docs/how_to/extraction_parse)\\n\\n### Chatbots\\n\\nChatbots involve using an LLM to have a conversation.\\nFor a high-level tutorial on building chatbots, check out [this guide](/docs/tutorials/chatbot/).\\n\\n- [How to: manage memory](/docs/how_to/chatbots_memory)\\n- [How to: do retrieval](/docs/how_to/chatbots_retrieval)\\n- [How to: use tools](/docs/how_to/chatbots_tools)\\n- [How to: manage large chat history](/docs/how_to/trim_messages/)\\n\\n### Query analysis\\n\\nQuery Analysis is the task of using an LLM to generate a query to send to a retriever.\\nFor a high-level tutorial on query analysis, check out [this guide](/docs/tutorials/rag/#query-analysis).\\n\\n- [How to: add examples to the prompt](/docs/how_to/query_few_shot)\\n- [How to: handle cases where no queries are generated](/docs/how_to/query_no_queries)\\n- [How to: handle multiple queries](/docs/how_to/query_multiple_queries)\\n- [How to: handle multiple retrievers](/docs/how_to/query_multiple_retrievers)\\n- [How to: construct filters](/docs/how_to/query_constructing_filters)\\n- [How to: deal with high cardinality categorical variables](/docs/how_to/query_high_cardinality)\\n\\n### Q&A over SQL + CSV\\n\\nYou can use LLMs to do question answering over tabular data.\\nFor a high-level tutorial, check out [this guide](/docs/tutorials/sql_qa/).\\n\\n- [How to: use prompting to improve results](/docs/how_to/sql_prompting)\\n- [How to: do query validation](/docs/how_to/sql_query_checking)\\n- [How to: deal with large databases](/docs/how_to/sql_large_db)\\n- [How to: deal with CSV files](/docs/how_to/sql_csv)\\n\\n### Q&A over graph databases\\n\\nYou can use an LLM to do question answering over graph databases.\\nFor a high-level tutorial, check out [this guide](/docs/tutorials/graph/).\\n\\n- [How to: add a semantic layer over the database](/docs/how_to/graph_semantic)\\n- [How to: construct knowledge graphs](/docs/how_to/graph_constructing)\\n\\n### Summarization\\n\\nLLMs can summarize and otherwise distill desired information from text, including\\nlarge volumes of text. For a high-level tutorial, check out [this guide](/docs/tutorials/summarization).\\n\\n- [How to: summarize text in a single LLM call](/docs/how_to/summarize_stuff)\\n- [How to: summarize text through parallelization](/docs/how_to/summarize_map_reduce)\\n- [How to: summarize text through iterative refinement](/docs/how_to/summarize_refine)\\n\\n## LangChain Expression Language (LCEL)\\n\\n:::note Should I use LCEL?\\n\\nLCEL is an orchestration solution. See our\\n[concepts page](/docs/concepts/lcel/#should-i-use-lcel) for recommendations on when to\\nuse LCEL.\\n\\n:::\\n\\n[LangChain Expression Language](/docs/concepts/lcel) is a way to create arbitrary custom chains. It is built on the [Runnable](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html) protocol.\\n\\n[**LCEL cheatsheet**](/docs/how_to/lcel_cheatsheet/): For a quick overview of how to use the main LCEL primitives.\\n\\n[**Migration guide**](/docs/versions/migrating_chains): For migrating legacy chain abstractions to LCEL.\\n\\n- [How to: chain runnables](/docs/how_to/sequence)\\n- [How to: stream runnables](/docs/how_to/streaming)\\n- [How to: invoke runnables in parallel](/docs/how_to/parallel/)\\n- [How to: add default invocation args to runnables](/docs/how_to/binding/)\\n- [How to: turn any function into a runnable](/docs/how_to/functions)\\n- [How to: pass through inputs from one chain step to the next](/docs/how_to/passthrough)\\n- [How to: configure runnable behavior at runtime](/docs/how_to/configure)\\n- [How to: add message history (memory) to a chain](/docs/how_to/message_history)\\n- [How to: route between sub-chains](/docs/how_to/routing)\\n- [How to: create a dynamic (self-constructing) chain](/docs/how_to/dynamic_chain/)\\n- [How to: inspect runnables](/docs/how_to/inspect)\\n- [How to: add fallbacks to a runnable](/docs/how_to/fallbacks)\\n- [How to: pass runtime secrets to a runnable](/docs/how_to/runnable_runtime_secrets)\\n\\n## [LangGraph](https://langchain-ai.github.io/langgraph)\\n\\nLangGraph is an extension of LangChain aimed at\\nbuilding robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.\\n\\nLangGraph documentation is currently hosted on a separate site.\\nYou can peruse [LangGraph how-to guides here](https://langchain-ai.github.io/langgraph/how-tos/).\\n\\n## [LangSmith](https://docs.smith.langchain.com/)\\n\\nLangSmith allows you to closely trace, monitor and evaluate your LLM application.\\nIt seamlessly integrates with LangChain and LangGraph, and you can use it to inspect and debug individual steps of your chains and agents as you build.\\n\\nLangSmith documentation is hosted on a separate site.\\nYou can peruse [LangSmith how-to guides here](https://docs.smith.langchain.com/), but we\\'ll highlight a few sections that are particularly\\nrelevant to LangChain below:\\n\\n### Evaluation\\n<span data-heading-keywords=\"evaluation,evaluate\"></span>\\n\\nEvaluating performance is a vital part of building LLM-powered applications.\\nLangSmith helps with every step of the process from creating a dataset to defining metrics to running evaluators.\\n\\nTo learn more, check out the [LangSmith evaluation how-to guides](https://docs.smith.langchain.com/how_to_guides#evaluation).\\n\\n### Tracing\\n<span data-heading-keywords=\"trace,tracing\"></span>\\n\\nTracing gives you observability inside your chains and agents, and is vital in diagnosing issues.\\n\\n- [How to: trace with LangChain](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain)\\n- [How to: add metadata and tags to traces](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#add-metadata-and-tags-to-traces)\\n\\nYou can see general tracing-related how-tos [in this section of the LangSmith docs](https://docs.smith.langchain.com/how_to_guides/tracing).\\n')]\n",
      "rank:0\n",
      "content_score_mapping:{'# Why LangChain?\\n\\nThe goal of `langchain` the Python package and LangChain the company is to make it as easy as possible for developers to build applications that reason.\\nWhile LangChain originally started as a single open source package, it has evolved into a company and a whole ecosystem.\\nThis page will talk about the LangChain ecosystem as a whole.\\nMost of the components within the LangChain ecosystem can be used by themselves - so if you feel particularly drawn to certain components but not others, that is totally fine! Pick and choose whichever components you like best for your own use case!\\n\\n## Features\\n\\nThere are several primary needs that LangChain aims to address:\\n\\n1. **Standardized component interfaces:** The growing number of [models](/docs/integrations/chat/) and [related components](/docs/integrations/vectorstores/) for AI applications has resulted in a wide variety of different APIs that developers need to learn and use.\\nThis diversity can make it challenging for developers to switch between providers or combine components when building applications.\\nLangChain exposes a standard interface for key components, making it easy to switch between providers.\\n\\n2. **Orchestration:** As applications become more complex, combining multiple components and models, there\\'s [a growing need to efficiently connect these elements into control flows](https://lilianweng.github.io/posts/2023-06-23-agent/) that can [accomplish diverse tasks](https://www.sequoiacap.com/article/generative-ais-act-o1/).\\n[Orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)) is crucial for building such applications.\\n\\n3. **Observability and evaluation:** As applications become more complex, it becomes increasingly difficult to understand what is happening within them.\\nFurthermore, the pace of development can become rate-limited by the [paradox of choice](https://en.wikipedia.org/wiki/Paradox_of_choice).\\nFor example, developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\n[Observability](https://en.wikipedia.org/wiki/Observability) and evaluations can help developers monitor their applications and rapidly answer these types of questions with confidence.\\n\\n\\n## Standardized component interfaces\\n\\nLangChain provides common interfaces for components that are central to many AI applications.\\nAs an example, all [chat models](/docs/concepts/chat_models/) implement the [BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) interface.\\nThis provides a standard way to interact with chat models, supporting important but often provider-specific features like [tool calling](/docs/concepts/tool_calling/) and [structured outputs](/docs/concepts/structured_outputs/).\\n\\n\\n### Example: chat models \\n\\nMany [model providers](/docs/concepts/chat_models/) support [tool calling](/docs/concepts/tool_calling/), a critical feature for many applications (e.g., [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/)), that allows a developer to request model responses that match a particular schema.\\nThe APIs for each provider differ. \\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to bind [tools](/docs/concepts/tools) to a model in order to support [tool calling](/docs/concepts/tool_calling/):\\n\\n```python\\n# Tool creation\\ntools = [my_tool]\\n# Tool binding\\nmodel_with_tools = model.bind_tools(tools)\\n```\\n\\nSimilarly, getting models to produce [structured outputs](/docs/concepts/structured_outputs/) is an extremely common use case. \\nProviders support different approaches for this, including [JSON mode or tool calling](https://platform.openai.com/docs/guides/structured-outputs), with different APIs.\\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to produce structured outputs using the `with_structured_output()` method:\\n\\n```python\\n# Define schema\\nschema = ...\\n# Bind schema to model\\nmodel_with_structure = model.with_structured_output(schema)\\n```\\n\\n### Example: retrievers\\n\\nIn the context of [RAG](/docs/concepts/rag/) and LLM application components, LangChain\\'s [retriever](/docs/concepts/retrievers/) interface provides a standard way to connect to many different types of data services or databases (e.g., [vector stores](/docs/concepts/vectorstores) or databases).\\nThe underlying implementation of the retriever depends on the type of data store or database you are connecting to, but all retrievers implement the [runnable interface](/docs/concepts/runnables/), meaning they can be invoked in a common manner.\\n\\n```python\\ndocuments = my_retriever.invoke(\"What is the meaning of life?\")\\n```\\n\\n## Orchestration \\n\\nWhile standardization for individual components is useful, we\\'ve increasingly seen that developers want to *combine* components into more complex applications. \\nThis motivates the need for [orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)).\\nThere are several common characteristics of LLM applications that this orchestration layer should support:\\n\\n* **Complex control flow:** The application requires complex patterns such as cycles (e.g., a loop that reiterates until a condition is met).\\n* **[Persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/):** The application needs to maintain [short-term and / or long-term memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n* **[Human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/):** The application needs human interaction, e.g., pausing, reviewing, editing, approving certain steps.\\n\\nThe recommended way to orchestrate components for complex applications is [LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/).\\nLangGraph is a library that gives developers a high degree of control by expressing the flow of the application as a set of nodes and edges.\\nLangGraph comes with built-in support for [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), [memory](https://langchain-ai.github.io/langgraph/concepts/memory/), and other features.\\nIt\\'s particularly well suited for building [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/) or [multi-agent](https://langchain-ai.github.io/langgraph/concepts/multi_agent/) applications. \\nImportantly, individual LangChain components can be used as LangGraph nodes, but you can also use LangGraph **without** using LangChain components.\\n\\n:::info[Further reading]\\n\\nHave a look at our free course, [Introduction to LangGraph](https://academy.langchain.com/courses/intro-to-langgraph), to learn more about how to use LangGraph to build complex applications.\\n\\n:::\\n\\n## Observability and evaluation\\n\\nThe pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. \\nDevelopers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\nHigh quality tracing and evaluations can help you rapidly answer these types of questions with confidence.\\n[LangSmith](https://docs.smith.langchain.com/) is our platform that supports observability and evaluation for AI applications.\\nSee our conceptual guides on [evaluations](https://docs.smith.langchain.com/concepts/evaluation) and [tracing](https://docs.smith.langchain.com/concepts/tracing) for more details.\\n\\n:::info[Further reading]\\n\\nSee our video playlist on [LangSmith tracing and evaluations](https://youtube.com/playlist?list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S&feature=shared) for more details.\\n\\n:::\\n\\n## Conclusion\\n\\nLangChain offers standard interfaces for components that are central to many AI applications, which offers a few specific advantages:\\n- **Ease of swapping providers:** It allows you to swap out different component providers without having to change the underlying code.\\n- **Advanced features:** It provides common methods for more advanced features, such as [streaming](/docs/concepts/streaming) and [tool calling](/docs/concepts/tool_calling/).\\n\\n[LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/) makes it possible to orchestrate complex applications (e.g., [agents](/docs/concepts/agents/)) and provide features like including [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), or [memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n\\n[LangSmith](https://docs.smith.langchain.com/) makes it possible to iterate with confidence on your applications, by providing LLM-specific observability and framework for testing and evaluating your application.\\n': 0.03333333333333333, '---\\nsidebar_position: 0\\nsidebar_class_name: hidden\\n---\\n\\n# Introduction\\n\\n**LangChain** is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:\\n- **Development**: Build your applications using LangChain\\'s open-source [components](/docs/concepts) and [third-party integrations](/docs/integrations/providers/).\\nUse [LangGraph](/docs/concepts/architecture/#langgraph) to build stateful agents with first-class streaming and human-in-the-loop support.\\n- **Productionization**: Use [LangSmith](https://docs.smith.langchain.com/) to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\n- **Deployment**: Turn your LangGraph applications into production-ready APIs and Assistants with [LangGraph Platform](https://langchain-ai.github.io/langgraph/cloud/).\\n\\nimport ThemedImage from \\'@theme/ThemedImage\\';\\nimport useBaseUrl from \\'@docusaurus/useBaseUrl\\';\\n\\n<ThemedImage\\n  alt=\"Diagram outlining the hierarchical organization of the LangChain framework, displaying the interconnected parts across multiple layers.\"\\n  sources={{\\n    light: useBaseUrl(\\'/svg/langchain_stack_112024.svg\\'),\\n    dark: useBaseUrl(\\'/svg/langchain_stack_112024_dark.svg\\'),\\n  }}\\n  style={{ width: \"100%\" }}\\n  title=\"LangChain Framework Overview\"\\n/>\\n\\nLangChain implements a standard interface for large language models and related\\ntechnologies, such as embedding models and vector stores, and integrates with\\nhundreds of providers. See the [integrations](/docs/integrations/providers/) page for\\nmore.\\n\\nimport ChatModelTabs from \"@theme/ChatModelTabs\";\\n\\n<ChatModelTabs/>\\n\\n```python\\nmodel.invoke(\"Hello, world!\")\\n```\\n\\n:::note\\n\\nThese docs focus on the Python LangChain library. [Head here](https://js.langchain.com) for docs on the JavaScript LangChain library.\\n\\n:::\\n\\n## Architecture\\n\\nThe LangChain framework consists of multiple open-source libraries. Read more in the\\n[Architecture](/docs/concepts/architecture/) page.\\n\\n- **`langchain-core`**: Base abstractions for chat models and other components.\\n- **Integration packages** (e.g. `langchain-openai`, `langchain-anthropic`, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\\n- **`langchain`**: Chains, agents, and retrieval strategies that make up an application\\'s cognitive architecture.\\n- **`langchain-community`**: Third-party integrations that are community maintained.\\n- **`langgraph`**: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See [LangGraph documentation](https://langchain-ai.github.io/langgraph/).\\n\\n## Guides\\n\\n### [Tutorials](/docs/tutorials)\\n\\nIf you\\'re looking to build something specific or are more of a hands-on learner, check out our [tutorials section](/docs/tutorials).\\nThis is the best place to get started.\\n\\nThese are the best ones to get started with:\\n\\n- [Build a Simple LLM Application](/docs/tutorials/llm_chain)\\n- [Build a Chatbot](/docs/tutorials/chatbot)\\n- [Build an Agent](/docs/tutorials/agents)\\n- [Introduction to LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/)\\n\\nExplore the full list of LangChain tutorials [here](/docs/tutorials), and check out other [LangGraph tutorials here](https://langchain-ai.github.io/langgraph/tutorials/). To learn more about LangGraph, check out our first LangChain Academy course, *Introduction to LangGraph*, available [here](https://academy.langchain.com/courses/intro-to-langgraph).\\n\\n\\n### [How-to guides](/docs/how_to)\\n\\n[Here](/docs/how_to) you’ll find short answers to “How do I….?” types of questions.\\nThese how-to guides don’t cover topics in depth – you’ll find that material in the [Tutorials](/docs/tutorials) and the [API Reference](https://python.langchain.com/api_reference/).\\nHowever, these guides will help you quickly accomplish common tasks using [chat models](/docs/how_to/#chat-models),\\n[vector stores](/docs/how_to/#vector-stores), and other common LangChain components.\\n\\nCheck out [LangGraph-specific how-tos here](https://langchain-ai.github.io/langgraph/how-tos/).\\n\\n### [Conceptual guide](/docs/concepts)\\n\\nIntroductions to all the key parts of LangChain you’ll need to know! [Here](/docs/concepts) you\\'ll find high level explanations of all LangChain concepts.\\n\\nFor a deeper dive into LangGraph concepts, check out [this page](https://langchain-ai.github.io/langgraph/concepts/).\\n\\n### [Integrations](integrations/providers/index.mdx)\\n\\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\\nIf you\\'re looking to get up and running quickly with [chat models](/docs/integrations/chat/), [vector stores](/docs/integrations/vectorstores/),\\nor other LangChain components from a specific provider, check out our growing list of [integrations](/docs/integrations/providers/).\\n\\n\\n### [API reference](https://python.langchain.com/api_reference/)\\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\\n\\n## Ecosystem\\n\\n### [🦜🛠️ LangSmith](https://docs.smith.langchain.com)\\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\\n\\n### [🦜🕸️ LangGraph](https://langchain-ai.github.io/langgraph)\\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by Linkedin, Uber, Klarna, GitLab, and many more.\\n\\n## Additional resources\\n\\n### [Versions](/docs/versions/v0_3/)\\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\\n\\n### [Security](/docs/security)\\nRead up on [security](/docs/security) best practices to make sure you\\'re developing safely with LangChain.\\n\\n### [Contributing](contributing/index.mdx)\\nCheck out the developer\\'s guide for guidelines on contributing and help getting your dev environment set up.\\n': 0.01639344262295082, '# LangChain Expression Language (LCEL)\\n\\n:::info Prerequisites\\n* [Runnable Interface](/docs/concepts/runnables)\\n:::\\n\\nThe **L**ang**C**hain **E**xpression **L**anguage (LCEL) takes a [declarative](https://en.wikipedia.org/wiki/Declarative_programming) approach to building new [Runnables](/docs/concepts/runnables) from existing Runnables.\\n\\nThis means that you describe what *should* happen, rather than *how* it should happen, allowing LangChain to optimize the run-time execution of the chains.\\n\\nWe often refer to a `Runnable` created using LCEL as a \"chain\". It\\'s important to remember that a \"chain\" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).\\n\\n:::note\\n* The [LCEL cheatsheet](/docs/how_to/lcel_cheatsheet/) shows common patterns that involve the Runnable interface and LCEL expressions.\\n* Please see the following list of [how-to guides](/docs/how_to/#langchain-expression-language-lcel) that cover common tasks with LCEL.\\n* A list of built-in `Runnables` can be found in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html). Many of these Runnables are useful when composing custom \"chains\" in LangChain using LCEL.\\n:::\\n\\n## Benefits of LCEL\\n\\nLangChain optimizes the run-time execution of chains built with LCEL in a number of ways:\\n\\n- **Optimized parallel execution**: Run Runnables in parallel using [RunnableParallel](#runnableparallel) or run multiple inputs through a given chain in parallel using the [Runnable Batch API](/docs/concepts/runnables/#optimized-parallel-execution-batch). Parallel execution can significantly reduce the latency as processing can be done in parallel instead of sequentially.\\n- **Guaranteed Async support**: Any chain built with LCEL can be run asynchronously using the [Runnable Async API](/docs/concepts/runnables/#asynchronous-support). This can be useful when running chains in a server environment where you want to handle large number of requests concurrently.\\n- **Simplify streaming**: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(time elapsed until the first chunk of output from a [chat model](/docs/concepts/chat_models) or [llm](/docs/concepts/text_llms) comes out).\\n\\nOther benefits include:\\n\\n- [**Seamless LangSmith tracing**](https://docs.smith.langchain.com)\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, **all** steps are automatically logged to [LangSmith](https://docs.smith.langchain.com/) for maximum observability and debuggability.\\n- **Standard API**: Because all chains are built using the Runnable interface, they can be used in the same way as any other Runnable.\\n- [**Deployable with LangServe**](/docs/concepts/architecture#langserve): Chains built with LCEL can be deployed using for production use.\\n\\n## Should I use LCEL?\\n\\nLCEL is an [orchestration solution](https://en.wikipedia.org/wiki/Orchestration_(computing)) -- it allows LangChain to handle run-time execution of chains in an optimized way.\\n\\nWhile we have seen users run chains with hundreds of steps in production, we generally recommend using LCEL for simpler orchestration tasks. When the application requires complex state management, branching, cycles or multiple agents, we recommend that users take advantage of [LangGraph](/docs/concepts/architecture#langgraph).\\n\\nIn LangGraph, users define graphs that specify the application\\'s flow. This allows users to keep using LCEL within individual nodes when LCEL is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.\\n\\nHere are some guidelines:\\n\\n* If you are making a single LLM call, you don\\'t need LCEL; instead call the underlying [chat model](/docs/concepts/chat_models) directly.\\n* If you have a simple chain (e.g., prompt + llm + parser, simple retrieval set up etc.), LCEL is a reasonable fit, if you\\'re taking advantage of the LCEL benefits.\\n* If you\\'re building a complex chain (e.g., with branching, cycles, multiple agents, etc.) use [LangGraph](/docs/concepts/architecture#langgraph) instead. Remember that you can always use LCEL within individual nodes in LangGraph.\\n\\n## Composition Primitives\\n\\n`LCEL` chains are built by composing existing `Runnables` together. The two main composition primitives are [RunnableSequence](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableSequence.html#langchain_core.runnables.base.RunnableSequence) and [RunnableParallel](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableParallel.html#langchain_core.runnables.base.RunnableParallel).\\n\\nMany other composition primitives (e.g., [RunnableAssign](\\nhttps://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnableAssign.html#langchain_core.runnables.passthrough.RunnableAssign\\n)) can be thought of as variations of these two primitives.\\n\\n:::note\\nYou can find a list of all composition primitives in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html).\\n:::\\n\\n### RunnableSequence\\n\\n`RunnableSequence` is a composition primitive that allows you \"chain\" multiple runnables sequentially, with the output of one runnable serving as the input to the next.\\n\\n```python\\nfrom langchain_core.runnables import RunnableSequence\\nchain = RunnableSequence([runnable1, runnable2])\\n```\\n\\nInvoking the `chain` with some input:\\n\\n```python\\nfinal_output = chain.invoke(some_input)\\n```\\n\\ncorresponds to the following:\\n\\n```python\\noutput1 = runnable1.invoke(some_input)\\nfinal_output = runnable2.invoke(output1)\\n```\\n\\n:::note\\n`runnable1` and `runnable2` are placeholders for any `Runnable` that you want to chain together.\\n:::\\n\\n### RunnableParallel\\n\\n`RunnableParallel` is a composition primitive that allows you to run multiple runnables concurrently, with the same input provided to each.\\n\\n```python\\nfrom langchain_core.runnables import RunnableParallel\\nchain = RunnableParallel({\\n    \"key1\": runnable1,\\n    \"key2\": runnable2,\\n})\\n```\\n\\nInvoking the `chain` with some input:\\n\\n```python\\nfinal_output = chain.invoke(some_input)\\n```\\n\\nWill yield a `final_output` dictionary with the same keys as the input dictionary, but with the values replaced by the output of the corresponding runnable.\\n\\n```python\\n{\\n    \"key1\": runnable1.invoke(some_input),\\n    \"key2\": runnable2.invoke(some_input),\\n}\\n```\\n\\nRecall, that the runnables are executed in parallel, so while the result is the same as\\ndictionary comprehension shown above, the execution time is much faster.\\n\\n:::note\\n`RunnableParallel`supports both synchronous and asynchronous execution (as all `Runnables` do).\\n\\n* For synchronous execution, `RunnableParallel` uses a [ThreadPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor) to run the runnables concurrently.\\n* For asynchronous execution, `RunnableParallel` uses [asyncio.gather](https://docs.python.org/3/library/asyncio.html#asyncio.gather) to run the runnables concurrently.\\n:::\\n\\n## Composition Syntax\\n\\nThe usage of `RunnableSequence` and `RunnableParallel` is so common that we created a shorthand syntax for using them. This helps\\nto make the code more readable and concise.\\n\\n### The `|` operator\\n\\nWe have [overloaded](https://docs.python.org/3/reference/datamodel.html#special-method-names) the `|` operator to create a `RunnableSequence` from two `Runnables`.\\n\\n```python\\nchain = runnable1 | runnable2\\n```\\n\\nis Equivalent to:\\n\\n```python\\nchain = RunnableSequence([runnable1, runnable2])\\n```\\n\\n### The `.pipe` method\\n\\nIf you have moral qualms with operator overloading, you can use the `.pipe` method instead. This is equivalent to the `|` operator.\\n\\n```python\\nchain = runnable1.pipe(runnable2)\\n```\\n\\n### Coercion\\n\\nLCEL applies automatic type coercion to make it easier to compose chains.\\n\\nIf you do not understand the type coercion, you can always use the `RunnableSequence` and `RunnableParallel` classes directly.\\n\\nThis will make the code more verbose, but it will also make it more explicit.\\n\\n#### Dictionary to RunnableParallel\\n\\nInside an LCEL expression, a dictionary is automatically converted to a `RunnableParallel`.\\n\\nFor example, the following code:\\n\\n```python\\nmapping = {\\n    \"key1\": runnable1,\\n    \"key2\": runnable2,\\n}\\n\\nchain = mapping | runnable3\\n```\\n\\nIt gets automatically converted to the following:\\n\\n```python\\nchain = RunnableSequence([RunnableParallel(mapping), runnable3])\\n```\\n\\n:::caution\\nYou have to be careful because the `mapping` dictionary is not a `RunnableParallel` object, it is just a dictionary. This means that the following code will raise an `AttributeError`:\\n\\n```python\\nmapping.invoke(some_input)\\n```\\n:::\\n\\n#### Function to RunnableLambda\\n\\nInside an LCEL expression, a function is automatically converted to a `RunnableLambda`.\\n\\n```\\ndef some_func(x):\\n    return x\\n\\nchain = some_func | runnable1\\n```\\n\\nIt gets automatically converted to the following:\\n\\n```python\\nchain = RunnableSequence([RunnableLambda(some_func), runnable1])\\n```\\n\\n:::caution\\nYou have to be careful because the lambda function is not a `RunnableLambda` object, it is just a function. This means that the following code will raise an `AttributeError`:\\n\\n```python\\nlambda x: x + 1.invoke(some_input)\\n```\\n:::\\n\\n## Legacy chains\\n\\nLCEL aims to provide consistency around behavior and customization over legacy subclassed chains such as `LLMChain` and\\n`ConversationalRetrievalChain`. Many of these legacy chains hide important details like prompts, and as a wider variety\\nof viable models emerge, customization has become more and more important.\\n\\nIf you are currently using one of these legacy chains, please see [this guide for guidance on how to migrate](/docs/versions/migrating_chains).\\n\\nFor guides on how to do specific tasks with LCEL, check out [the relevant how-to guides](/docs/how_to/#langchain-expression-language-lcel).\\n': 0.016129032258064516, '# Conceptual guide\\n\\nThis guide provides explanations of the key concepts behind the LangChain framework and AI applications more broadly.\\n\\nWe recommend that you go through at least one of the [Tutorials](/docs/tutorials) before diving into the conceptual guide. This will provide practical context that will make it easier to understand the concepts discussed here.\\n\\nThe conceptual guide does not cover step-by-step instructions or specific implementation examples — those are found in the [How-to guides](/docs/how_to/) and [Tutorials](/docs/tutorials). For detailed reference material, please see the [API reference](https://python.langchain.com/api_reference/).\\n\\n## High level\\n\\n- **[Why LangChain?](/docs/concepts/why_langchain)**: Overview of the value that LangChain provides.\\n- **[Architecture](/docs/concepts/architecture)**: How packages are organized in the LangChain ecosystem.\\n\\n## Concepts\\n\\n- **[Chat models](/docs/concepts/chat_models)**: LLMs exposed via a chat API that process sequences of messages as input and output a message.\\n- **[Messages](/docs/concepts/messages)**: The unit of communication in chat models, used to represent model input and output.\\n- **[Chat history](/docs/concepts/chat_history)**: A conversation represented as a sequence of messages, alternating between user messages and model responses.\\n- **[Tools](/docs/concepts/tools)**: A function with an associated schema defining the function\\'s name, description, and the arguments it accepts.\\n- **[Tool calling](/docs/concepts/tool_calling)**: A type of chat model API that accepts tool schemas, along with messages, as input and returns invocations of those tools as part of the output message.\\n- **[Structured output](/docs/concepts/structured_outputs)**: A technique to make a chat model respond in a structured format, such as JSON that matches a given schema.\\n- **[Memory](https://langchain-ai.github.io/langgraph/concepts/memory/)**: Information about a conversation that is persisted so that it can be used in future conversations.\\n- **[Multimodality](/docs/concepts/multimodality)**: The ability to work with data that comes in different forms, such as text, audio, images, and video.\\n- **[Runnable interface](/docs/concepts/runnables)**: The base abstraction that many LangChain components and the LangChain Expression Language are built on.\\n- **[Streaming](/docs/concepts/streaming)**: LangChain streaming APIs for surfacing results as they are generated.\\n- **[LangChain Expression Language (LCEL)](/docs/concepts/lcel)**: A syntax for orchestrating LangChain components. Most useful for simpler applications.\\n- **[Document loaders](/docs/concepts/document_loaders)**: Load a source as a list of documents.\\n- **[Retrieval](/docs/concepts/retrieval)**: Information retrieval systems can retrieve structured or unstructured data from a datasource in response to a query.\\n- **[Text splitters](/docs/concepts/text_splitters)**: Split long text into smaller chunks that can be individually indexed to enable granular retrieval.\\n- **[Embedding models](/docs/concepts/embedding_models)**: Models that represent data such as text or images in a vector space.\\n- **[Vector stores](/docs/concepts/vectorstores)**: Storage of and efficient search over vectors and associated metadata.\\n- **[Retriever](/docs/concepts/retrievers)**: A component that returns relevant documents from a knowledge base in response to a query.\\n- **[Retrieval Augmented Generation (RAG)](/docs/concepts/rag)**: A technique that enhances language models by combining them with external knowledge bases.\\n- **[Agents](/docs/concepts/agents)**: Use a [language model](/docs/concepts/chat_models) to choose a sequence of actions to take. Agents can interact with external resources via [tool](/docs/concepts/tools).\\n- **[Prompt templates](/docs/concepts/prompt_templates)**: Component for factoring out the static parts of a model \"prompt\" (usually a sequence of messages). Useful for serializing, versioning, and reusing these static parts.\\n- **[Output parsers](/docs/concepts/output_parsers)**: Responsible for taking the output of a model and transforming it into a more suitable format for downstream tasks. Output parsers were primarily useful prior to the general availability of [tool calling](/docs/concepts/tool_calling) and [structured outputs](/docs/concepts/structured_outputs).\\n- **[Few-shot prompting](/docs/concepts/few_shot_prompting)**: A technique for improving model performance by providing a few examples of the task to perform in the prompt.\\n- **[Example selectors](/docs/concepts/example_selectors)**: Used to select the most relevant examples from a dataset based on a given input. Example selectors are used in few-shot prompting to select examples for a prompt.\\n- **[Async programming](/docs/concepts/async)**: The basics that one should know to use LangChain in an asynchronous context.\\n- **[Callbacks](/docs/concepts/callbacks)**: Callbacks enable the execution of custom auxiliary code in built-in components. Callbacks are used to stream outputs from LLMs in LangChain, trace the intermediate steps of an application, and more.\\n- **[Tracing](/docs/concepts/tracing)**: The process of recording the steps that an application takes to go from input to output. Tracing is essential for debugging and diagnosing issues in complex applications.\\n- **[Evaluation](/docs/concepts/evaluation)**: The process of assessing the performance and effectiveness of AI applications. This involves testing the model\\'s responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose. This process is vital for building reliable applications.\\n- **[Testing](/docs/concepts/testing)**: The process of verifying that a component of an integration or application works as expected. Testing is essential for ensuring that the application behaves correctly and that changes to the codebase do not introduce new bugs.\\n\\n## Glossary\\n\\n- **[AIMessageChunk](/docs/concepts/messages#aimessagechunk)**: A partial response from an AI message. Used when streaming responses from a chat model.\\n- **[AIMessage](/docs/concepts/messages#aimessage)**: Represents a complete response from an AI model.\\n- **[astream_events](/docs/concepts/chat_models#key-methods)**: Stream granular information from [LCEL](/docs/concepts/lcel) chains.\\n- **[BaseTool](/docs/concepts/tools/#tool-interface)**: The base class for all tools in LangChain.\\n- **[batch](/docs/concepts/runnables)**: Use to execute a runnable with batch inputs.\\n- **[bind_tools](/docs/concepts/tool_calling/#tool-binding)**: Allows models to interact with tools.\\n- **[Caching](/docs/concepts/chat_models#caching)**: Storing results to avoid redundant calls to a chat model.\\n- **[Chat models](/docs/concepts/multimodality/#multimodality-in-chat-models)**: Chat models that handle multiple data modalities.\\n- **[Configurable runnables](/docs/concepts/runnables/#configurable-runnables)**: Creating configurable Runnables.\\n- **[Context window](/docs/concepts/chat_models#context-window)**: The maximum size of input a chat model can process.\\n- **[Conversation patterns](/docs/concepts/chat_history#conversation-patterns)**: Common patterns in chat interactions.\\n- **[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)**: LangChain\\'s representation of a document.\\n- **[Embedding models](/docs/concepts/multimodality/#multimodality-in-embedding-models)**: Models that generate vector embeddings for various data types.\\n- **[HumanMessage](/docs/concepts/messages#humanmessage)**: Represents a message from a human user.\\n- **[InjectedState](/docs/concepts/tools#injectedstate)**: A state injected into a tool function.\\n- **[InjectedStore](/docs/concepts/tools#injectedstore)**: A store that can be injected into a tool for data persistence.\\n- **[InjectedToolArg](/docs/concepts/tools#injectedtoolarg)**: Mechanism to inject arguments into tool functions.\\n- **[input and output types](/docs/concepts/runnables#input-and-output-types)**: Types used for input and output in Runnables.\\n- **[Integration packages](/docs/concepts/architecture/#integration-packages)**: Third-party packages that integrate with LangChain.\\n- **[Integration tests](/docs/concepts/testing#integration-tests)**: Tests that verify the correctness of the interaction between components, usually run with access to the underlying API that powers an integration.\\n- **[invoke](/docs/concepts/runnables)**: A standard method to invoke a Runnable.\\n- **[JSON mode](/docs/concepts/structured_outputs#json-mode)**: Returning responses in JSON format.\\n- **[langchain-community](/docs/concepts/architecture#langchain-community)**: Community-driven components for LangChain.\\n- **[langchain-core](/docs/concepts/architecture#langchain-core)**: Core langchain package. Includes base interfaces and in-memory implementations.\\n- **[langchain](/docs/concepts/architecture#langchain)**: A package for higher level components (e.g., some pre-built chains).\\n- **[langgraph](/docs/concepts/architecture#langgraph)**: Powerful orchestration layer for LangChain. Use to build complex pipelines and workflows.\\n- **[langserve](/docs/concepts/architecture#langserve)**: Used to deploy LangChain Runnables as REST endpoints. Uses FastAPI. Works primarily for LangChain Runnables, does not currently integrate with LangGraph.\\n- **[LLMs (legacy)](/docs/concepts/text_llms)**: Older language models that take a string as input and return a string as output.\\n- **[Managing chat history](/docs/concepts/chat_history#managing-chat-history)**: Techniques to maintain and manage the chat history.\\n- **[OpenAI format](/docs/concepts/messages#openai-format)**: OpenAI\\'s message format for chat models.\\n- **[Propagation of RunnableConfig](/docs/concepts/runnables/#propagation-of-runnableconfig)**: Propagating configuration through Runnables. Read if working with python 3.9, 3.10 and async.\\n- **[rate-limiting](/docs/concepts/chat_models#rate-limiting)**: Client side rate limiting for chat models.\\n- **[RemoveMessage](/docs/concepts/messages/#removemessage)**: An abstraction used to remove a message from chat history, used primarily in LangGraph.\\n- **[role](/docs/concepts/messages#role)**: Represents the role (e.g., user, assistant) of a chat message.\\n- **[RunnableConfig](/docs/concepts/runnables/#runnableconfig)**: Use to pass run time information to Runnables (e.g., `run_name`, `run_id`, `tags`, `metadata`, `max_concurrency`, `recursion_limit`, `configurable`).\\n- **[Standard parameters for chat models](/docs/concepts/chat_models#standard-parameters)**: Parameters such as API key, `temperature`, and `max_tokens`.\\n- **[Standard tests](/docs/concepts/testing#standard-tests)**: A defined set of unit and integration tests that all integrations must pass.\\n- **[stream](/docs/concepts/streaming)**: Use to stream output from a Runnable or a graph.\\n- **[Tokenization](/docs/concepts/tokens)**: The process of converting data into tokens and vice versa.\\n- **[Tokens](/docs/concepts/tokens)**: The basic unit that a language model reads, processes, and generates under the hood.\\n- **[Tool artifacts](/docs/concepts/tools#tool-artifacts)**: Add artifacts to the output of a tool that will not be sent to the model, but will be available for downstream processing.\\n- **[Tool binding](/docs/concepts/tool_calling#tool-binding)**: Binding tools to models.\\n- **[@tool](/docs/concepts/tools/#create-tools-using-the-tool-decorator)**: Decorator for creating tools in LangChain.\\n- **[Toolkits](/docs/concepts/tools#toolkits)**: A collection of tools that can be used together.\\n- **[ToolMessage](/docs/concepts/messages#toolmessage)**: Represents a message that contains the results of a tool execution.\\n- **[Unit tests](/docs/concepts/testing#unit-tests)**: Tests that verify the correctness of individual components, run in isolation without access to the Internet.\\n- **[Vector stores](/docs/concepts/vectorstores)**: Datastores specialized for storing and efficiently searching vector embeddings.\\n- **[with_structured_output](/docs/concepts/structured_outputs/#structured-output-method)**: A helper method for chat models that natively support [tool calling](/docs/concepts/tool_calling) to get structured output matching a given schema specified via Pydantic, JSON schema or a function.\\n- **[with_types](/docs/concepts/runnables#with_types)**: Method to overwrite the input and output types of a runnable. Useful when working with complex LCEL chains and deploying with LangServe.\\n': 0.015873015873015872}\n",
      "rank:1\n",
      "content_score_mapping:{'# Why LangChain?\\n\\nThe goal of `langchain` the Python package and LangChain the company is to make it as easy as possible for developers to build applications that reason.\\nWhile LangChain originally started as a single open source package, it has evolved into a company and a whole ecosystem.\\nThis page will talk about the LangChain ecosystem as a whole.\\nMost of the components within the LangChain ecosystem can be used by themselves - so if you feel particularly drawn to certain components but not others, that is totally fine! Pick and choose whichever components you like best for your own use case!\\n\\n## Features\\n\\nThere are several primary needs that LangChain aims to address:\\n\\n1. **Standardized component interfaces:** The growing number of [models](/docs/integrations/chat/) and [related components](/docs/integrations/vectorstores/) for AI applications has resulted in a wide variety of different APIs that developers need to learn and use.\\nThis diversity can make it challenging for developers to switch between providers or combine components when building applications.\\nLangChain exposes a standard interface for key components, making it easy to switch between providers.\\n\\n2. **Orchestration:** As applications become more complex, combining multiple components and models, there\\'s [a growing need to efficiently connect these elements into control flows](https://lilianweng.github.io/posts/2023-06-23-agent/) that can [accomplish diverse tasks](https://www.sequoiacap.com/article/generative-ais-act-o1/).\\n[Orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)) is crucial for building such applications.\\n\\n3. **Observability and evaluation:** As applications become more complex, it becomes increasingly difficult to understand what is happening within them.\\nFurthermore, the pace of development can become rate-limited by the [paradox of choice](https://en.wikipedia.org/wiki/Paradox_of_choice).\\nFor example, developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\n[Observability](https://en.wikipedia.org/wiki/Observability) and evaluations can help developers monitor their applications and rapidly answer these types of questions with confidence.\\n\\n\\n## Standardized component interfaces\\n\\nLangChain provides common interfaces for components that are central to many AI applications.\\nAs an example, all [chat models](/docs/concepts/chat_models/) implement the [BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) interface.\\nThis provides a standard way to interact with chat models, supporting important but often provider-specific features like [tool calling](/docs/concepts/tool_calling/) and [structured outputs](/docs/concepts/structured_outputs/).\\n\\n\\n### Example: chat models \\n\\nMany [model providers](/docs/concepts/chat_models/) support [tool calling](/docs/concepts/tool_calling/), a critical feature for many applications (e.g., [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/)), that allows a developer to request model responses that match a particular schema.\\nThe APIs for each provider differ. \\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to bind [tools](/docs/concepts/tools) to a model in order to support [tool calling](/docs/concepts/tool_calling/):\\n\\n```python\\n# Tool creation\\ntools = [my_tool]\\n# Tool binding\\nmodel_with_tools = model.bind_tools(tools)\\n```\\n\\nSimilarly, getting models to produce [structured outputs](/docs/concepts/structured_outputs/) is an extremely common use case. \\nProviders support different approaches for this, including [JSON mode or tool calling](https://platform.openai.com/docs/guides/structured-outputs), with different APIs.\\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to produce structured outputs using the `with_structured_output()` method:\\n\\n```python\\n# Define schema\\nschema = ...\\n# Bind schema to model\\nmodel_with_structure = model.with_structured_output(schema)\\n```\\n\\n### Example: retrievers\\n\\nIn the context of [RAG](/docs/concepts/rag/) and LLM application components, LangChain\\'s [retriever](/docs/concepts/retrievers/) interface provides a standard way to connect to many different types of data services or databases (e.g., [vector stores](/docs/concepts/vectorstores) or databases).\\nThe underlying implementation of the retriever depends on the type of data store or database you are connecting to, but all retrievers implement the [runnable interface](/docs/concepts/runnables/), meaning they can be invoked in a common manner.\\n\\n```python\\ndocuments = my_retriever.invoke(\"What is the meaning of life?\")\\n```\\n\\n## Orchestration \\n\\nWhile standardization for individual components is useful, we\\'ve increasingly seen that developers want to *combine* components into more complex applications. \\nThis motivates the need for [orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)).\\nThere are several common characteristics of LLM applications that this orchestration layer should support:\\n\\n* **Complex control flow:** The application requires complex patterns such as cycles (e.g., a loop that reiterates until a condition is met).\\n* **[Persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/):** The application needs to maintain [short-term and / or long-term memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n* **[Human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/):** The application needs human interaction, e.g., pausing, reviewing, editing, approving certain steps.\\n\\nThe recommended way to orchestrate components for complex applications is [LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/).\\nLangGraph is a library that gives developers a high degree of control by expressing the flow of the application as a set of nodes and edges.\\nLangGraph comes with built-in support for [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), [memory](https://langchain-ai.github.io/langgraph/concepts/memory/), and other features.\\nIt\\'s particularly well suited for building [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/) or [multi-agent](https://langchain-ai.github.io/langgraph/concepts/multi_agent/) applications. \\nImportantly, individual LangChain components can be used as LangGraph nodes, but you can also use LangGraph **without** using LangChain components.\\n\\n:::info[Further reading]\\n\\nHave a look at our free course, [Introduction to LangGraph](https://academy.langchain.com/courses/intro-to-langgraph), to learn more about how to use LangGraph to build complex applications.\\n\\n:::\\n\\n## Observability and evaluation\\n\\nThe pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. \\nDevelopers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\nHigh quality tracing and evaluations can help you rapidly answer these types of questions with confidence.\\n[LangSmith](https://docs.smith.langchain.com/) is our platform that supports observability and evaluation for AI applications.\\nSee our conceptual guides on [evaluations](https://docs.smith.langchain.com/concepts/evaluation) and [tracing](https://docs.smith.langchain.com/concepts/tracing) for more details.\\n\\n:::info[Further reading]\\n\\nSee our video playlist on [LangSmith tracing and evaluations](https://youtube.com/playlist?list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S&feature=shared) for more details.\\n\\n:::\\n\\n## Conclusion\\n\\nLangChain offers standard interfaces for components that are central to many AI applications, which offers a few specific advantages:\\n- **Ease of swapping providers:** It allows you to swap out different component providers without having to change the underlying code.\\n- **Advanced features:** It provides common methods for more advanced features, such as [streaming](/docs/concepts/streaming) and [tool calling](/docs/concepts/tool_calling/).\\n\\n[LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/) makes it possible to orchestrate complex applications (e.g., [agents](/docs/concepts/agents/)) and provide features like including [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), or [memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n\\n[LangSmith](https://docs.smith.langchain.com/) makes it possible to iterate with confidence on your applications, by providing LLM-specific observability and framework for testing and evaluating your application.\\n': 0.03333333333333333, '---\\nsidebar_position: 0\\nsidebar_class_name: hidden\\n---\\n\\n# Introduction\\n\\n**LangChain** is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:\\n- **Development**: Build your applications using LangChain\\'s open-source [components](/docs/concepts) and [third-party integrations](/docs/integrations/providers/).\\nUse [LangGraph](/docs/concepts/architecture/#langgraph) to build stateful agents with first-class streaming and human-in-the-loop support.\\n- **Productionization**: Use [LangSmith](https://docs.smith.langchain.com/) to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\n- **Deployment**: Turn your LangGraph applications into production-ready APIs and Assistants with [LangGraph Platform](https://langchain-ai.github.io/langgraph/cloud/).\\n\\nimport ThemedImage from \\'@theme/ThemedImage\\';\\nimport useBaseUrl from \\'@docusaurus/useBaseUrl\\';\\n\\n<ThemedImage\\n  alt=\"Diagram outlining the hierarchical organization of the LangChain framework, displaying the interconnected parts across multiple layers.\"\\n  sources={{\\n    light: useBaseUrl(\\'/svg/langchain_stack_112024.svg\\'),\\n    dark: useBaseUrl(\\'/svg/langchain_stack_112024_dark.svg\\'),\\n  }}\\n  style={{ width: \"100%\" }}\\n  title=\"LangChain Framework Overview\"\\n/>\\n\\nLangChain implements a standard interface for large language models and related\\ntechnologies, such as embedding models and vector stores, and integrates with\\nhundreds of providers. See the [integrations](/docs/integrations/providers/) page for\\nmore.\\n\\nimport ChatModelTabs from \"@theme/ChatModelTabs\";\\n\\n<ChatModelTabs/>\\n\\n```python\\nmodel.invoke(\"Hello, world!\")\\n```\\n\\n:::note\\n\\nThese docs focus on the Python LangChain library. [Head here](https://js.langchain.com) for docs on the JavaScript LangChain library.\\n\\n:::\\n\\n## Architecture\\n\\nThe LangChain framework consists of multiple open-source libraries. Read more in the\\n[Architecture](/docs/concepts/architecture/) page.\\n\\n- **`langchain-core`**: Base abstractions for chat models and other components.\\n- **Integration packages** (e.g. `langchain-openai`, `langchain-anthropic`, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\\n- **`langchain`**: Chains, agents, and retrieval strategies that make up an application\\'s cognitive architecture.\\n- **`langchain-community`**: Third-party integrations that are community maintained.\\n- **`langgraph`**: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See [LangGraph documentation](https://langchain-ai.github.io/langgraph/).\\n\\n## Guides\\n\\n### [Tutorials](/docs/tutorials)\\n\\nIf you\\'re looking to build something specific or are more of a hands-on learner, check out our [tutorials section](/docs/tutorials).\\nThis is the best place to get started.\\n\\nThese are the best ones to get started with:\\n\\n- [Build a Simple LLM Application](/docs/tutorials/llm_chain)\\n- [Build a Chatbot](/docs/tutorials/chatbot)\\n- [Build an Agent](/docs/tutorials/agents)\\n- [Introduction to LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/)\\n\\nExplore the full list of LangChain tutorials [here](/docs/tutorials), and check out other [LangGraph tutorials here](https://langchain-ai.github.io/langgraph/tutorials/). To learn more about LangGraph, check out our first LangChain Academy course, *Introduction to LangGraph*, available [here](https://academy.langchain.com/courses/intro-to-langgraph).\\n\\n\\n### [How-to guides](/docs/how_to)\\n\\n[Here](/docs/how_to) you’ll find short answers to “How do I….?” types of questions.\\nThese how-to guides don’t cover topics in depth – you’ll find that material in the [Tutorials](/docs/tutorials) and the [API Reference](https://python.langchain.com/api_reference/).\\nHowever, these guides will help you quickly accomplish common tasks using [chat models](/docs/how_to/#chat-models),\\n[vector stores](/docs/how_to/#vector-stores), and other common LangChain components.\\n\\nCheck out [LangGraph-specific how-tos here](https://langchain-ai.github.io/langgraph/how-tos/).\\n\\n### [Conceptual guide](/docs/concepts)\\n\\nIntroductions to all the key parts of LangChain you’ll need to know! [Here](/docs/concepts) you\\'ll find high level explanations of all LangChain concepts.\\n\\nFor a deeper dive into LangGraph concepts, check out [this page](https://langchain-ai.github.io/langgraph/concepts/).\\n\\n### [Integrations](integrations/providers/index.mdx)\\n\\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\\nIf you\\'re looking to get up and running quickly with [chat models](/docs/integrations/chat/), [vector stores](/docs/integrations/vectorstores/),\\nor other LangChain components from a specific provider, check out our growing list of [integrations](/docs/integrations/providers/).\\n\\n\\n### [API reference](https://python.langchain.com/api_reference/)\\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\\n\\n## Ecosystem\\n\\n### [🦜🛠️ LangSmith](https://docs.smith.langchain.com)\\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\\n\\n### [🦜🕸️ LangGraph](https://langchain-ai.github.io/langgraph)\\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by Linkedin, Uber, Klarna, GitLab, and many more.\\n\\n## Additional resources\\n\\n### [Versions](/docs/versions/v0_3/)\\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\\n\\n### [Security](/docs/security)\\nRead up on [security](/docs/security) best practices to make sure you\\'re developing safely with LangChain.\\n\\n### [Contributing](contributing/index.mdx)\\nCheck out the developer\\'s guide for guidelines on contributing and help getting your dev environment set up.\\n': 0.01639344262295082, '# LangChain Expression Language (LCEL)\\n\\n:::info Prerequisites\\n* [Runnable Interface](/docs/concepts/runnables)\\n:::\\n\\nThe **L**ang**C**hain **E**xpression **L**anguage (LCEL) takes a [declarative](https://en.wikipedia.org/wiki/Declarative_programming) approach to building new [Runnables](/docs/concepts/runnables) from existing Runnables.\\n\\nThis means that you describe what *should* happen, rather than *how* it should happen, allowing LangChain to optimize the run-time execution of the chains.\\n\\nWe often refer to a `Runnable` created using LCEL as a \"chain\". It\\'s important to remember that a \"chain\" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).\\n\\n:::note\\n* The [LCEL cheatsheet](/docs/how_to/lcel_cheatsheet/) shows common patterns that involve the Runnable interface and LCEL expressions.\\n* Please see the following list of [how-to guides](/docs/how_to/#langchain-expression-language-lcel) that cover common tasks with LCEL.\\n* A list of built-in `Runnables` can be found in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html). Many of these Runnables are useful when composing custom \"chains\" in LangChain using LCEL.\\n:::\\n\\n## Benefits of LCEL\\n\\nLangChain optimizes the run-time execution of chains built with LCEL in a number of ways:\\n\\n- **Optimized parallel execution**: Run Runnables in parallel using [RunnableParallel](#runnableparallel) or run multiple inputs through a given chain in parallel using the [Runnable Batch API](/docs/concepts/runnables/#optimized-parallel-execution-batch). Parallel execution can significantly reduce the latency as processing can be done in parallel instead of sequentially.\\n- **Guaranteed Async support**: Any chain built with LCEL can be run asynchronously using the [Runnable Async API](/docs/concepts/runnables/#asynchronous-support). This can be useful when running chains in a server environment where you want to handle large number of requests concurrently.\\n- **Simplify streaming**: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(time elapsed until the first chunk of output from a [chat model](/docs/concepts/chat_models) or [llm](/docs/concepts/text_llms) comes out).\\n\\nOther benefits include:\\n\\n- [**Seamless LangSmith tracing**](https://docs.smith.langchain.com)\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, **all** steps are automatically logged to [LangSmith](https://docs.smith.langchain.com/) for maximum observability and debuggability.\\n- **Standard API**: Because all chains are built using the Runnable interface, they can be used in the same way as any other Runnable.\\n- [**Deployable with LangServe**](/docs/concepts/architecture#langserve): Chains built with LCEL can be deployed using for production use.\\n\\n## Should I use LCEL?\\n\\nLCEL is an [orchestration solution](https://en.wikipedia.org/wiki/Orchestration_(computing)) -- it allows LangChain to handle run-time execution of chains in an optimized way.\\n\\nWhile we have seen users run chains with hundreds of steps in production, we generally recommend using LCEL for simpler orchestration tasks. When the application requires complex state management, branching, cycles or multiple agents, we recommend that users take advantage of [LangGraph](/docs/concepts/architecture#langgraph).\\n\\nIn LangGraph, users define graphs that specify the application\\'s flow. This allows users to keep using LCEL within individual nodes when LCEL is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.\\n\\nHere are some guidelines:\\n\\n* If you are making a single LLM call, you don\\'t need LCEL; instead call the underlying [chat model](/docs/concepts/chat_models) directly.\\n* If you have a simple chain (e.g., prompt + llm + parser, simple retrieval set up etc.), LCEL is a reasonable fit, if you\\'re taking advantage of the LCEL benefits.\\n* If you\\'re building a complex chain (e.g., with branching, cycles, multiple agents, etc.) use [LangGraph](/docs/concepts/architecture#langgraph) instead. Remember that you can always use LCEL within individual nodes in LangGraph.\\n\\n## Composition Primitives\\n\\n`LCEL` chains are built by composing existing `Runnables` together. The two main composition primitives are [RunnableSequence](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableSequence.html#langchain_core.runnables.base.RunnableSequence) and [RunnableParallel](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableParallel.html#langchain_core.runnables.base.RunnableParallel).\\n\\nMany other composition primitives (e.g., [RunnableAssign](\\nhttps://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnableAssign.html#langchain_core.runnables.passthrough.RunnableAssign\\n)) can be thought of as variations of these two primitives.\\n\\n:::note\\nYou can find a list of all composition primitives in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html).\\n:::\\n\\n### RunnableSequence\\n\\n`RunnableSequence` is a composition primitive that allows you \"chain\" multiple runnables sequentially, with the output of one runnable serving as the input to the next.\\n\\n```python\\nfrom langchain_core.runnables import RunnableSequence\\nchain = RunnableSequence([runnable1, runnable2])\\n```\\n\\nInvoking the `chain` with some input:\\n\\n```python\\nfinal_output = chain.invoke(some_input)\\n```\\n\\ncorresponds to the following:\\n\\n```python\\noutput1 = runnable1.invoke(some_input)\\nfinal_output = runnable2.invoke(output1)\\n```\\n\\n:::note\\n`runnable1` and `runnable2` are placeholders for any `Runnable` that you want to chain together.\\n:::\\n\\n### RunnableParallel\\n\\n`RunnableParallel` is a composition primitive that allows you to run multiple runnables concurrently, with the same input provided to each.\\n\\n```python\\nfrom langchain_core.runnables import RunnableParallel\\nchain = RunnableParallel({\\n    \"key1\": runnable1,\\n    \"key2\": runnable2,\\n})\\n```\\n\\nInvoking the `chain` with some input:\\n\\n```python\\nfinal_output = chain.invoke(some_input)\\n```\\n\\nWill yield a `final_output` dictionary with the same keys as the input dictionary, but with the values replaced by the output of the corresponding runnable.\\n\\n```python\\n{\\n    \"key1\": runnable1.invoke(some_input),\\n    \"key2\": runnable2.invoke(some_input),\\n}\\n```\\n\\nRecall, that the runnables are executed in parallel, so while the result is the same as\\ndictionary comprehension shown above, the execution time is much faster.\\n\\n:::note\\n`RunnableParallel`supports both synchronous and asynchronous execution (as all `Runnables` do).\\n\\n* For synchronous execution, `RunnableParallel` uses a [ThreadPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor) to run the runnables concurrently.\\n* For asynchronous execution, `RunnableParallel` uses [asyncio.gather](https://docs.python.org/3/library/asyncio.html#asyncio.gather) to run the runnables concurrently.\\n:::\\n\\n## Composition Syntax\\n\\nThe usage of `RunnableSequence` and `RunnableParallel` is so common that we created a shorthand syntax for using them. This helps\\nto make the code more readable and concise.\\n\\n### The `|` operator\\n\\nWe have [overloaded](https://docs.python.org/3/reference/datamodel.html#special-method-names) the `|` operator to create a `RunnableSequence` from two `Runnables`.\\n\\n```python\\nchain = runnable1 | runnable2\\n```\\n\\nis Equivalent to:\\n\\n```python\\nchain = RunnableSequence([runnable1, runnable2])\\n```\\n\\n### The `.pipe` method\\n\\nIf you have moral qualms with operator overloading, you can use the `.pipe` method instead. This is equivalent to the `|` operator.\\n\\n```python\\nchain = runnable1.pipe(runnable2)\\n```\\n\\n### Coercion\\n\\nLCEL applies automatic type coercion to make it easier to compose chains.\\n\\nIf you do not understand the type coercion, you can always use the `RunnableSequence` and `RunnableParallel` classes directly.\\n\\nThis will make the code more verbose, but it will also make it more explicit.\\n\\n#### Dictionary to RunnableParallel\\n\\nInside an LCEL expression, a dictionary is automatically converted to a `RunnableParallel`.\\n\\nFor example, the following code:\\n\\n```python\\nmapping = {\\n    \"key1\": runnable1,\\n    \"key2\": runnable2,\\n}\\n\\nchain = mapping | runnable3\\n```\\n\\nIt gets automatically converted to the following:\\n\\n```python\\nchain = RunnableSequence([RunnableParallel(mapping), runnable3])\\n```\\n\\n:::caution\\nYou have to be careful because the `mapping` dictionary is not a `RunnableParallel` object, it is just a dictionary. This means that the following code will raise an `AttributeError`:\\n\\n```python\\nmapping.invoke(some_input)\\n```\\n:::\\n\\n#### Function to RunnableLambda\\n\\nInside an LCEL expression, a function is automatically converted to a `RunnableLambda`.\\n\\n```\\ndef some_func(x):\\n    return x\\n\\nchain = some_func | runnable1\\n```\\n\\nIt gets automatically converted to the following:\\n\\n```python\\nchain = RunnableSequence([RunnableLambda(some_func), runnable1])\\n```\\n\\n:::caution\\nYou have to be careful because the lambda function is not a `RunnableLambda` object, it is just a function. This means that the following code will raise an `AttributeError`:\\n\\n```python\\nlambda x: x + 1.invoke(some_input)\\n```\\n:::\\n\\n## Legacy chains\\n\\nLCEL aims to provide consistency around behavior and customization over legacy subclassed chains such as `LLMChain` and\\n`ConversationalRetrievalChain`. Many of these legacy chains hide important details like prompts, and as a wider variety\\nof viable models emerge, customization has become more and more important.\\n\\nIf you are currently using one of these legacy chains, please see [this guide for guidance on how to migrate](/docs/versions/migrating_chains).\\n\\nFor guides on how to do specific tasks with LCEL, check out [the relevant how-to guides](/docs/how_to/#langchain-expression-language-lcel).\\n': 0.03252247488101534, '# Conceptual guide\\n\\nThis guide provides explanations of the key concepts behind the LangChain framework and AI applications more broadly.\\n\\nWe recommend that you go through at least one of the [Tutorials](/docs/tutorials) before diving into the conceptual guide. This will provide practical context that will make it easier to understand the concepts discussed here.\\n\\nThe conceptual guide does not cover step-by-step instructions or specific implementation examples — those are found in the [How-to guides](/docs/how_to/) and [Tutorials](/docs/tutorials). For detailed reference material, please see the [API reference](https://python.langchain.com/api_reference/).\\n\\n## High level\\n\\n- **[Why LangChain?](/docs/concepts/why_langchain)**: Overview of the value that LangChain provides.\\n- **[Architecture](/docs/concepts/architecture)**: How packages are organized in the LangChain ecosystem.\\n\\n## Concepts\\n\\n- **[Chat models](/docs/concepts/chat_models)**: LLMs exposed via a chat API that process sequences of messages as input and output a message.\\n- **[Messages](/docs/concepts/messages)**: The unit of communication in chat models, used to represent model input and output.\\n- **[Chat history](/docs/concepts/chat_history)**: A conversation represented as a sequence of messages, alternating between user messages and model responses.\\n- **[Tools](/docs/concepts/tools)**: A function with an associated schema defining the function\\'s name, description, and the arguments it accepts.\\n- **[Tool calling](/docs/concepts/tool_calling)**: A type of chat model API that accepts tool schemas, along with messages, as input and returns invocations of those tools as part of the output message.\\n- **[Structured output](/docs/concepts/structured_outputs)**: A technique to make a chat model respond in a structured format, such as JSON that matches a given schema.\\n- **[Memory](https://langchain-ai.github.io/langgraph/concepts/memory/)**: Information about a conversation that is persisted so that it can be used in future conversations.\\n- **[Multimodality](/docs/concepts/multimodality)**: The ability to work with data that comes in different forms, such as text, audio, images, and video.\\n- **[Runnable interface](/docs/concepts/runnables)**: The base abstraction that many LangChain components and the LangChain Expression Language are built on.\\n- **[Streaming](/docs/concepts/streaming)**: LangChain streaming APIs for surfacing results as they are generated.\\n- **[LangChain Expression Language (LCEL)](/docs/concepts/lcel)**: A syntax for orchestrating LangChain components. Most useful for simpler applications.\\n- **[Document loaders](/docs/concepts/document_loaders)**: Load a source as a list of documents.\\n- **[Retrieval](/docs/concepts/retrieval)**: Information retrieval systems can retrieve structured or unstructured data from a datasource in response to a query.\\n- **[Text splitters](/docs/concepts/text_splitters)**: Split long text into smaller chunks that can be individually indexed to enable granular retrieval.\\n- **[Embedding models](/docs/concepts/embedding_models)**: Models that represent data such as text or images in a vector space.\\n- **[Vector stores](/docs/concepts/vectorstores)**: Storage of and efficient search over vectors and associated metadata.\\n- **[Retriever](/docs/concepts/retrievers)**: A component that returns relevant documents from a knowledge base in response to a query.\\n- **[Retrieval Augmented Generation (RAG)](/docs/concepts/rag)**: A technique that enhances language models by combining them with external knowledge bases.\\n- **[Agents](/docs/concepts/agents)**: Use a [language model](/docs/concepts/chat_models) to choose a sequence of actions to take. Agents can interact with external resources via [tool](/docs/concepts/tools).\\n- **[Prompt templates](/docs/concepts/prompt_templates)**: Component for factoring out the static parts of a model \"prompt\" (usually a sequence of messages). Useful for serializing, versioning, and reusing these static parts.\\n- **[Output parsers](/docs/concepts/output_parsers)**: Responsible for taking the output of a model and transforming it into a more suitable format for downstream tasks. Output parsers were primarily useful prior to the general availability of [tool calling](/docs/concepts/tool_calling) and [structured outputs](/docs/concepts/structured_outputs).\\n- **[Few-shot prompting](/docs/concepts/few_shot_prompting)**: A technique for improving model performance by providing a few examples of the task to perform in the prompt.\\n- **[Example selectors](/docs/concepts/example_selectors)**: Used to select the most relevant examples from a dataset based on a given input. Example selectors are used in few-shot prompting to select examples for a prompt.\\n- **[Async programming](/docs/concepts/async)**: The basics that one should know to use LangChain in an asynchronous context.\\n- **[Callbacks](/docs/concepts/callbacks)**: Callbacks enable the execution of custom auxiliary code in built-in components. Callbacks are used to stream outputs from LLMs in LangChain, trace the intermediate steps of an application, and more.\\n- **[Tracing](/docs/concepts/tracing)**: The process of recording the steps that an application takes to go from input to output. Tracing is essential for debugging and diagnosing issues in complex applications.\\n- **[Evaluation](/docs/concepts/evaluation)**: The process of assessing the performance and effectiveness of AI applications. This involves testing the model\\'s responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose. This process is vital for building reliable applications.\\n- **[Testing](/docs/concepts/testing)**: The process of verifying that a component of an integration or application works as expected. Testing is essential for ensuring that the application behaves correctly and that changes to the codebase do not introduce new bugs.\\n\\n## Glossary\\n\\n- **[AIMessageChunk](/docs/concepts/messages#aimessagechunk)**: A partial response from an AI message. Used when streaming responses from a chat model.\\n- **[AIMessage](/docs/concepts/messages#aimessage)**: Represents a complete response from an AI model.\\n- **[astream_events](/docs/concepts/chat_models#key-methods)**: Stream granular information from [LCEL](/docs/concepts/lcel) chains.\\n- **[BaseTool](/docs/concepts/tools/#tool-interface)**: The base class for all tools in LangChain.\\n- **[batch](/docs/concepts/runnables)**: Use to execute a runnable with batch inputs.\\n- **[bind_tools](/docs/concepts/tool_calling/#tool-binding)**: Allows models to interact with tools.\\n- **[Caching](/docs/concepts/chat_models#caching)**: Storing results to avoid redundant calls to a chat model.\\n- **[Chat models](/docs/concepts/multimodality/#multimodality-in-chat-models)**: Chat models that handle multiple data modalities.\\n- **[Configurable runnables](/docs/concepts/runnables/#configurable-runnables)**: Creating configurable Runnables.\\n- **[Context window](/docs/concepts/chat_models#context-window)**: The maximum size of input a chat model can process.\\n- **[Conversation patterns](/docs/concepts/chat_history#conversation-patterns)**: Common patterns in chat interactions.\\n- **[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)**: LangChain\\'s representation of a document.\\n- **[Embedding models](/docs/concepts/multimodality/#multimodality-in-embedding-models)**: Models that generate vector embeddings for various data types.\\n- **[HumanMessage](/docs/concepts/messages#humanmessage)**: Represents a message from a human user.\\n- **[InjectedState](/docs/concepts/tools#injectedstate)**: A state injected into a tool function.\\n- **[InjectedStore](/docs/concepts/tools#injectedstore)**: A store that can be injected into a tool for data persistence.\\n- **[InjectedToolArg](/docs/concepts/tools#injectedtoolarg)**: Mechanism to inject arguments into tool functions.\\n- **[input and output types](/docs/concepts/runnables#input-and-output-types)**: Types used for input and output in Runnables.\\n- **[Integration packages](/docs/concepts/architecture/#integration-packages)**: Third-party packages that integrate with LangChain.\\n- **[Integration tests](/docs/concepts/testing#integration-tests)**: Tests that verify the correctness of the interaction between components, usually run with access to the underlying API that powers an integration.\\n- **[invoke](/docs/concepts/runnables)**: A standard method to invoke a Runnable.\\n- **[JSON mode](/docs/concepts/structured_outputs#json-mode)**: Returning responses in JSON format.\\n- **[langchain-community](/docs/concepts/architecture#langchain-community)**: Community-driven components for LangChain.\\n- **[langchain-core](/docs/concepts/architecture#langchain-core)**: Core langchain package. Includes base interfaces and in-memory implementations.\\n- **[langchain](/docs/concepts/architecture#langchain)**: A package for higher level components (e.g., some pre-built chains).\\n- **[langgraph](/docs/concepts/architecture#langgraph)**: Powerful orchestration layer for LangChain. Use to build complex pipelines and workflows.\\n- **[langserve](/docs/concepts/architecture#langserve)**: Used to deploy LangChain Runnables as REST endpoints. Uses FastAPI. Works primarily for LangChain Runnables, does not currently integrate with LangGraph.\\n- **[LLMs (legacy)](/docs/concepts/text_llms)**: Older language models that take a string as input and return a string as output.\\n- **[Managing chat history](/docs/concepts/chat_history#managing-chat-history)**: Techniques to maintain and manage the chat history.\\n- **[OpenAI format](/docs/concepts/messages#openai-format)**: OpenAI\\'s message format for chat models.\\n- **[Propagation of RunnableConfig](/docs/concepts/runnables/#propagation-of-runnableconfig)**: Propagating configuration through Runnables. Read if working with python 3.9, 3.10 and async.\\n- **[rate-limiting](/docs/concepts/chat_models#rate-limiting)**: Client side rate limiting for chat models.\\n- **[RemoveMessage](/docs/concepts/messages/#removemessage)**: An abstraction used to remove a message from chat history, used primarily in LangGraph.\\n- **[role](/docs/concepts/messages#role)**: Represents the role (e.g., user, assistant) of a chat message.\\n- **[RunnableConfig](/docs/concepts/runnables/#runnableconfig)**: Use to pass run time information to Runnables (e.g., `run_name`, `run_id`, `tags`, `metadata`, `max_concurrency`, `recursion_limit`, `configurable`).\\n- **[Standard parameters for chat models](/docs/concepts/chat_models#standard-parameters)**: Parameters such as API key, `temperature`, and `max_tokens`.\\n- **[Standard tests](/docs/concepts/testing#standard-tests)**: A defined set of unit and integration tests that all integrations must pass.\\n- **[stream](/docs/concepts/streaming)**: Use to stream output from a Runnable or a graph.\\n- **[Tokenization](/docs/concepts/tokens)**: The process of converting data into tokens and vice versa.\\n- **[Tokens](/docs/concepts/tokens)**: The basic unit that a language model reads, processes, and generates under the hood.\\n- **[Tool artifacts](/docs/concepts/tools#tool-artifacts)**: Add artifacts to the output of a tool that will not be sent to the model, but will be available for downstream processing.\\n- **[Tool binding](/docs/concepts/tool_calling#tool-binding)**: Binding tools to models.\\n- **[@tool](/docs/concepts/tools/#create-tools-using-the-tool-decorator)**: Decorator for creating tools in LangChain.\\n- **[Toolkits](/docs/concepts/tools#toolkits)**: A collection of tools that can be used together.\\n- **[ToolMessage](/docs/concepts/messages#toolmessage)**: Represents a message that contains the results of a tool execution.\\n- **[Unit tests](/docs/concepts/testing#unit-tests)**: Tests that verify the correctness of individual components, run in isolation without access to the Internet.\\n- **[Vector stores](/docs/concepts/vectorstores)**: Datastores specialized for storing and efficiently searching vector embeddings.\\n- **[with_structured_output](/docs/concepts/structured_outputs/#structured-output-method)**: A helper method for chat models that natively support [tool calling](/docs/concepts/tool_calling) to get structured output matching a given schema specified via Pydantic, JSON schema or a function.\\n- **[with_types](/docs/concepts/runnables#with_types)**: Method to overwrite the input and output types of a runnable. Useful when working with complex LCEL chains and deploying with LangServe.\\n': 0.015873015873015872}\n",
      "rank:2\n",
      "content_score_mapping:{'# Why LangChain?\\n\\nThe goal of `langchain` the Python package and LangChain the company is to make it as easy as possible for developers to build applications that reason.\\nWhile LangChain originally started as a single open source package, it has evolved into a company and a whole ecosystem.\\nThis page will talk about the LangChain ecosystem as a whole.\\nMost of the components within the LangChain ecosystem can be used by themselves - so if you feel particularly drawn to certain components but not others, that is totally fine! Pick and choose whichever components you like best for your own use case!\\n\\n## Features\\n\\nThere are several primary needs that LangChain aims to address:\\n\\n1. **Standardized component interfaces:** The growing number of [models](/docs/integrations/chat/) and [related components](/docs/integrations/vectorstores/) for AI applications has resulted in a wide variety of different APIs that developers need to learn and use.\\nThis diversity can make it challenging for developers to switch between providers or combine components when building applications.\\nLangChain exposes a standard interface for key components, making it easy to switch between providers.\\n\\n2. **Orchestration:** As applications become more complex, combining multiple components and models, there\\'s [a growing need to efficiently connect these elements into control flows](https://lilianweng.github.io/posts/2023-06-23-agent/) that can [accomplish diverse tasks](https://www.sequoiacap.com/article/generative-ais-act-o1/).\\n[Orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)) is crucial for building such applications.\\n\\n3. **Observability and evaluation:** As applications become more complex, it becomes increasingly difficult to understand what is happening within them.\\nFurthermore, the pace of development can become rate-limited by the [paradox of choice](https://en.wikipedia.org/wiki/Paradox_of_choice).\\nFor example, developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\n[Observability](https://en.wikipedia.org/wiki/Observability) and evaluations can help developers monitor their applications and rapidly answer these types of questions with confidence.\\n\\n\\n## Standardized component interfaces\\n\\nLangChain provides common interfaces for components that are central to many AI applications.\\nAs an example, all [chat models](/docs/concepts/chat_models/) implement the [BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) interface.\\nThis provides a standard way to interact with chat models, supporting important but often provider-specific features like [tool calling](/docs/concepts/tool_calling/) and [structured outputs](/docs/concepts/structured_outputs/).\\n\\n\\n### Example: chat models \\n\\nMany [model providers](/docs/concepts/chat_models/) support [tool calling](/docs/concepts/tool_calling/), a critical feature for many applications (e.g., [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/)), that allows a developer to request model responses that match a particular schema.\\nThe APIs for each provider differ. \\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to bind [tools](/docs/concepts/tools) to a model in order to support [tool calling](/docs/concepts/tool_calling/):\\n\\n```python\\n# Tool creation\\ntools = [my_tool]\\n# Tool binding\\nmodel_with_tools = model.bind_tools(tools)\\n```\\n\\nSimilarly, getting models to produce [structured outputs](/docs/concepts/structured_outputs/) is an extremely common use case. \\nProviders support different approaches for this, including [JSON mode or tool calling](https://platform.openai.com/docs/guides/structured-outputs), with different APIs.\\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to produce structured outputs using the `with_structured_output()` method:\\n\\n```python\\n# Define schema\\nschema = ...\\n# Bind schema to model\\nmodel_with_structure = model.with_structured_output(schema)\\n```\\n\\n### Example: retrievers\\n\\nIn the context of [RAG](/docs/concepts/rag/) and LLM application components, LangChain\\'s [retriever](/docs/concepts/retrievers/) interface provides a standard way to connect to many different types of data services or databases (e.g., [vector stores](/docs/concepts/vectorstores) or databases).\\nThe underlying implementation of the retriever depends on the type of data store or database you are connecting to, but all retrievers implement the [runnable interface](/docs/concepts/runnables/), meaning they can be invoked in a common manner.\\n\\n```python\\ndocuments = my_retriever.invoke(\"What is the meaning of life?\")\\n```\\n\\n## Orchestration \\n\\nWhile standardization for individual components is useful, we\\'ve increasingly seen that developers want to *combine* components into more complex applications. \\nThis motivates the need for [orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)).\\nThere are several common characteristics of LLM applications that this orchestration layer should support:\\n\\n* **Complex control flow:** The application requires complex patterns such as cycles (e.g., a loop that reiterates until a condition is met).\\n* **[Persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/):** The application needs to maintain [short-term and / or long-term memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n* **[Human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/):** The application needs human interaction, e.g., pausing, reviewing, editing, approving certain steps.\\n\\nThe recommended way to orchestrate components for complex applications is [LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/).\\nLangGraph is a library that gives developers a high degree of control by expressing the flow of the application as a set of nodes and edges.\\nLangGraph comes with built-in support for [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), [memory](https://langchain-ai.github.io/langgraph/concepts/memory/), and other features.\\nIt\\'s particularly well suited for building [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/) or [multi-agent](https://langchain-ai.github.io/langgraph/concepts/multi_agent/) applications. \\nImportantly, individual LangChain components can be used as LangGraph nodes, but you can also use LangGraph **without** using LangChain components.\\n\\n:::info[Further reading]\\n\\nHave a look at our free course, [Introduction to LangGraph](https://academy.langchain.com/courses/intro-to-langgraph), to learn more about how to use LangGraph to build complex applications.\\n\\n:::\\n\\n## Observability and evaluation\\n\\nThe pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. \\nDevelopers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\nHigh quality tracing and evaluations can help you rapidly answer these types of questions with confidence.\\n[LangSmith](https://docs.smith.langchain.com/) is our platform that supports observability and evaluation for AI applications.\\nSee our conceptual guides on [evaluations](https://docs.smith.langchain.com/concepts/evaluation) and [tracing](https://docs.smith.langchain.com/concepts/tracing) for more details.\\n\\n:::info[Further reading]\\n\\nSee our video playlist on [LangSmith tracing and evaluations](https://youtube.com/playlist?list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S&feature=shared) for more details.\\n\\n:::\\n\\n## Conclusion\\n\\nLangChain offers standard interfaces for components that are central to many AI applications, which offers a few specific advantages:\\n- **Ease of swapping providers:** It allows you to swap out different component providers without having to change the underlying code.\\n- **Advanced features:** It provides common methods for more advanced features, such as [streaming](/docs/concepts/streaming) and [tool calling](/docs/concepts/tool_calling/).\\n\\n[LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/) makes it possible to orchestrate complex applications (e.g., [agents](/docs/concepts/agents/)) and provide features like including [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), or [memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n\\n[LangSmith](https://docs.smith.langchain.com/) makes it possible to iterate with confidence on your applications, by providing LLM-specific observability and framework for testing and evaluating your application.\\n': 0.03333333333333333, '---\\nsidebar_position: 0\\nsidebar_class_name: hidden\\n---\\n\\n# Introduction\\n\\n**LangChain** is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:\\n- **Development**: Build your applications using LangChain\\'s open-source [components](/docs/concepts) and [third-party integrations](/docs/integrations/providers/).\\nUse [LangGraph](/docs/concepts/architecture/#langgraph) to build stateful agents with first-class streaming and human-in-the-loop support.\\n- **Productionization**: Use [LangSmith](https://docs.smith.langchain.com/) to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\n- **Deployment**: Turn your LangGraph applications into production-ready APIs and Assistants with [LangGraph Platform](https://langchain-ai.github.io/langgraph/cloud/).\\n\\nimport ThemedImage from \\'@theme/ThemedImage\\';\\nimport useBaseUrl from \\'@docusaurus/useBaseUrl\\';\\n\\n<ThemedImage\\n  alt=\"Diagram outlining the hierarchical organization of the LangChain framework, displaying the interconnected parts across multiple layers.\"\\n  sources={{\\n    light: useBaseUrl(\\'/svg/langchain_stack_112024.svg\\'),\\n    dark: useBaseUrl(\\'/svg/langchain_stack_112024_dark.svg\\'),\\n  }}\\n  style={{ width: \"100%\" }}\\n  title=\"LangChain Framework Overview\"\\n/>\\n\\nLangChain implements a standard interface for large language models and related\\ntechnologies, such as embedding models and vector stores, and integrates with\\nhundreds of providers. See the [integrations](/docs/integrations/providers/) page for\\nmore.\\n\\nimport ChatModelTabs from \"@theme/ChatModelTabs\";\\n\\n<ChatModelTabs/>\\n\\n```python\\nmodel.invoke(\"Hello, world!\")\\n```\\n\\n:::note\\n\\nThese docs focus on the Python LangChain library. [Head here](https://js.langchain.com) for docs on the JavaScript LangChain library.\\n\\n:::\\n\\n## Architecture\\n\\nThe LangChain framework consists of multiple open-source libraries. Read more in the\\n[Architecture](/docs/concepts/architecture/) page.\\n\\n- **`langchain-core`**: Base abstractions for chat models and other components.\\n- **Integration packages** (e.g. `langchain-openai`, `langchain-anthropic`, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\\n- **`langchain`**: Chains, agents, and retrieval strategies that make up an application\\'s cognitive architecture.\\n- **`langchain-community`**: Third-party integrations that are community maintained.\\n- **`langgraph`**: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See [LangGraph documentation](https://langchain-ai.github.io/langgraph/).\\n\\n## Guides\\n\\n### [Tutorials](/docs/tutorials)\\n\\nIf you\\'re looking to build something specific or are more of a hands-on learner, check out our [tutorials section](/docs/tutorials).\\nThis is the best place to get started.\\n\\nThese are the best ones to get started with:\\n\\n- [Build a Simple LLM Application](/docs/tutorials/llm_chain)\\n- [Build a Chatbot](/docs/tutorials/chatbot)\\n- [Build an Agent](/docs/tutorials/agents)\\n- [Introduction to LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/)\\n\\nExplore the full list of LangChain tutorials [here](/docs/tutorials), and check out other [LangGraph tutorials here](https://langchain-ai.github.io/langgraph/tutorials/). To learn more about LangGraph, check out our first LangChain Academy course, *Introduction to LangGraph*, available [here](https://academy.langchain.com/courses/intro-to-langgraph).\\n\\n\\n### [How-to guides](/docs/how_to)\\n\\n[Here](/docs/how_to) you’ll find short answers to “How do I….?” types of questions.\\nThese how-to guides don’t cover topics in depth – you’ll find that material in the [Tutorials](/docs/tutorials) and the [API Reference](https://python.langchain.com/api_reference/).\\nHowever, these guides will help you quickly accomplish common tasks using [chat models](/docs/how_to/#chat-models),\\n[vector stores](/docs/how_to/#vector-stores), and other common LangChain components.\\n\\nCheck out [LangGraph-specific how-tos here](https://langchain-ai.github.io/langgraph/how-tos/).\\n\\n### [Conceptual guide](/docs/concepts)\\n\\nIntroductions to all the key parts of LangChain you’ll need to know! [Here](/docs/concepts) you\\'ll find high level explanations of all LangChain concepts.\\n\\nFor a deeper dive into LangGraph concepts, check out [this page](https://langchain-ai.github.io/langgraph/concepts/).\\n\\n### [Integrations](integrations/providers/index.mdx)\\n\\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\\nIf you\\'re looking to get up and running quickly with [chat models](/docs/integrations/chat/), [vector stores](/docs/integrations/vectorstores/),\\nor other LangChain components from a specific provider, check out our growing list of [integrations](/docs/integrations/providers/).\\n\\n\\n### [API reference](https://python.langchain.com/api_reference/)\\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\\n\\n## Ecosystem\\n\\n### [🦜🛠️ LangSmith](https://docs.smith.langchain.com)\\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\\n\\n### [🦜🕸️ LangGraph](https://langchain-ai.github.io/langgraph)\\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by Linkedin, Uber, Klarna, GitLab, and many more.\\n\\n## Additional resources\\n\\n### [Versions](/docs/versions/v0_3/)\\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\\n\\n### [Security](/docs/security)\\nRead up on [security](/docs/security) best practices to make sure you\\'re developing safely with LangChain.\\n\\n### [Contributing](contributing/index.mdx)\\nCheck out the developer\\'s guide for guidelines on contributing and help getting your dev environment set up.\\n': 0.03252247488101534, '# LangChain Expression Language (LCEL)\\n\\n:::info Prerequisites\\n* [Runnable Interface](/docs/concepts/runnables)\\n:::\\n\\nThe **L**ang**C**hain **E**xpression **L**anguage (LCEL) takes a [declarative](https://en.wikipedia.org/wiki/Declarative_programming) approach to building new [Runnables](/docs/concepts/runnables) from existing Runnables.\\n\\nThis means that you describe what *should* happen, rather than *how* it should happen, allowing LangChain to optimize the run-time execution of the chains.\\n\\nWe often refer to a `Runnable` created using LCEL as a \"chain\". It\\'s important to remember that a \"chain\" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).\\n\\n:::note\\n* The [LCEL cheatsheet](/docs/how_to/lcel_cheatsheet/) shows common patterns that involve the Runnable interface and LCEL expressions.\\n* Please see the following list of [how-to guides](/docs/how_to/#langchain-expression-language-lcel) that cover common tasks with LCEL.\\n* A list of built-in `Runnables` can be found in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html). Many of these Runnables are useful when composing custom \"chains\" in LangChain using LCEL.\\n:::\\n\\n## Benefits of LCEL\\n\\nLangChain optimizes the run-time execution of chains built with LCEL in a number of ways:\\n\\n- **Optimized parallel execution**: Run Runnables in parallel using [RunnableParallel](#runnableparallel) or run multiple inputs through a given chain in parallel using the [Runnable Batch API](/docs/concepts/runnables/#optimized-parallel-execution-batch). Parallel execution can significantly reduce the latency as processing can be done in parallel instead of sequentially.\\n- **Guaranteed Async support**: Any chain built with LCEL can be run asynchronously using the [Runnable Async API](/docs/concepts/runnables/#asynchronous-support). This can be useful when running chains in a server environment where you want to handle large number of requests concurrently.\\n- **Simplify streaming**: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(time elapsed until the first chunk of output from a [chat model](/docs/concepts/chat_models) or [llm](/docs/concepts/text_llms) comes out).\\n\\nOther benefits include:\\n\\n- [**Seamless LangSmith tracing**](https://docs.smith.langchain.com)\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, **all** steps are automatically logged to [LangSmith](https://docs.smith.langchain.com/) for maximum observability and debuggability.\\n- **Standard API**: Because all chains are built using the Runnable interface, they can be used in the same way as any other Runnable.\\n- [**Deployable with LangServe**](/docs/concepts/architecture#langserve): Chains built with LCEL can be deployed using for production use.\\n\\n## Should I use LCEL?\\n\\nLCEL is an [orchestration solution](https://en.wikipedia.org/wiki/Orchestration_(computing)) -- it allows LangChain to handle run-time execution of chains in an optimized way.\\n\\nWhile we have seen users run chains with hundreds of steps in production, we generally recommend using LCEL for simpler orchestration tasks. When the application requires complex state management, branching, cycles or multiple agents, we recommend that users take advantage of [LangGraph](/docs/concepts/architecture#langgraph).\\n\\nIn LangGraph, users define graphs that specify the application\\'s flow. This allows users to keep using LCEL within individual nodes when LCEL is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.\\n\\nHere are some guidelines:\\n\\n* If you are making a single LLM call, you don\\'t need LCEL; instead call the underlying [chat model](/docs/concepts/chat_models) directly.\\n* If you have a simple chain (e.g., prompt + llm + parser, simple retrieval set up etc.), LCEL is a reasonable fit, if you\\'re taking advantage of the LCEL benefits.\\n* If you\\'re building a complex chain (e.g., with branching, cycles, multiple agents, etc.) use [LangGraph](/docs/concepts/architecture#langgraph) instead. Remember that you can always use LCEL within individual nodes in LangGraph.\\n\\n## Composition Primitives\\n\\n`LCEL` chains are built by composing existing `Runnables` together. The two main composition primitives are [RunnableSequence](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableSequence.html#langchain_core.runnables.base.RunnableSequence) and [RunnableParallel](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableParallel.html#langchain_core.runnables.base.RunnableParallel).\\n\\nMany other composition primitives (e.g., [RunnableAssign](\\nhttps://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnableAssign.html#langchain_core.runnables.passthrough.RunnableAssign\\n)) can be thought of as variations of these two primitives.\\n\\n:::note\\nYou can find a list of all composition primitives in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html).\\n:::\\n\\n### RunnableSequence\\n\\n`RunnableSequence` is a composition primitive that allows you \"chain\" multiple runnables sequentially, with the output of one runnable serving as the input to the next.\\n\\n```python\\nfrom langchain_core.runnables import RunnableSequence\\nchain = RunnableSequence([runnable1, runnable2])\\n```\\n\\nInvoking the `chain` with some input:\\n\\n```python\\nfinal_output = chain.invoke(some_input)\\n```\\n\\ncorresponds to the following:\\n\\n```python\\noutput1 = runnable1.invoke(some_input)\\nfinal_output = runnable2.invoke(output1)\\n```\\n\\n:::note\\n`runnable1` and `runnable2` are placeholders for any `Runnable` that you want to chain together.\\n:::\\n\\n### RunnableParallel\\n\\n`RunnableParallel` is a composition primitive that allows you to run multiple runnables concurrently, with the same input provided to each.\\n\\n```python\\nfrom langchain_core.runnables import RunnableParallel\\nchain = RunnableParallel({\\n    \"key1\": runnable1,\\n    \"key2\": runnable2,\\n})\\n```\\n\\nInvoking the `chain` with some input:\\n\\n```python\\nfinal_output = chain.invoke(some_input)\\n```\\n\\nWill yield a `final_output` dictionary with the same keys as the input dictionary, but with the values replaced by the output of the corresponding runnable.\\n\\n```python\\n{\\n    \"key1\": runnable1.invoke(some_input),\\n    \"key2\": runnable2.invoke(some_input),\\n}\\n```\\n\\nRecall, that the runnables are executed in parallel, so while the result is the same as\\ndictionary comprehension shown above, the execution time is much faster.\\n\\n:::note\\n`RunnableParallel`supports both synchronous and asynchronous execution (as all `Runnables` do).\\n\\n* For synchronous execution, `RunnableParallel` uses a [ThreadPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor) to run the runnables concurrently.\\n* For asynchronous execution, `RunnableParallel` uses [asyncio.gather](https://docs.python.org/3/library/asyncio.html#asyncio.gather) to run the runnables concurrently.\\n:::\\n\\n## Composition Syntax\\n\\nThe usage of `RunnableSequence` and `RunnableParallel` is so common that we created a shorthand syntax for using them. This helps\\nto make the code more readable and concise.\\n\\n### The `|` operator\\n\\nWe have [overloaded](https://docs.python.org/3/reference/datamodel.html#special-method-names) the `|` operator to create a `RunnableSequence` from two `Runnables`.\\n\\n```python\\nchain = runnable1 | runnable2\\n```\\n\\nis Equivalent to:\\n\\n```python\\nchain = RunnableSequence([runnable1, runnable2])\\n```\\n\\n### The `.pipe` method\\n\\nIf you have moral qualms with operator overloading, you can use the `.pipe` method instead. This is equivalent to the `|` operator.\\n\\n```python\\nchain = runnable1.pipe(runnable2)\\n```\\n\\n### Coercion\\n\\nLCEL applies automatic type coercion to make it easier to compose chains.\\n\\nIf you do not understand the type coercion, you can always use the `RunnableSequence` and `RunnableParallel` classes directly.\\n\\nThis will make the code more verbose, but it will also make it more explicit.\\n\\n#### Dictionary to RunnableParallel\\n\\nInside an LCEL expression, a dictionary is automatically converted to a `RunnableParallel`.\\n\\nFor example, the following code:\\n\\n```python\\nmapping = {\\n    \"key1\": runnable1,\\n    \"key2\": runnable2,\\n}\\n\\nchain = mapping | runnable3\\n```\\n\\nIt gets automatically converted to the following:\\n\\n```python\\nchain = RunnableSequence([RunnableParallel(mapping), runnable3])\\n```\\n\\n:::caution\\nYou have to be careful because the `mapping` dictionary is not a `RunnableParallel` object, it is just a dictionary. This means that the following code will raise an `AttributeError`:\\n\\n```python\\nmapping.invoke(some_input)\\n```\\n:::\\n\\n#### Function to RunnableLambda\\n\\nInside an LCEL expression, a function is automatically converted to a `RunnableLambda`.\\n\\n```\\ndef some_func(x):\\n    return x\\n\\nchain = some_func | runnable1\\n```\\n\\nIt gets automatically converted to the following:\\n\\n```python\\nchain = RunnableSequence([RunnableLambda(some_func), runnable1])\\n```\\n\\n:::caution\\nYou have to be careful because the lambda function is not a `RunnableLambda` object, it is just a function. This means that the following code will raise an `AttributeError`:\\n\\n```python\\nlambda x: x + 1.invoke(some_input)\\n```\\n:::\\n\\n## Legacy chains\\n\\nLCEL aims to provide consistency around behavior and customization over legacy subclassed chains such as `LLMChain` and\\n`ConversationalRetrievalChain`. Many of these legacy chains hide important details like prompts, and as a wider variety\\nof viable models emerge, customization has become more and more important.\\n\\nIf you are currently using one of these legacy chains, please see [this guide for guidance on how to migrate](/docs/versions/migrating_chains).\\n\\nFor guides on how to do specific tasks with LCEL, check out [the relevant how-to guides](/docs/how_to/#langchain-expression-language-lcel).\\n': 0.03252247488101534, '# Conceptual guide\\n\\nThis guide provides explanations of the key concepts behind the LangChain framework and AI applications more broadly.\\n\\nWe recommend that you go through at least one of the [Tutorials](/docs/tutorials) before diving into the conceptual guide. This will provide practical context that will make it easier to understand the concepts discussed here.\\n\\nThe conceptual guide does not cover step-by-step instructions or specific implementation examples — those are found in the [How-to guides](/docs/how_to/) and [Tutorials](/docs/tutorials). For detailed reference material, please see the [API reference](https://python.langchain.com/api_reference/).\\n\\n## High level\\n\\n- **[Why LangChain?](/docs/concepts/why_langchain)**: Overview of the value that LangChain provides.\\n- **[Architecture](/docs/concepts/architecture)**: How packages are organized in the LangChain ecosystem.\\n\\n## Concepts\\n\\n- **[Chat models](/docs/concepts/chat_models)**: LLMs exposed via a chat API that process sequences of messages as input and output a message.\\n- **[Messages](/docs/concepts/messages)**: The unit of communication in chat models, used to represent model input and output.\\n- **[Chat history](/docs/concepts/chat_history)**: A conversation represented as a sequence of messages, alternating between user messages and model responses.\\n- **[Tools](/docs/concepts/tools)**: A function with an associated schema defining the function\\'s name, description, and the arguments it accepts.\\n- **[Tool calling](/docs/concepts/tool_calling)**: A type of chat model API that accepts tool schemas, along with messages, as input and returns invocations of those tools as part of the output message.\\n- **[Structured output](/docs/concepts/structured_outputs)**: A technique to make a chat model respond in a structured format, such as JSON that matches a given schema.\\n- **[Memory](https://langchain-ai.github.io/langgraph/concepts/memory/)**: Information about a conversation that is persisted so that it can be used in future conversations.\\n- **[Multimodality](/docs/concepts/multimodality)**: The ability to work with data that comes in different forms, such as text, audio, images, and video.\\n- **[Runnable interface](/docs/concepts/runnables)**: The base abstraction that many LangChain components and the LangChain Expression Language are built on.\\n- **[Streaming](/docs/concepts/streaming)**: LangChain streaming APIs for surfacing results as they are generated.\\n- **[LangChain Expression Language (LCEL)](/docs/concepts/lcel)**: A syntax for orchestrating LangChain components. Most useful for simpler applications.\\n- **[Document loaders](/docs/concepts/document_loaders)**: Load a source as a list of documents.\\n- **[Retrieval](/docs/concepts/retrieval)**: Information retrieval systems can retrieve structured or unstructured data from a datasource in response to a query.\\n- **[Text splitters](/docs/concepts/text_splitters)**: Split long text into smaller chunks that can be individually indexed to enable granular retrieval.\\n- **[Embedding models](/docs/concepts/embedding_models)**: Models that represent data such as text or images in a vector space.\\n- **[Vector stores](/docs/concepts/vectorstores)**: Storage of and efficient search over vectors and associated metadata.\\n- **[Retriever](/docs/concepts/retrievers)**: A component that returns relevant documents from a knowledge base in response to a query.\\n- **[Retrieval Augmented Generation (RAG)](/docs/concepts/rag)**: A technique that enhances language models by combining them with external knowledge bases.\\n- **[Agents](/docs/concepts/agents)**: Use a [language model](/docs/concepts/chat_models) to choose a sequence of actions to take. Agents can interact with external resources via [tool](/docs/concepts/tools).\\n- **[Prompt templates](/docs/concepts/prompt_templates)**: Component for factoring out the static parts of a model \"prompt\" (usually a sequence of messages). Useful for serializing, versioning, and reusing these static parts.\\n- **[Output parsers](/docs/concepts/output_parsers)**: Responsible for taking the output of a model and transforming it into a more suitable format for downstream tasks. Output parsers were primarily useful prior to the general availability of [tool calling](/docs/concepts/tool_calling) and [structured outputs](/docs/concepts/structured_outputs).\\n- **[Few-shot prompting](/docs/concepts/few_shot_prompting)**: A technique for improving model performance by providing a few examples of the task to perform in the prompt.\\n- **[Example selectors](/docs/concepts/example_selectors)**: Used to select the most relevant examples from a dataset based on a given input. Example selectors are used in few-shot prompting to select examples for a prompt.\\n- **[Async programming](/docs/concepts/async)**: The basics that one should know to use LangChain in an asynchronous context.\\n- **[Callbacks](/docs/concepts/callbacks)**: Callbacks enable the execution of custom auxiliary code in built-in components. Callbacks are used to stream outputs from LLMs in LangChain, trace the intermediate steps of an application, and more.\\n- **[Tracing](/docs/concepts/tracing)**: The process of recording the steps that an application takes to go from input to output. Tracing is essential for debugging and diagnosing issues in complex applications.\\n- **[Evaluation](/docs/concepts/evaluation)**: The process of assessing the performance and effectiveness of AI applications. This involves testing the model\\'s responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose. This process is vital for building reliable applications.\\n- **[Testing](/docs/concepts/testing)**: The process of verifying that a component of an integration or application works as expected. Testing is essential for ensuring that the application behaves correctly and that changes to the codebase do not introduce new bugs.\\n\\n## Glossary\\n\\n- **[AIMessageChunk](/docs/concepts/messages#aimessagechunk)**: A partial response from an AI message. Used when streaming responses from a chat model.\\n- **[AIMessage](/docs/concepts/messages#aimessage)**: Represents a complete response from an AI model.\\n- **[astream_events](/docs/concepts/chat_models#key-methods)**: Stream granular information from [LCEL](/docs/concepts/lcel) chains.\\n- **[BaseTool](/docs/concepts/tools/#tool-interface)**: The base class for all tools in LangChain.\\n- **[batch](/docs/concepts/runnables)**: Use to execute a runnable with batch inputs.\\n- **[bind_tools](/docs/concepts/tool_calling/#tool-binding)**: Allows models to interact with tools.\\n- **[Caching](/docs/concepts/chat_models#caching)**: Storing results to avoid redundant calls to a chat model.\\n- **[Chat models](/docs/concepts/multimodality/#multimodality-in-chat-models)**: Chat models that handle multiple data modalities.\\n- **[Configurable runnables](/docs/concepts/runnables/#configurable-runnables)**: Creating configurable Runnables.\\n- **[Context window](/docs/concepts/chat_models#context-window)**: The maximum size of input a chat model can process.\\n- **[Conversation patterns](/docs/concepts/chat_history#conversation-patterns)**: Common patterns in chat interactions.\\n- **[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)**: LangChain\\'s representation of a document.\\n- **[Embedding models](/docs/concepts/multimodality/#multimodality-in-embedding-models)**: Models that generate vector embeddings for various data types.\\n- **[HumanMessage](/docs/concepts/messages#humanmessage)**: Represents a message from a human user.\\n- **[InjectedState](/docs/concepts/tools#injectedstate)**: A state injected into a tool function.\\n- **[InjectedStore](/docs/concepts/tools#injectedstore)**: A store that can be injected into a tool for data persistence.\\n- **[InjectedToolArg](/docs/concepts/tools#injectedtoolarg)**: Mechanism to inject arguments into tool functions.\\n- **[input and output types](/docs/concepts/runnables#input-and-output-types)**: Types used for input and output in Runnables.\\n- **[Integration packages](/docs/concepts/architecture/#integration-packages)**: Third-party packages that integrate with LangChain.\\n- **[Integration tests](/docs/concepts/testing#integration-tests)**: Tests that verify the correctness of the interaction between components, usually run with access to the underlying API that powers an integration.\\n- **[invoke](/docs/concepts/runnables)**: A standard method to invoke a Runnable.\\n- **[JSON mode](/docs/concepts/structured_outputs#json-mode)**: Returning responses in JSON format.\\n- **[langchain-community](/docs/concepts/architecture#langchain-community)**: Community-driven components for LangChain.\\n- **[langchain-core](/docs/concepts/architecture#langchain-core)**: Core langchain package. Includes base interfaces and in-memory implementations.\\n- **[langchain](/docs/concepts/architecture#langchain)**: A package for higher level components (e.g., some pre-built chains).\\n- **[langgraph](/docs/concepts/architecture#langgraph)**: Powerful orchestration layer for LangChain. Use to build complex pipelines and workflows.\\n- **[langserve](/docs/concepts/architecture#langserve)**: Used to deploy LangChain Runnables as REST endpoints. Uses FastAPI. Works primarily for LangChain Runnables, does not currently integrate with LangGraph.\\n- **[LLMs (legacy)](/docs/concepts/text_llms)**: Older language models that take a string as input and return a string as output.\\n- **[Managing chat history](/docs/concepts/chat_history#managing-chat-history)**: Techniques to maintain and manage the chat history.\\n- **[OpenAI format](/docs/concepts/messages#openai-format)**: OpenAI\\'s message format for chat models.\\n- **[Propagation of RunnableConfig](/docs/concepts/runnables/#propagation-of-runnableconfig)**: Propagating configuration through Runnables. Read if working with python 3.9, 3.10 and async.\\n- **[rate-limiting](/docs/concepts/chat_models#rate-limiting)**: Client side rate limiting for chat models.\\n- **[RemoveMessage](/docs/concepts/messages/#removemessage)**: An abstraction used to remove a message from chat history, used primarily in LangGraph.\\n- **[role](/docs/concepts/messages#role)**: Represents the role (e.g., user, assistant) of a chat message.\\n- **[RunnableConfig](/docs/concepts/runnables/#runnableconfig)**: Use to pass run time information to Runnables (e.g., `run_name`, `run_id`, `tags`, `metadata`, `max_concurrency`, `recursion_limit`, `configurable`).\\n- **[Standard parameters for chat models](/docs/concepts/chat_models#standard-parameters)**: Parameters such as API key, `temperature`, and `max_tokens`.\\n- **[Standard tests](/docs/concepts/testing#standard-tests)**: A defined set of unit and integration tests that all integrations must pass.\\n- **[stream](/docs/concepts/streaming)**: Use to stream output from a Runnable or a graph.\\n- **[Tokenization](/docs/concepts/tokens)**: The process of converting data into tokens and vice versa.\\n- **[Tokens](/docs/concepts/tokens)**: The basic unit that a language model reads, processes, and generates under the hood.\\n- **[Tool artifacts](/docs/concepts/tools#tool-artifacts)**: Add artifacts to the output of a tool that will not be sent to the model, but will be available for downstream processing.\\n- **[Tool binding](/docs/concepts/tool_calling#tool-binding)**: Binding tools to models.\\n- **[@tool](/docs/concepts/tools/#create-tools-using-the-tool-decorator)**: Decorator for creating tools in LangChain.\\n- **[Toolkits](/docs/concepts/tools#toolkits)**: A collection of tools that can be used together.\\n- **[ToolMessage](/docs/concepts/messages#toolmessage)**: Represents a message that contains the results of a tool execution.\\n- **[Unit tests](/docs/concepts/testing#unit-tests)**: Tests that verify the correctness of individual components, run in isolation without access to the Internet.\\n- **[Vector stores](/docs/concepts/vectorstores)**: Datastores specialized for storing and efficiently searching vector embeddings.\\n- **[with_structured_output](/docs/concepts/structured_outputs/#structured-output-method)**: A helper method for chat models that natively support [tool calling](/docs/concepts/tool_calling) to get structured output matching a given schema specified via Pydantic, JSON schema or a function.\\n- **[with_types](/docs/concepts/runnables#with_types)**: Method to overwrite the input and output types of a runnable. Useful when working with complex LCEL chains and deploying with LangServe.\\n': 0.015873015873015872}\n",
      "rank:3\n",
      "content_score_mapping:{'# Why LangChain?\\n\\nThe goal of `langchain` the Python package and LangChain the company is to make it as easy as possible for developers to build applications that reason.\\nWhile LangChain originally started as a single open source package, it has evolved into a company and a whole ecosystem.\\nThis page will talk about the LangChain ecosystem as a whole.\\nMost of the components within the LangChain ecosystem can be used by themselves - so if you feel particularly drawn to certain components but not others, that is totally fine! Pick and choose whichever components you like best for your own use case!\\n\\n## Features\\n\\nThere are several primary needs that LangChain aims to address:\\n\\n1. **Standardized component interfaces:** The growing number of [models](/docs/integrations/chat/) and [related components](/docs/integrations/vectorstores/) for AI applications has resulted in a wide variety of different APIs that developers need to learn and use.\\nThis diversity can make it challenging for developers to switch between providers or combine components when building applications.\\nLangChain exposes a standard interface for key components, making it easy to switch between providers.\\n\\n2. **Orchestration:** As applications become more complex, combining multiple components and models, there\\'s [a growing need to efficiently connect these elements into control flows](https://lilianweng.github.io/posts/2023-06-23-agent/) that can [accomplish diverse tasks](https://www.sequoiacap.com/article/generative-ais-act-o1/).\\n[Orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)) is crucial for building such applications.\\n\\n3. **Observability and evaluation:** As applications become more complex, it becomes increasingly difficult to understand what is happening within them.\\nFurthermore, the pace of development can become rate-limited by the [paradox of choice](https://en.wikipedia.org/wiki/Paradox_of_choice).\\nFor example, developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\n[Observability](https://en.wikipedia.org/wiki/Observability) and evaluations can help developers monitor their applications and rapidly answer these types of questions with confidence.\\n\\n\\n## Standardized component interfaces\\n\\nLangChain provides common interfaces for components that are central to many AI applications.\\nAs an example, all [chat models](/docs/concepts/chat_models/) implement the [BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) interface.\\nThis provides a standard way to interact with chat models, supporting important but often provider-specific features like [tool calling](/docs/concepts/tool_calling/) and [structured outputs](/docs/concepts/structured_outputs/).\\n\\n\\n### Example: chat models \\n\\nMany [model providers](/docs/concepts/chat_models/) support [tool calling](/docs/concepts/tool_calling/), a critical feature for many applications (e.g., [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/)), that allows a developer to request model responses that match a particular schema.\\nThe APIs for each provider differ. \\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to bind [tools](/docs/concepts/tools) to a model in order to support [tool calling](/docs/concepts/tool_calling/):\\n\\n```python\\n# Tool creation\\ntools = [my_tool]\\n# Tool binding\\nmodel_with_tools = model.bind_tools(tools)\\n```\\n\\nSimilarly, getting models to produce [structured outputs](/docs/concepts/structured_outputs/) is an extremely common use case. \\nProviders support different approaches for this, including [JSON mode or tool calling](https://platform.openai.com/docs/guides/structured-outputs), with different APIs.\\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to produce structured outputs using the `with_structured_output()` method:\\n\\n```python\\n# Define schema\\nschema = ...\\n# Bind schema to model\\nmodel_with_structure = model.with_structured_output(schema)\\n```\\n\\n### Example: retrievers\\n\\nIn the context of [RAG](/docs/concepts/rag/) and LLM application components, LangChain\\'s [retriever](/docs/concepts/retrievers/) interface provides a standard way to connect to many different types of data services or databases (e.g., [vector stores](/docs/concepts/vectorstores) or databases).\\nThe underlying implementation of the retriever depends on the type of data store or database you are connecting to, but all retrievers implement the [runnable interface](/docs/concepts/runnables/), meaning they can be invoked in a common manner.\\n\\n```python\\ndocuments = my_retriever.invoke(\"What is the meaning of life?\")\\n```\\n\\n## Orchestration \\n\\nWhile standardization for individual components is useful, we\\'ve increasingly seen that developers want to *combine* components into more complex applications. \\nThis motivates the need for [orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)).\\nThere are several common characteristics of LLM applications that this orchestration layer should support:\\n\\n* **Complex control flow:** The application requires complex patterns such as cycles (e.g., a loop that reiterates until a condition is met).\\n* **[Persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/):** The application needs to maintain [short-term and / or long-term memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n* **[Human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/):** The application needs human interaction, e.g., pausing, reviewing, editing, approving certain steps.\\n\\nThe recommended way to orchestrate components for complex applications is [LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/).\\nLangGraph is a library that gives developers a high degree of control by expressing the flow of the application as a set of nodes and edges.\\nLangGraph comes with built-in support for [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), [memory](https://langchain-ai.github.io/langgraph/concepts/memory/), and other features.\\nIt\\'s particularly well suited for building [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/) or [multi-agent](https://langchain-ai.github.io/langgraph/concepts/multi_agent/) applications. \\nImportantly, individual LangChain components can be used as LangGraph nodes, but you can also use LangGraph **without** using LangChain components.\\n\\n:::info[Further reading]\\n\\nHave a look at our free course, [Introduction to LangGraph](https://academy.langchain.com/courses/intro-to-langgraph), to learn more about how to use LangGraph to build complex applications.\\n\\n:::\\n\\n## Observability and evaluation\\n\\nThe pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. \\nDevelopers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\nHigh quality tracing and evaluations can help you rapidly answer these types of questions with confidence.\\n[LangSmith](https://docs.smith.langchain.com/) is our platform that supports observability and evaluation for AI applications.\\nSee our conceptual guides on [evaluations](https://docs.smith.langchain.com/concepts/evaluation) and [tracing](https://docs.smith.langchain.com/concepts/tracing) for more details.\\n\\n:::info[Further reading]\\n\\nSee our video playlist on [LangSmith tracing and evaluations](https://youtube.com/playlist?list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S&feature=shared) for more details.\\n\\n:::\\n\\n## Conclusion\\n\\nLangChain offers standard interfaces for components that are central to many AI applications, which offers a few specific advantages:\\n- **Ease of swapping providers:** It allows you to swap out different component providers without having to change the underlying code.\\n- **Advanced features:** It provides common methods for more advanced features, such as [streaming](/docs/concepts/streaming) and [tool calling](/docs/concepts/tool_calling/).\\n\\n[LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/) makes it possible to orchestrate complex applications (e.g., [agents](/docs/concepts/agents/)) and provide features like including [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), or [memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n\\n[LangSmith](https://docs.smith.langchain.com/) makes it possible to iterate with confidence on your applications, by providing LLM-specific observability and framework for testing and evaluating your application.\\n': 0.03333333333333333, '---\\nsidebar_position: 0\\nsidebar_class_name: hidden\\n---\\n\\n# Introduction\\n\\n**LangChain** is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:\\n- **Development**: Build your applications using LangChain\\'s open-source [components](/docs/concepts) and [third-party integrations](/docs/integrations/providers/).\\nUse [LangGraph](/docs/concepts/architecture/#langgraph) to build stateful agents with first-class streaming and human-in-the-loop support.\\n- **Productionization**: Use [LangSmith](https://docs.smith.langchain.com/) to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\n- **Deployment**: Turn your LangGraph applications into production-ready APIs and Assistants with [LangGraph Platform](https://langchain-ai.github.io/langgraph/cloud/).\\n\\nimport ThemedImage from \\'@theme/ThemedImage\\';\\nimport useBaseUrl from \\'@docusaurus/useBaseUrl\\';\\n\\n<ThemedImage\\n  alt=\"Diagram outlining the hierarchical organization of the LangChain framework, displaying the interconnected parts across multiple layers.\"\\n  sources={{\\n    light: useBaseUrl(\\'/svg/langchain_stack_112024.svg\\'),\\n    dark: useBaseUrl(\\'/svg/langchain_stack_112024_dark.svg\\'),\\n  }}\\n  style={{ width: \"100%\" }}\\n  title=\"LangChain Framework Overview\"\\n/>\\n\\nLangChain implements a standard interface for large language models and related\\ntechnologies, such as embedding models and vector stores, and integrates with\\nhundreds of providers. See the [integrations](/docs/integrations/providers/) page for\\nmore.\\n\\nimport ChatModelTabs from \"@theme/ChatModelTabs\";\\n\\n<ChatModelTabs/>\\n\\n```python\\nmodel.invoke(\"Hello, world!\")\\n```\\n\\n:::note\\n\\nThese docs focus on the Python LangChain library. [Head here](https://js.langchain.com) for docs on the JavaScript LangChain library.\\n\\n:::\\n\\n## Architecture\\n\\nThe LangChain framework consists of multiple open-source libraries. Read more in the\\n[Architecture](/docs/concepts/architecture/) page.\\n\\n- **`langchain-core`**: Base abstractions for chat models and other components.\\n- **Integration packages** (e.g. `langchain-openai`, `langchain-anthropic`, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\\n- **`langchain`**: Chains, agents, and retrieval strategies that make up an application\\'s cognitive architecture.\\n- **`langchain-community`**: Third-party integrations that are community maintained.\\n- **`langgraph`**: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See [LangGraph documentation](https://langchain-ai.github.io/langgraph/).\\n\\n## Guides\\n\\n### [Tutorials](/docs/tutorials)\\n\\nIf you\\'re looking to build something specific or are more of a hands-on learner, check out our [tutorials section](/docs/tutorials).\\nThis is the best place to get started.\\n\\nThese are the best ones to get started with:\\n\\n- [Build a Simple LLM Application](/docs/tutorials/llm_chain)\\n- [Build a Chatbot](/docs/tutorials/chatbot)\\n- [Build an Agent](/docs/tutorials/agents)\\n- [Introduction to LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/)\\n\\nExplore the full list of LangChain tutorials [here](/docs/tutorials), and check out other [LangGraph tutorials here](https://langchain-ai.github.io/langgraph/tutorials/). To learn more about LangGraph, check out our first LangChain Academy course, *Introduction to LangGraph*, available [here](https://academy.langchain.com/courses/intro-to-langgraph).\\n\\n\\n### [How-to guides](/docs/how_to)\\n\\n[Here](/docs/how_to) you’ll find short answers to “How do I….?” types of questions.\\nThese how-to guides don’t cover topics in depth – you’ll find that material in the [Tutorials](/docs/tutorials) and the [API Reference](https://python.langchain.com/api_reference/).\\nHowever, these guides will help you quickly accomplish common tasks using [chat models](/docs/how_to/#chat-models),\\n[vector stores](/docs/how_to/#vector-stores), and other common LangChain components.\\n\\nCheck out [LangGraph-specific how-tos here](https://langchain-ai.github.io/langgraph/how-tos/).\\n\\n### [Conceptual guide](/docs/concepts)\\n\\nIntroductions to all the key parts of LangChain you’ll need to know! [Here](/docs/concepts) you\\'ll find high level explanations of all LangChain concepts.\\n\\nFor a deeper dive into LangGraph concepts, check out [this page](https://langchain-ai.github.io/langgraph/concepts/).\\n\\n### [Integrations](integrations/providers/index.mdx)\\n\\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\\nIf you\\'re looking to get up and running quickly with [chat models](/docs/integrations/chat/), [vector stores](/docs/integrations/vectorstores/),\\nor other LangChain components from a specific provider, check out our growing list of [integrations](/docs/integrations/providers/).\\n\\n\\n### [API reference](https://python.langchain.com/api_reference/)\\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\\n\\n## Ecosystem\\n\\n### [🦜🛠️ LangSmith](https://docs.smith.langchain.com)\\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\\n\\n### [🦜🕸️ LangGraph](https://langchain-ai.github.io/langgraph)\\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by Linkedin, Uber, Klarna, GitLab, and many more.\\n\\n## Additional resources\\n\\n### [Versions](/docs/versions/v0_3/)\\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\\n\\n### [Security](/docs/security)\\nRead up on [security](/docs/security) best practices to make sure you\\'re developing safely with LangChain.\\n\\n### [Contributing](contributing/index.mdx)\\nCheck out the developer\\'s guide for guidelines on contributing and help getting your dev environment set up.\\n': 0.03252247488101534, '# LangChain Expression Language (LCEL)\\n\\n:::info Prerequisites\\n* [Runnable Interface](/docs/concepts/runnables)\\n:::\\n\\nThe **L**ang**C**hain **E**xpression **L**anguage (LCEL) takes a [declarative](https://en.wikipedia.org/wiki/Declarative_programming) approach to building new [Runnables](/docs/concepts/runnables) from existing Runnables.\\n\\nThis means that you describe what *should* happen, rather than *how* it should happen, allowing LangChain to optimize the run-time execution of the chains.\\n\\nWe often refer to a `Runnable` created using LCEL as a \"chain\". It\\'s important to remember that a \"chain\" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).\\n\\n:::note\\n* The [LCEL cheatsheet](/docs/how_to/lcel_cheatsheet/) shows common patterns that involve the Runnable interface and LCEL expressions.\\n* Please see the following list of [how-to guides](/docs/how_to/#langchain-expression-language-lcel) that cover common tasks with LCEL.\\n* A list of built-in `Runnables` can be found in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html). Many of these Runnables are useful when composing custom \"chains\" in LangChain using LCEL.\\n:::\\n\\n## Benefits of LCEL\\n\\nLangChain optimizes the run-time execution of chains built with LCEL in a number of ways:\\n\\n- **Optimized parallel execution**: Run Runnables in parallel using [RunnableParallel](#runnableparallel) or run multiple inputs through a given chain in parallel using the [Runnable Batch API](/docs/concepts/runnables/#optimized-parallel-execution-batch). Parallel execution can significantly reduce the latency as processing can be done in parallel instead of sequentially.\\n- **Guaranteed Async support**: Any chain built with LCEL can be run asynchronously using the [Runnable Async API](/docs/concepts/runnables/#asynchronous-support). This can be useful when running chains in a server environment where you want to handle large number of requests concurrently.\\n- **Simplify streaming**: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(time elapsed until the first chunk of output from a [chat model](/docs/concepts/chat_models) or [llm](/docs/concepts/text_llms) comes out).\\n\\nOther benefits include:\\n\\n- [**Seamless LangSmith tracing**](https://docs.smith.langchain.com)\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, **all** steps are automatically logged to [LangSmith](https://docs.smith.langchain.com/) for maximum observability and debuggability.\\n- **Standard API**: Because all chains are built using the Runnable interface, they can be used in the same way as any other Runnable.\\n- [**Deployable with LangServe**](/docs/concepts/architecture#langserve): Chains built with LCEL can be deployed using for production use.\\n\\n## Should I use LCEL?\\n\\nLCEL is an [orchestration solution](https://en.wikipedia.org/wiki/Orchestration_(computing)) -- it allows LangChain to handle run-time execution of chains in an optimized way.\\n\\nWhile we have seen users run chains with hundreds of steps in production, we generally recommend using LCEL for simpler orchestration tasks. When the application requires complex state management, branching, cycles or multiple agents, we recommend that users take advantage of [LangGraph](/docs/concepts/architecture#langgraph).\\n\\nIn LangGraph, users define graphs that specify the application\\'s flow. This allows users to keep using LCEL within individual nodes when LCEL is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.\\n\\nHere are some guidelines:\\n\\n* If you are making a single LLM call, you don\\'t need LCEL; instead call the underlying [chat model](/docs/concepts/chat_models) directly.\\n* If you have a simple chain (e.g., prompt + llm + parser, simple retrieval set up etc.), LCEL is a reasonable fit, if you\\'re taking advantage of the LCEL benefits.\\n* If you\\'re building a complex chain (e.g., with branching, cycles, multiple agents, etc.) use [LangGraph](/docs/concepts/architecture#langgraph) instead. Remember that you can always use LCEL within individual nodes in LangGraph.\\n\\n## Composition Primitives\\n\\n`LCEL` chains are built by composing existing `Runnables` together. The two main composition primitives are [RunnableSequence](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableSequence.html#langchain_core.runnables.base.RunnableSequence) and [RunnableParallel](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableParallel.html#langchain_core.runnables.base.RunnableParallel).\\n\\nMany other composition primitives (e.g., [RunnableAssign](\\nhttps://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnableAssign.html#langchain_core.runnables.passthrough.RunnableAssign\\n)) can be thought of as variations of these two primitives.\\n\\n:::note\\nYou can find a list of all composition primitives in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html).\\n:::\\n\\n### RunnableSequence\\n\\n`RunnableSequence` is a composition primitive that allows you \"chain\" multiple runnables sequentially, with the output of one runnable serving as the input to the next.\\n\\n```python\\nfrom langchain_core.runnables import RunnableSequence\\nchain = RunnableSequence([runnable1, runnable2])\\n```\\n\\nInvoking the `chain` with some input:\\n\\n```python\\nfinal_output = chain.invoke(some_input)\\n```\\n\\ncorresponds to the following:\\n\\n```python\\noutput1 = runnable1.invoke(some_input)\\nfinal_output = runnable2.invoke(output1)\\n```\\n\\n:::note\\n`runnable1` and `runnable2` are placeholders for any `Runnable` that you want to chain together.\\n:::\\n\\n### RunnableParallel\\n\\n`RunnableParallel` is a composition primitive that allows you to run multiple runnables concurrently, with the same input provided to each.\\n\\n```python\\nfrom langchain_core.runnables import RunnableParallel\\nchain = RunnableParallel({\\n    \"key1\": runnable1,\\n    \"key2\": runnable2,\\n})\\n```\\n\\nInvoking the `chain` with some input:\\n\\n```python\\nfinal_output = chain.invoke(some_input)\\n```\\n\\nWill yield a `final_output` dictionary with the same keys as the input dictionary, but with the values replaced by the output of the corresponding runnable.\\n\\n```python\\n{\\n    \"key1\": runnable1.invoke(some_input),\\n    \"key2\": runnable2.invoke(some_input),\\n}\\n```\\n\\nRecall, that the runnables are executed in parallel, so while the result is the same as\\ndictionary comprehension shown above, the execution time is much faster.\\n\\n:::note\\n`RunnableParallel`supports both synchronous and asynchronous execution (as all `Runnables` do).\\n\\n* For synchronous execution, `RunnableParallel` uses a [ThreadPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor) to run the runnables concurrently.\\n* For asynchronous execution, `RunnableParallel` uses [asyncio.gather](https://docs.python.org/3/library/asyncio.html#asyncio.gather) to run the runnables concurrently.\\n:::\\n\\n## Composition Syntax\\n\\nThe usage of `RunnableSequence` and `RunnableParallel` is so common that we created a shorthand syntax for using them. This helps\\nto make the code more readable and concise.\\n\\n### The `|` operator\\n\\nWe have [overloaded](https://docs.python.org/3/reference/datamodel.html#special-method-names) the `|` operator to create a `RunnableSequence` from two `Runnables`.\\n\\n```python\\nchain = runnable1 | runnable2\\n```\\n\\nis Equivalent to:\\n\\n```python\\nchain = RunnableSequence([runnable1, runnable2])\\n```\\n\\n### The `.pipe` method\\n\\nIf you have moral qualms with operator overloading, you can use the `.pipe` method instead. This is equivalent to the `|` operator.\\n\\n```python\\nchain = runnable1.pipe(runnable2)\\n```\\n\\n### Coercion\\n\\nLCEL applies automatic type coercion to make it easier to compose chains.\\n\\nIf you do not understand the type coercion, you can always use the `RunnableSequence` and `RunnableParallel` classes directly.\\n\\nThis will make the code more verbose, but it will also make it more explicit.\\n\\n#### Dictionary to RunnableParallel\\n\\nInside an LCEL expression, a dictionary is automatically converted to a `RunnableParallel`.\\n\\nFor example, the following code:\\n\\n```python\\nmapping = {\\n    \"key1\": runnable1,\\n    \"key2\": runnable2,\\n}\\n\\nchain = mapping | runnable3\\n```\\n\\nIt gets automatically converted to the following:\\n\\n```python\\nchain = RunnableSequence([RunnableParallel(mapping), runnable3])\\n```\\n\\n:::caution\\nYou have to be careful because the `mapping` dictionary is not a `RunnableParallel` object, it is just a dictionary. This means that the following code will raise an `AttributeError`:\\n\\n```python\\nmapping.invoke(some_input)\\n```\\n:::\\n\\n#### Function to RunnableLambda\\n\\nInside an LCEL expression, a function is automatically converted to a `RunnableLambda`.\\n\\n```\\ndef some_func(x):\\n    return x\\n\\nchain = some_func | runnable1\\n```\\n\\nIt gets automatically converted to the following:\\n\\n```python\\nchain = RunnableSequence([RunnableLambda(some_func), runnable1])\\n```\\n\\n:::caution\\nYou have to be careful because the lambda function is not a `RunnableLambda` object, it is just a function. This means that the following code will raise an `AttributeError`:\\n\\n```python\\nlambda x: x + 1.invoke(some_input)\\n```\\n:::\\n\\n## Legacy chains\\n\\nLCEL aims to provide consistency around behavior and customization over legacy subclassed chains such as `LLMChain` and\\n`ConversationalRetrievalChain`. Many of these legacy chains hide important details like prompts, and as a wider variety\\nof viable models emerge, customization has become more and more important.\\n\\nIf you are currently using one of these legacy chains, please see [this guide for guidance on how to migrate](/docs/versions/migrating_chains).\\n\\nFor guides on how to do specific tasks with LCEL, check out [the relevant how-to guides](/docs/how_to/#langchain-expression-language-lcel).\\n': 0.03252247488101534, '# Conceptual guide\\n\\nThis guide provides explanations of the key concepts behind the LangChain framework and AI applications more broadly.\\n\\nWe recommend that you go through at least one of the [Tutorials](/docs/tutorials) before diving into the conceptual guide. This will provide practical context that will make it easier to understand the concepts discussed here.\\n\\nThe conceptual guide does not cover step-by-step instructions or specific implementation examples — those are found in the [How-to guides](/docs/how_to/) and [Tutorials](/docs/tutorials). For detailed reference material, please see the [API reference](https://python.langchain.com/api_reference/).\\n\\n## High level\\n\\n- **[Why LangChain?](/docs/concepts/why_langchain)**: Overview of the value that LangChain provides.\\n- **[Architecture](/docs/concepts/architecture)**: How packages are organized in the LangChain ecosystem.\\n\\n## Concepts\\n\\n- **[Chat models](/docs/concepts/chat_models)**: LLMs exposed via a chat API that process sequences of messages as input and output a message.\\n- **[Messages](/docs/concepts/messages)**: The unit of communication in chat models, used to represent model input and output.\\n- **[Chat history](/docs/concepts/chat_history)**: A conversation represented as a sequence of messages, alternating between user messages and model responses.\\n- **[Tools](/docs/concepts/tools)**: A function with an associated schema defining the function\\'s name, description, and the arguments it accepts.\\n- **[Tool calling](/docs/concepts/tool_calling)**: A type of chat model API that accepts tool schemas, along with messages, as input and returns invocations of those tools as part of the output message.\\n- **[Structured output](/docs/concepts/structured_outputs)**: A technique to make a chat model respond in a structured format, such as JSON that matches a given schema.\\n- **[Memory](https://langchain-ai.github.io/langgraph/concepts/memory/)**: Information about a conversation that is persisted so that it can be used in future conversations.\\n- **[Multimodality](/docs/concepts/multimodality)**: The ability to work with data that comes in different forms, such as text, audio, images, and video.\\n- **[Runnable interface](/docs/concepts/runnables)**: The base abstraction that many LangChain components and the LangChain Expression Language are built on.\\n- **[Streaming](/docs/concepts/streaming)**: LangChain streaming APIs for surfacing results as they are generated.\\n- **[LangChain Expression Language (LCEL)](/docs/concepts/lcel)**: A syntax for orchestrating LangChain components. Most useful for simpler applications.\\n- **[Document loaders](/docs/concepts/document_loaders)**: Load a source as a list of documents.\\n- **[Retrieval](/docs/concepts/retrieval)**: Information retrieval systems can retrieve structured or unstructured data from a datasource in response to a query.\\n- **[Text splitters](/docs/concepts/text_splitters)**: Split long text into smaller chunks that can be individually indexed to enable granular retrieval.\\n- **[Embedding models](/docs/concepts/embedding_models)**: Models that represent data such as text or images in a vector space.\\n- **[Vector stores](/docs/concepts/vectorstores)**: Storage of and efficient search over vectors and associated metadata.\\n- **[Retriever](/docs/concepts/retrievers)**: A component that returns relevant documents from a knowledge base in response to a query.\\n- **[Retrieval Augmented Generation (RAG)](/docs/concepts/rag)**: A technique that enhances language models by combining them with external knowledge bases.\\n- **[Agents](/docs/concepts/agents)**: Use a [language model](/docs/concepts/chat_models) to choose a sequence of actions to take. Agents can interact with external resources via [tool](/docs/concepts/tools).\\n- **[Prompt templates](/docs/concepts/prompt_templates)**: Component for factoring out the static parts of a model \"prompt\" (usually a sequence of messages). Useful for serializing, versioning, and reusing these static parts.\\n- **[Output parsers](/docs/concepts/output_parsers)**: Responsible for taking the output of a model and transforming it into a more suitable format for downstream tasks. Output parsers were primarily useful prior to the general availability of [tool calling](/docs/concepts/tool_calling) and [structured outputs](/docs/concepts/structured_outputs).\\n- **[Few-shot prompting](/docs/concepts/few_shot_prompting)**: A technique for improving model performance by providing a few examples of the task to perform in the prompt.\\n- **[Example selectors](/docs/concepts/example_selectors)**: Used to select the most relevant examples from a dataset based on a given input. Example selectors are used in few-shot prompting to select examples for a prompt.\\n- **[Async programming](/docs/concepts/async)**: The basics that one should know to use LangChain in an asynchronous context.\\n- **[Callbacks](/docs/concepts/callbacks)**: Callbacks enable the execution of custom auxiliary code in built-in components. Callbacks are used to stream outputs from LLMs in LangChain, trace the intermediate steps of an application, and more.\\n- **[Tracing](/docs/concepts/tracing)**: The process of recording the steps that an application takes to go from input to output. Tracing is essential for debugging and diagnosing issues in complex applications.\\n- **[Evaluation](/docs/concepts/evaluation)**: The process of assessing the performance and effectiveness of AI applications. This involves testing the model\\'s responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose. This process is vital for building reliable applications.\\n- **[Testing](/docs/concepts/testing)**: The process of verifying that a component of an integration or application works as expected. Testing is essential for ensuring that the application behaves correctly and that changes to the codebase do not introduce new bugs.\\n\\n## Glossary\\n\\n- **[AIMessageChunk](/docs/concepts/messages#aimessagechunk)**: A partial response from an AI message. Used when streaming responses from a chat model.\\n- **[AIMessage](/docs/concepts/messages#aimessage)**: Represents a complete response from an AI model.\\n- **[astream_events](/docs/concepts/chat_models#key-methods)**: Stream granular information from [LCEL](/docs/concepts/lcel) chains.\\n- **[BaseTool](/docs/concepts/tools/#tool-interface)**: The base class for all tools in LangChain.\\n- **[batch](/docs/concepts/runnables)**: Use to execute a runnable with batch inputs.\\n- **[bind_tools](/docs/concepts/tool_calling/#tool-binding)**: Allows models to interact with tools.\\n- **[Caching](/docs/concepts/chat_models#caching)**: Storing results to avoid redundant calls to a chat model.\\n- **[Chat models](/docs/concepts/multimodality/#multimodality-in-chat-models)**: Chat models that handle multiple data modalities.\\n- **[Configurable runnables](/docs/concepts/runnables/#configurable-runnables)**: Creating configurable Runnables.\\n- **[Context window](/docs/concepts/chat_models#context-window)**: The maximum size of input a chat model can process.\\n- **[Conversation patterns](/docs/concepts/chat_history#conversation-patterns)**: Common patterns in chat interactions.\\n- **[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)**: LangChain\\'s representation of a document.\\n- **[Embedding models](/docs/concepts/multimodality/#multimodality-in-embedding-models)**: Models that generate vector embeddings for various data types.\\n- **[HumanMessage](/docs/concepts/messages#humanmessage)**: Represents a message from a human user.\\n- **[InjectedState](/docs/concepts/tools#injectedstate)**: A state injected into a tool function.\\n- **[InjectedStore](/docs/concepts/tools#injectedstore)**: A store that can be injected into a tool for data persistence.\\n- **[InjectedToolArg](/docs/concepts/tools#injectedtoolarg)**: Mechanism to inject arguments into tool functions.\\n- **[input and output types](/docs/concepts/runnables#input-and-output-types)**: Types used for input and output in Runnables.\\n- **[Integration packages](/docs/concepts/architecture/#integration-packages)**: Third-party packages that integrate with LangChain.\\n- **[Integration tests](/docs/concepts/testing#integration-tests)**: Tests that verify the correctness of the interaction between components, usually run with access to the underlying API that powers an integration.\\n- **[invoke](/docs/concepts/runnables)**: A standard method to invoke a Runnable.\\n- **[JSON mode](/docs/concepts/structured_outputs#json-mode)**: Returning responses in JSON format.\\n- **[langchain-community](/docs/concepts/architecture#langchain-community)**: Community-driven components for LangChain.\\n- **[langchain-core](/docs/concepts/architecture#langchain-core)**: Core langchain package. Includes base interfaces and in-memory implementations.\\n- **[langchain](/docs/concepts/architecture#langchain)**: A package for higher level components (e.g., some pre-built chains).\\n- **[langgraph](/docs/concepts/architecture#langgraph)**: Powerful orchestration layer for LangChain. Use to build complex pipelines and workflows.\\n- **[langserve](/docs/concepts/architecture#langserve)**: Used to deploy LangChain Runnables as REST endpoints. Uses FastAPI. Works primarily for LangChain Runnables, does not currently integrate with LangGraph.\\n- **[LLMs (legacy)](/docs/concepts/text_llms)**: Older language models that take a string as input and return a string as output.\\n- **[Managing chat history](/docs/concepts/chat_history#managing-chat-history)**: Techniques to maintain and manage the chat history.\\n- **[OpenAI format](/docs/concepts/messages#openai-format)**: OpenAI\\'s message format for chat models.\\n- **[Propagation of RunnableConfig](/docs/concepts/runnables/#propagation-of-runnableconfig)**: Propagating configuration through Runnables. Read if working with python 3.9, 3.10 and async.\\n- **[rate-limiting](/docs/concepts/chat_models#rate-limiting)**: Client side rate limiting for chat models.\\n- **[RemoveMessage](/docs/concepts/messages/#removemessage)**: An abstraction used to remove a message from chat history, used primarily in LangGraph.\\n- **[role](/docs/concepts/messages#role)**: Represents the role (e.g., user, assistant) of a chat message.\\n- **[RunnableConfig](/docs/concepts/runnables/#runnableconfig)**: Use to pass run time information to Runnables (e.g., `run_name`, `run_id`, `tags`, `metadata`, `max_concurrency`, `recursion_limit`, `configurable`).\\n- **[Standard parameters for chat models](/docs/concepts/chat_models#standard-parameters)**: Parameters such as API key, `temperature`, and `max_tokens`.\\n- **[Standard tests](/docs/concepts/testing#standard-tests)**: A defined set of unit and integration tests that all integrations must pass.\\n- **[stream](/docs/concepts/streaming)**: Use to stream output from a Runnable or a graph.\\n- **[Tokenization](/docs/concepts/tokens)**: The process of converting data into tokens and vice versa.\\n- **[Tokens](/docs/concepts/tokens)**: The basic unit that a language model reads, processes, and generates under the hood.\\n- **[Tool artifacts](/docs/concepts/tools#tool-artifacts)**: Add artifacts to the output of a tool that will not be sent to the model, but will be available for downstream processing.\\n- **[Tool binding](/docs/concepts/tool_calling#tool-binding)**: Binding tools to models.\\n- **[@tool](/docs/concepts/tools/#create-tools-using-the-tool-decorator)**: Decorator for creating tools in LangChain.\\n- **[Toolkits](/docs/concepts/tools#toolkits)**: A collection of tools that can be used together.\\n- **[ToolMessage](/docs/concepts/messages#toolmessage)**: Represents a message that contains the results of a tool execution.\\n- **[Unit tests](/docs/concepts/testing#unit-tests)**: Tests that verify the correctness of individual components, run in isolation without access to the Internet.\\n- **[Vector stores](/docs/concepts/vectorstores)**: Datastores specialized for storing and efficiently searching vector embeddings.\\n- **[with_structured_output](/docs/concepts/structured_outputs/#structured-output-method)**: A helper method for chat models that natively support [tool calling](/docs/concepts/tool_calling) to get structured output matching a given schema specified via Pydantic, JSON schema or a function.\\n- **[with_types](/docs/concepts/runnables#with_types)**: Method to overwrite the input and output types of a runnable. Useful when working with complex LCEL chains and deploying with LangServe.\\n': 0.015873015873015872, '---\\nsidebar_position: 0\\nsidebar_class_name: hidden\\n---\\n\\n# How-to guides\\n\\nHere you’ll find answers to “How do I….?” types of questions.\\nThese guides are *goal-oriented* and *concrete*; they\\'re meant to help you complete a specific task.\\nFor conceptual explanations see the [Conceptual guide](/docs/concepts/).\\nFor end-to-end walkthroughs see [Tutorials](/docs/tutorials).\\nFor comprehensive descriptions of every class and function see the [API Reference](https://python.langchain.com/api_reference/).\\n\\n## Installation\\n\\n- [How to: install LangChain packages](/docs/how_to/installation/)\\n- [How to: use LangChain with different Pydantic versions](/docs/how_to/pydantic_compatibility)\\n\\n## Key features\\n\\nThis highlights functionality that is core to using LangChain.\\n\\n- [How to: return structured data from a model](/docs/how_to/structured_output/)\\n- [How to: use a model to call tools](/docs/how_to/tool_calling)\\n- [How to: stream runnables](/docs/how_to/streaming)\\n- [How to: debug your LLM apps](/docs/how_to/debugging/)\\n\\n## Components\\n\\nThese are the core building blocks you can use when building applications.\\n\\n### Chat models\\n\\n[Chat Models](/docs/concepts/chat_models) are newer forms of language models that take messages in and output a message.\\nSee [supported integrations](/docs/integrations/chat/) for details on getting started with chat models from a specific provider.\\n\\n- [How to: do function/tool calling](/docs/how_to/tool_calling)\\n- [How to: get models to return structured output](/docs/how_to/structured_output)\\n- [How to: cache model responses](/docs/how_to/chat_model_caching)\\n- [How to: get log probabilities](/docs/how_to/logprobs)\\n- [How to: create a custom chat model class](/docs/how_to/custom_chat_model)\\n- [How to: stream a response back](/docs/how_to/chat_streaming)\\n- [How to: track token usage](/docs/how_to/chat_token_usage_tracking)\\n- [How to: track response metadata across providers](/docs/how_to/response_metadata)\\n- [How to: use chat model to call tools](/docs/how_to/tool_calling)\\n- [How to: stream tool calls](/docs/how_to/tool_streaming)\\n- [How to: handle rate limits](/docs/how_to/chat_model_rate_limiting)\\n- [How to: few shot prompt tool behavior](/docs/how_to/tools_few_shot)\\n- [How to: bind model-specific formatted tools](/docs/how_to/tools_model_specific)\\n- [How to: force a specific tool call](/docs/how_to/tool_choice)\\n- [How to: work with local models](/docs/how_to/local_llms)\\n- [How to: init any model in one line](/docs/how_to/chat_models_universal_init/)\\n- [How to: pass multimodal data directly to models](/docs/how_to/multimodal_inputs/)\\n\\n### Messages\\n\\n[Messages](/docs/concepts/messages) are the input and output of chat models. They have some `content` and a `role`, which describes the source of the message.\\n\\n- [How to: trim messages](/docs/how_to/trim_messages/)\\n- [How to: filter messages](/docs/how_to/filter_messages/)\\n- [How to: merge consecutive messages of the same type](/docs/how_to/merge_message_runs/)\\n\\n### Prompt templates\\n\\n[Prompt Templates](/docs/concepts/prompt_templates) are responsible for formatting user input into a format that can be passed to a language model.\\n\\n- [How to: use few shot examples](/docs/how_to/few_shot_examples)\\n- [How to: use few shot examples in chat models](/docs/how_to/few_shot_examples_chat/)\\n- [How to: partially format prompt templates](/docs/how_to/prompts_partial)\\n- [How to: compose prompts together](/docs/how_to/prompts_composition)\\n- [How to: use multimodal prompts](/docs/how_to/multimodal_prompts/)\\n\\n### Example selectors\\n\\n[Example Selectors](/docs/concepts/example_selectors) are responsible for selecting the correct few shot examples to pass to the prompt.\\n\\n- [How to: use example selectors](/docs/how_to/example_selectors)\\n- [How to: select examples by length](/docs/how_to/example_selectors_length_based)\\n- [How to: select examples by semantic similarity](/docs/how_to/example_selectors_similarity)\\n- [How to: select examples by semantic ngram overlap](/docs/how_to/example_selectors_ngram)\\n- [How to: select examples by maximal marginal relevance](/docs/how_to/example_selectors_mmr)\\n- [How to: select examples from LangSmith few-shot datasets](/docs/how_to/example_selectors_langsmith/)\\n\\n### LLMs\\n\\nWhat LangChain calls [LLMs](/docs/concepts/text_llms) are older forms of language models that take a string in and output a string.\\n\\n- [How to: cache model responses](/docs/how_to/llm_caching)\\n- [How to: create a custom LLM class](/docs/how_to/custom_llm)\\n- [How to: stream a response back](/docs/how_to/streaming_llm)\\n- [How to: track token usage](/docs/how_to/llm_token_usage_tracking)\\n- [How to: work with local models](/docs/how_to/local_llms)\\n\\n### Output parsers\\n\\n[Output Parsers](/docs/concepts/output_parsers) are responsible for taking the output of an LLM and parsing into more structured format.\\n\\n- [How to: parse text from message objects](/docs/how_to/output_parser_string)\\n- [How to: use output parsers to parse an LLM response into structured format](/docs/how_to/output_parser_structured)\\n- [How to: parse JSON output](/docs/how_to/output_parser_json)\\n- [How to: parse XML output](/docs/how_to/output_parser_xml)\\n- [How to: parse YAML output](/docs/how_to/output_parser_yaml)\\n- [How to: retry when output parsing errors occur](/docs/how_to/output_parser_retry)\\n- [How to: try to fix errors in output parsing](/docs/how_to/output_parser_fixing)\\n- [How to: write a custom output parser class](/docs/how_to/output_parser_custom)\\n\\n### Document loaders\\n\\n[Document Loaders](/docs/concepts/document_loaders) are responsible for loading documents from a variety of sources.\\n\\n- [How to: load PDF files](/docs/how_to/document_loader_pdf)\\n- [How to: load web pages](/docs/how_to/document_loader_web)\\n- [How to: load CSV data](/docs/how_to/document_loader_csv)\\n- [How to: load data from a directory](/docs/how_to/document_loader_directory)\\n- [How to: load HTML data](/docs/how_to/document_loader_html)\\n- [How to: load JSON data](/docs/how_to/document_loader_json)\\n- [How to: load Markdown data](/docs/how_to/document_loader_markdown)\\n- [How to: load Microsoft Office data](/docs/how_to/document_loader_office_file)\\n- [How to: write a custom document loader](/docs/how_to/document_loader_custom)\\n\\n### Text splitters\\n\\n[Text Splitters](/docs/concepts/text_splitters) take a document and split into chunks that can be used for retrieval.\\n\\n- [How to: recursively split text](/docs/how_to/recursive_text_splitter)\\n- [How to: split HTML](/docs/how_to/split_html)\\n- [How to: split by character](/docs/how_to/character_text_splitter)\\n- [How to: split code](/docs/how_to/code_splitter)\\n- [How to: split Markdown by headers](/docs/how_to/markdown_header_metadata_splitter)\\n- [How to: recursively split JSON](/docs/how_to/recursive_json_splitter)\\n- [How to: split text into semantic chunks](/docs/how_to/semantic-chunker)\\n- [How to: split by tokens](/docs/how_to/split_by_token)\\n\\n### Embedding models\\n\\n[Embedding Models](/docs/concepts/embedding_models) take a piece of text and create a numerical representation of it.\\nSee [supported integrations](/docs/integrations/text_embedding/) for details on getting started with embedding models from a specific provider.\\n\\n- [How to: embed text data](/docs/how_to/embed_text)\\n- [How to: cache embedding results](/docs/how_to/caching_embeddings)\\n- [How to: create a custom embeddings class](/docs/how_to/custom_embeddings)\\n\\n### Vector stores\\n\\n[Vector stores](/docs/concepts/vectorstores) are databases that can efficiently store and retrieve embeddings.\\nSee [supported integrations](/docs/integrations/vectorstores/) for details on getting started with vector stores from a specific provider.\\n\\n- [How to: use a vector store to retrieve data](/docs/how_to/vectorstores)\\n\\n### Retrievers\\n\\n[Retrievers](/docs/concepts/retrievers) are responsible for taking a query and returning relevant documents.\\n\\n- [How to: use a vector store to retrieve data](/docs/how_to/vectorstore_retriever)\\n- [How to: generate multiple queries to retrieve data for](/docs/how_to/MultiQueryRetriever)\\n- [How to: use contextual compression to compress the data retrieved](/docs/how_to/contextual_compression)\\n- [How to: write a custom retriever class](/docs/how_to/custom_retriever)\\n- [How to: add similarity scores to retriever results](/docs/how_to/add_scores_retriever)\\n- [How to: combine the results from multiple retrievers](/docs/how_to/ensemble_retriever)\\n- [How to: reorder retrieved results to mitigate the \"lost in the middle\" effect](/docs/how_to/long_context_reorder)\\n- [How to: generate multiple embeddings per document](/docs/how_to/multi_vector)\\n- [How to: retrieve the whole document for a chunk](/docs/how_to/parent_document_retriever)\\n- [How to: generate metadata filters](/docs/how_to/self_query)\\n- [How to: create a time-weighted retriever](/docs/how_to/time_weighted_vectorstore)\\n- [How to: use hybrid vector and keyword retrieval](/docs/how_to/hybrid)\\n\\n### Indexing\\n\\nIndexing is the process of keeping your vectorstore in-sync with the underlying data source.\\n\\n- [How to: reindex data to keep your vectorstore in-sync with the underlying data source](/docs/how_to/indexing)\\n\\n### Tools\\n\\nLangChain [Tools](/docs/concepts/tools) contain a description of the tool (to pass to the language model) as well as the implementation of the function to call. Refer [here](/docs/integrations/tools/) for a list of pre-built tools. \\n\\n- [How to: create tools](/docs/how_to/custom_tools)\\n- [How to: use built-in tools and toolkits](/docs/how_to/tools_builtin)\\n- [How to: use chat models to call tools](/docs/how_to/tool_calling)\\n- [How to: pass tool outputs to chat models](/docs/how_to/tool_results_pass_to_model)\\n- [How to: pass run time values to tools](/docs/how_to/tool_runtime)\\n- [How to: add a human-in-the-loop for tools](/docs/how_to/tools_human)\\n- [How to: handle tool errors](/docs/how_to/tools_error)\\n- [How to: force models to call a tool](/docs/how_to/tool_choice)\\n- [How to: disable parallel tool calling](/docs/how_to/tool_calling_parallel)\\n- [How to: access the `RunnableConfig` from a tool](/docs/how_to/tool_configure)\\n- [How to: stream events from a tool](/docs/how_to/tool_stream_events)\\n- [How to: return artifacts from a tool](/docs/how_to/tool_artifacts/)\\n- [How to: convert Runnables to tools](/docs/how_to/convert_runnable_to_tool)\\n- [How to: add ad-hoc tool calling capability to models](/docs/how_to/tools_prompting)\\n- [How to: pass in runtime secrets](/docs/how_to/runnable_runtime_secrets)\\n\\n### Multimodal\\n\\n- [How to: pass multimodal data directly to models](/docs/how_to/multimodal_inputs/)\\n- [How to: use multimodal prompts](/docs/how_to/multimodal_prompts/)\\n\\n\\n### Agents\\n\\n:::note\\n\\nFor in depth how-to guides for agents, please check out [LangGraph](https://langchain-ai.github.io/langgraph/) documentation.\\n\\n:::\\n\\n- [How to: use legacy LangChain Agents (AgentExecutor)](/docs/how_to/agent_executor)\\n- [How to: migrate from legacy LangChain agents to LangGraph](/docs/how_to/migrate_agent)\\n\\n### Callbacks\\n\\n[Callbacks](/docs/concepts/callbacks) allow you to hook into the various stages of your LLM application\\'s execution.\\n\\n- [How to: pass in callbacks at runtime](/docs/how_to/callbacks_runtime)\\n- [How to: attach callbacks to a module](/docs/how_to/callbacks_attach)\\n- [How to: pass callbacks into a module constructor](/docs/how_to/callbacks_constructor)\\n- [How to: create custom callback handlers](/docs/how_to/custom_callbacks)\\n- [How to: use callbacks in async environments](/docs/how_to/callbacks_async)\\n- [How to: dispatch custom callback events](/docs/how_to/callbacks_custom_events)\\n\\n### Custom\\n\\nAll of LangChain components can easily be extended to support your own versions.\\n\\n- [How to: create a custom chat model class](/docs/how_to/custom_chat_model)\\n- [How to: create a custom LLM class](/docs/how_to/custom_llm)\\n- [How to: create a custom embeddings class](/docs/how_to/custom_embeddings)\\n- [How to: write a custom retriever class](/docs/how_to/custom_retriever)\\n- [How to: write a custom document loader](/docs/how_to/document_loader_custom)\\n- [How to: write a custom output parser class](/docs/how_to/output_parser_custom)\\n- [How to: create custom callback handlers](/docs/how_to/custom_callbacks)\\n- [How to: define a custom tool](/docs/how_to/custom_tools)\\n- [How to: dispatch custom callback events](/docs/how_to/callbacks_custom_events)\\n\\n### Serialization\\n- [How to: save and load LangChain objects](/docs/how_to/serialization)\\n\\n## Use cases\\n\\nThese guides cover use-case specific details.\\n\\n### Q&A with RAG\\n\\nRetrieval Augmented Generation (RAG) is a way to connect LLMs to external sources of data.\\nFor a high-level tutorial on RAG, check out [this guide](/docs/tutorials/rag/).\\n\\n- [How to: add chat history](/docs/how_to/qa_chat_history_how_to/)\\n- [How to: stream](/docs/how_to/qa_streaming/)\\n- [How to: return sources](/docs/how_to/qa_sources/)\\n- [How to: return citations](/docs/how_to/qa_citations/)\\n- [How to: do per-user retrieval](/docs/how_to/qa_per_user/)\\n\\n\\n### Extraction\\n\\nExtraction is when you use LLMs to extract structured information from unstructured text.\\nFor a high level tutorial on extraction, check out [this guide](/docs/tutorials/extraction/).\\n\\n- [How to: use reference examples](/docs/how_to/extraction_examples/)\\n- [How to: handle long text](/docs/how_to/extraction_long_text/)\\n- [How to: do extraction without using function calling](/docs/how_to/extraction_parse)\\n\\n### Chatbots\\n\\nChatbots involve using an LLM to have a conversation.\\nFor a high-level tutorial on building chatbots, check out [this guide](/docs/tutorials/chatbot/).\\n\\n- [How to: manage memory](/docs/how_to/chatbots_memory)\\n- [How to: do retrieval](/docs/how_to/chatbots_retrieval)\\n- [How to: use tools](/docs/how_to/chatbots_tools)\\n- [How to: manage large chat history](/docs/how_to/trim_messages/)\\n\\n### Query analysis\\n\\nQuery Analysis is the task of using an LLM to generate a query to send to a retriever.\\nFor a high-level tutorial on query analysis, check out [this guide](/docs/tutorials/rag/#query-analysis).\\n\\n- [How to: add examples to the prompt](/docs/how_to/query_few_shot)\\n- [How to: handle cases where no queries are generated](/docs/how_to/query_no_queries)\\n- [How to: handle multiple queries](/docs/how_to/query_multiple_queries)\\n- [How to: handle multiple retrievers](/docs/how_to/query_multiple_retrievers)\\n- [How to: construct filters](/docs/how_to/query_constructing_filters)\\n- [How to: deal with high cardinality categorical variables](/docs/how_to/query_high_cardinality)\\n\\n### Q&A over SQL + CSV\\n\\nYou can use LLMs to do question answering over tabular data.\\nFor a high-level tutorial, check out [this guide](/docs/tutorials/sql_qa/).\\n\\n- [How to: use prompting to improve results](/docs/how_to/sql_prompting)\\n- [How to: do query validation](/docs/how_to/sql_query_checking)\\n- [How to: deal with large databases](/docs/how_to/sql_large_db)\\n- [How to: deal with CSV files](/docs/how_to/sql_csv)\\n\\n### Q&A over graph databases\\n\\nYou can use an LLM to do question answering over graph databases.\\nFor a high-level tutorial, check out [this guide](/docs/tutorials/graph/).\\n\\n- [How to: add a semantic layer over the database](/docs/how_to/graph_semantic)\\n- [How to: construct knowledge graphs](/docs/how_to/graph_constructing)\\n\\n### Summarization\\n\\nLLMs can summarize and otherwise distill desired information from text, including\\nlarge volumes of text. For a high-level tutorial, check out [this guide](/docs/tutorials/summarization).\\n\\n- [How to: summarize text in a single LLM call](/docs/how_to/summarize_stuff)\\n- [How to: summarize text through parallelization](/docs/how_to/summarize_map_reduce)\\n- [How to: summarize text through iterative refinement](/docs/how_to/summarize_refine)\\n\\n## LangChain Expression Language (LCEL)\\n\\n:::note Should I use LCEL?\\n\\nLCEL is an orchestration solution. See our\\n[concepts page](/docs/concepts/lcel/#should-i-use-lcel) for recommendations on when to\\nuse LCEL.\\n\\n:::\\n\\n[LangChain Expression Language](/docs/concepts/lcel) is a way to create arbitrary custom chains. It is built on the [Runnable](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html) protocol.\\n\\n[**LCEL cheatsheet**](/docs/how_to/lcel_cheatsheet/): For a quick overview of how to use the main LCEL primitives.\\n\\n[**Migration guide**](/docs/versions/migrating_chains): For migrating legacy chain abstractions to LCEL.\\n\\n- [How to: chain runnables](/docs/how_to/sequence)\\n- [How to: stream runnables](/docs/how_to/streaming)\\n- [How to: invoke runnables in parallel](/docs/how_to/parallel/)\\n- [How to: add default invocation args to runnables](/docs/how_to/binding/)\\n- [How to: turn any function into a runnable](/docs/how_to/functions)\\n- [How to: pass through inputs from one chain step to the next](/docs/how_to/passthrough)\\n- [How to: configure runnable behavior at runtime](/docs/how_to/configure)\\n- [How to: add message history (memory) to a chain](/docs/how_to/message_history)\\n- [How to: route between sub-chains](/docs/how_to/routing)\\n- [How to: create a dynamic (self-constructing) chain](/docs/how_to/dynamic_chain/)\\n- [How to: inspect runnables](/docs/how_to/inspect)\\n- [How to: add fallbacks to a runnable](/docs/how_to/fallbacks)\\n- [How to: pass runtime secrets to a runnable](/docs/how_to/runnable_runtime_secrets)\\n\\n## [LangGraph](https://langchain-ai.github.io/langgraph)\\n\\nLangGraph is an extension of LangChain aimed at\\nbuilding robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.\\n\\nLangGraph documentation is currently hosted on a separate site.\\nYou can peruse [LangGraph how-to guides here](https://langchain-ai.github.io/langgraph/how-tos/).\\n\\n## [LangSmith](https://docs.smith.langchain.com/)\\n\\nLangSmith allows you to closely trace, monitor and evaluate your LLM application.\\nIt seamlessly integrates with LangChain and LangGraph, and you can use it to inspect and debug individual steps of your chains and agents as you build.\\n\\nLangSmith documentation is hosted on a separate site.\\nYou can peruse [LangSmith how-to guides here](https://docs.smith.langchain.com/), but we\\'ll highlight a few sections that are particularly\\nrelevant to LangChain below:\\n\\n### Evaluation\\n<span data-heading-keywords=\"evaluation,evaluate\"></span>\\n\\nEvaluating performance is a vital part of building LLM-powered applications.\\nLangSmith helps with every step of the process from creating a dataset to defining metrics to running evaluators.\\n\\nTo learn more, check out the [LangSmith evaluation how-to guides](https://docs.smith.langchain.com/how_to_guides#evaluation).\\n\\n### Tracing\\n<span data-heading-keywords=\"trace,tracing\"></span>\\n\\nTracing gives you observability inside your chains and agents, and is vital in diagnosing issues.\\n\\n- [How to: trace with LangChain](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain)\\n- [How to: add metadata and tags to traces](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#add-metadata-and-tags-to-traces)\\n\\nYou can see general tracing-related how-tos [in this section of the LangSmith docs](https://docs.smith.langchain.com/how_to_guides/tracing).\\n': 0.015873015873015872}\n",
      "docs:[Document(metadata={'file_name': 'why_langchain.mdx', 'file_path': 'docs\\\\docs\\\\concepts\\\\why_langchain.mdx', 'file_type': '.mdx', 'source': 'docs\\\\docs\\\\concepts\\\\why_langchain.mdx'}, page_content='# Why LangChain?\\n\\nThe goal of `langchain` the Python package and LangChain the company is to make it as easy as possible for developers to build applications that reason.\\nWhile LangChain originally started as a single open source package, it has evolved into a company and a whole ecosystem.\\nThis page will talk about the LangChain ecosystem as a whole.\\nMost of the components within the LangChain ecosystem can be used by themselves - so if you feel particularly drawn to certain components but not others, that is totally fine! Pick and choose whichever components you like best for your own use case!\\n\\n## Features\\n\\nThere are several primary needs that LangChain aims to address:\\n\\n1. **Standardized component interfaces:** The growing number of [models](/docs/integrations/chat/) and [related components](/docs/integrations/vectorstores/) for AI applications has resulted in a wide variety of different APIs that developers need to learn and use.\\nThis diversity can make it challenging for developers to switch between providers or combine components when building applications.\\nLangChain exposes a standard interface for key components, making it easy to switch between providers.\\n\\n2. **Orchestration:** As applications become more complex, combining multiple components and models, there\\'s [a growing need to efficiently connect these elements into control flows](https://lilianweng.github.io/posts/2023-06-23-agent/) that can [accomplish diverse tasks](https://www.sequoiacap.com/article/generative-ais-act-o1/).\\n[Orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)) is crucial for building such applications.\\n\\n3. **Observability and evaluation:** As applications become more complex, it becomes increasingly difficult to understand what is happening within them.\\nFurthermore, the pace of development can become rate-limited by the [paradox of choice](https://en.wikipedia.org/wiki/Paradox_of_choice).\\nFor example, developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\n[Observability](https://en.wikipedia.org/wiki/Observability) and evaluations can help developers monitor their applications and rapidly answer these types of questions with confidence.\\n\\n\\n## Standardized component interfaces\\n\\nLangChain provides common interfaces for components that are central to many AI applications.\\nAs an example, all [chat models](/docs/concepts/chat_models/) implement the [BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) interface.\\nThis provides a standard way to interact with chat models, supporting important but often provider-specific features like [tool calling](/docs/concepts/tool_calling/) and [structured outputs](/docs/concepts/structured_outputs/).\\n\\n\\n### Example: chat models \\n\\nMany [model providers](/docs/concepts/chat_models/) support [tool calling](/docs/concepts/tool_calling/), a critical feature for many applications (e.g., [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/)), that allows a developer to request model responses that match a particular schema.\\nThe APIs for each provider differ. \\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to bind [tools](/docs/concepts/tools) to a model in order to support [tool calling](/docs/concepts/tool_calling/):\\n\\n```python\\n# Tool creation\\ntools = [my_tool]\\n# Tool binding\\nmodel_with_tools = model.bind_tools(tools)\\n```\\n\\nSimilarly, getting models to produce [structured outputs](/docs/concepts/structured_outputs/) is an extremely common use case. \\nProviders support different approaches for this, including [JSON mode or tool calling](https://platform.openai.com/docs/guides/structured-outputs), with different APIs.\\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to produce structured outputs using the `with_structured_output()` method:\\n\\n```python\\n# Define schema\\nschema = ...\\n# Bind schema to model\\nmodel_with_structure = model.with_structured_output(schema)\\n```\\n\\n### Example: retrievers\\n\\nIn the context of [RAG](/docs/concepts/rag/) and LLM application components, LangChain\\'s [retriever](/docs/concepts/retrievers/) interface provides a standard way to connect to many different types of data services or databases (e.g., [vector stores](/docs/concepts/vectorstores) or databases).\\nThe underlying implementation of the retriever depends on the type of data store or database you are connecting to, but all retrievers implement the [runnable interface](/docs/concepts/runnables/), meaning they can be invoked in a common manner.\\n\\n```python\\ndocuments = my_retriever.invoke(\"What is the meaning of life?\")\\n```\\n\\n## Orchestration \\n\\nWhile standardization for individual components is useful, we\\'ve increasingly seen that developers want to *combine* components into more complex applications. \\nThis motivates the need for [orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)).\\nThere are several common characteristics of LLM applications that this orchestration layer should support:\\n\\n* **Complex control flow:** The application requires complex patterns such as cycles (e.g., a loop that reiterates until a condition is met).\\n* **[Persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/):** The application needs to maintain [short-term and / or long-term memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n* **[Human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/):** The application needs human interaction, e.g., pausing, reviewing, editing, approving certain steps.\\n\\nThe recommended way to orchestrate components for complex applications is [LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/).\\nLangGraph is a library that gives developers a high degree of control by expressing the flow of the application as a set of nodes and edges.\\nLangGraph comes with built-in support for [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), [memory](https://langchain-ai.github.io/langgraph/concepts/memory/), and other features.\\nIt\\'s particularly well suited for building [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/) or [multi-agent](https://langchain-ai.github.io/langgraph/concepts/multi_agent/) applications. \\nImportantly, individual LangChain components can be used as LangGraph nodes, but you can also use LangGraph **without** using LangChain components.\\n\\n:::info[Further reading]\\n\\nHave a look at our free course, [Introduction to LangGraph](https://academy.langchain.com/courses/intro-to-langgraph), to learn more about how to use LangGraph to build complex applications.\\n\\n:::\\n\\n## Observability and evaluation\\n\\nThe pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. \\nDevelopers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\nHigh quality tracing and evaluations can help you rapidly answer these types of questions with confidence.\\n[LangSmith](https://docs.smith.langchain.com/) is our platform that supports observability and evaluation for AI applications.\\nSee our conceptual guides on [evaluations](https://docs.smith.langchain.com/concepts/evaluation) and [tracing](https://docs.smith.langchain.com/concepts/tracing) for more details.\\n\\n:::info[Further reading]\\n\\nSee our video playlist on [LangSmith tracing and evaluations](https://youtube.com/playlist?list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S&feature=shared) for more details.\\n\\n:::\\n\\n## Conclusion\\n\\nLangChain offers standard interfaces for components that are central to many AI applications, which offers a few specific advantages:\\n- **Ease of swapping providers:** It allows you to swap out different component providers without having to change the underlying code.\\n- **Advanced features:** It provides common methods for more advanced features, such as [streaming](/docs/concepts/streaming) and [tool calling](/docs/concepts/tool_calling/).\\n\\n[LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/) makes it possible to orchestrate complex applications (e.g., [agents](/docs/concepts/agents/)) and provide features like including [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), or [memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n\\n[LangSmith](https://docs.smith.langchain.com/) makes it possible to iterate with confidence on your applications, by providing LLM-specific observability and framework for testing and evaluating your application.\\n'), Document(metadata={'file_name': 'introduction.mdx', 'file_path': 'docs\\\\docs\\\\introduction.mdx', 'file_type': '.mdx', 'source': 'docs\\\\docs\\\\introduction.mdx'}, page_content='---\\nsidebar_position: 0\\nsidebar_class_name: hidden\\n---\\n\\n# Introduction\\n\\n**LangChain** is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:\\n- **Development**: Build your applications using LangChain\\'s open-source [components](/docs/concepts) and [third-party integrations](/docs/integrations/providers/).\\nUse [LangGraph](/docs/concepts/architecture/#langgraph) to build stateful agents with first-class streaming and human-in-the-loop support.\\n- **Productionization**: Use [LangSmith](https://docs.smith.langchain.com/) to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\n- **Deployment**: Turn your LangGraph applications into production-ready APIs and Assistants with [LangGraph Platform](https://langchain-ai.github.io/langgraph/cloud/).\\n\\nimport ThemedImage from \\'@theme/ThemedImage\\';\\nimport useBaseUrl from \\'@docusaurus/useBaseUrl\\';\\n\\n<ThemedImage\\n  alt=\"Diagram outlining the hierarchical organization of the LangChain framework, displaying the interconnected parts across multiple layers.\"\\n  sources={{\\n    light: useBaseUrl(\\'/svg/langchain_stack_112024.svg\\'),\\n    dark: useBaseUrl(\\'/svg/langchain_stack_112024_dark.svg\\'),\\n  }}\\n  style={{ width: \"100%\" }}\\n  title=\"LangChain Framework Overview\"\\n/>\\n\\nLangChain implements a standard interface for large language models and related\\ntechnologies, such as embedding models and vector stores, and integrates with\\nhundreds of providers. See the [integrations](/docs/integrations/providers/) page for\\nmore.\\n\\nimport ChatModelTabs from \"@theme/ChatModelTabs\";\\n\\n<ChatModelTabs/>\\n\\n```python\\nmodel.invoke(\"Hello, world!\")\\n```\\n\\n:::note\\n\\nThese docs focus on the Python LangChain library. [Head here](https://js.langchain.com) for docs on the JavaScript LangChain library.\\n\\n:::\\n\\n## Architecture\\n\\nThe LangChain framework consists of multiple open-source libraries. Read more in the\\n[Architecture](/docs/concepts/architecture/) page.\\n\\n- **`langchain-core`**: Base abstractions for chat models and other components.\\n- **Integration packages** (e.g. `langchain-openai`, `langchain-anthropic`, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\\n- **`langchain`**: Chains, agents, and retrieval strategies that make up an application\\'s cognitive architecture.\\n- **`langchain-community`**: Third-party integrations that are community maintained.\\n- **`langgraph`**: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See [LangGraph documentation](https://langchain-ai.github.io/langgraph/).\\n\\n## Guides\\n\\n### [Tutorials](/docs/tutorials)\\n\\nIf you\\'re looking to build something specific or are more of a hands-on learner, check out our [tutorials section](/docs/tutorials).\\nThis is the best place to get started.\\n\\nThese are the best ones to get started with:\\n\\n- [Build a Simple LLM Application](/docs/tutorials/llm_chain)\\n- [Build a Chatbot](/docs/tutorials/chatbot)\\n- [Build an Agent](/docs/tutorials/agents)\\n- [Introduction to LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/)\\n\\nExplore the full list of LangChain tutorials [here](/docs/tutorials), and check out other [LangGraph tutorials here](https://langchain-ai.github.io/langgraph/tutorials/). To learn more about LangGraph, check out our first LangChain Academy course, *Introduction to LangGraph*, available [here](https://academy.langchain.com/courses/intro-to-langgraph).\\n\\n\\n### [How-to guides](/docs/how_to)\\n\\n[Here](/docs/how_to) you’ll find short answers to “How do I….?” types of questions.\\nThese how-to guides don’t cover topics in depth – you’ll find that material in the [Tutorials](/docs/tutorials) and the [API Reference](https://python.langchain.com/api_reference/).\\nHowever, these guides will help you quickly accomplish common tasks using [chat models](/docs/how_to/#chat-models),\\n[vector stores](/docs/how_to/#vector-stores), and other common LangChain components.\\n\\nCheck out [LangGraph-specific how-tos here](https://langchain-ai.github.io/langgraph/how-tos/).\\n\\n### [Conceptual guide](/docs/concepts)\\n\\nIntroductions to all the key parts of LangChain you’ll need to know! [Here](/docs/concepts) you\\'ll find high level explanations of all LangChain concepts.\\n\\nFor a deeper dive into LangGraph concepts, check out [this page](https://langchain-ai.github.io/langgraph/concepts/).\\n\\n### [Integrations](integrations/providers/index.mdx)\\n\\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\\nIf you\\'re looking to get up and running quickly with [chat models](/docs/integrations/chat/), [vector stores](/docs/integrations/vectorstores/),\\nor other LangChain components from a specific provider, check out our growing list of [integrations](/docs/integrations/providers/).\\n\\n\\n### [API reference](https://python.langchain.com/api_reference/)\\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\\n\\n## Ecosystem\\n\\n### [🦜🛠️ LangSmith](https://docs.smith.langchain.com)\\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\\n\\n### [🦜🕸️ LangGraph](https://langchain-ai.github.io/langgraph)\\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by Linkedin, Uber, Klarna, GitLab, and many more.\\n\\n## Additional resources\\n\\n### [Versions](/docs/versions/v0_3/)\\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\\n\\n### [Security](/docs/security)\\nRead up on [security](/docs/security) best practices to make sure you\\'re developing safely with LangChain.\\n\\n### [Contributing](contributing/index.mdx)\\nCheck out the developer\\'s guide for guidelines on contributing and help getting your dev environment set up.\\n'), Document(metadata={'file_name': 'lcel.mdx', 'file_path': 'docs\\\\docs\\\\concepts\\\\lcel.mdx', 'file_type': '.mdx', 'source': 'docs\\\\docs\\\\concepts\\\\lcel.mdx'}, page_content='# LangChain Expression Language (LCEL)\\n\\n:::info Prerequisites\\n* [Runnable Interface](/docs/concepts/runnables)\\n:::\\n\\nThe **L**ang**C**hain **E**xpression **L**anguage (LCEL) takes a [declarative](https://en.wikipedia.org/wiki/Declarative_programming) approach to building new [Runnables](/docs/concepts/runnables) from existing Runnables.\\n\\nThis means that you describe what *should* happen, rather than *how* it should happen, allowing LangChain to optimize the run-time execution of the chains.\\n\\nWe often refer to a `Runnable` created using LCEL as a \"chain\". It\\'s important to remember that a \"chain\" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).\\n\\n:::note\\n* The [LCEL cheatsheet](/docs/how_to/lcel_cheatsheet/) shows common patterns that involve the Runnable interface and LCEL expressions.\\n* Please see the following list of [how-to guides](/docs/how_to/#langchain-expression-language-lcel) that cover common tasks with LCEL.\\n* A list of built-in `Runnables` can be found in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html). Many of these Runnables are useful when composing custom \"chains\" in LangChain using LCEL.\\n:::\\n\\n## Benefits of LCEL\\n\\nLangChain optimizes the run-time execution of chains built with LCEL in a number of ways:\\n\\n- **Optimized parallel execution**: Run Runnables in parallel using [RunnableParallel](#runnableparallel) or run multiple inputs through a given chain in parallel using the [Runnable Batch API](/docs/concepts/runnables/#optimized-parallel-execution-batch). Parallel execution can significantly reduce the latency as processing can be done in parallel instead of sequentially.\\n- **Guaranteed Async support**: Any chain built with LCEL can be run asynchronously using the [Runnable Async API](/docs/concepts/runnables/#asynchronous-support). This can be useful when running chains in a server environment where you want to handle large number of requests concurrently.\\n- **Simplify streaming**: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(time elapsed until the first chunk of output from a [chat model](/docs/concepts/chat_models) or [llm](/docs/concepts/text_llms) comes out).\\n\\nOther benefits include:\\n\\n- [**Seamless LangSmith tracing**](https://docs.smith.langchain.com)\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, **all** steps are automatically logged to [LangSmith](https://docs.smith.langchain.com/) for maximum observability and debuggability.\\n- **Standard API**: Because all chains are built using the Runnable interface, they can be used in the same way as any other Runnable.\\n- [**Deployable with LangServe**](/docs/concepts/architecture#langserve): Chains built with LCEL can be deployed using for production use.\\n\\n## Should I use LCEL?\\n\\nLCEL is an [orchestration solution](https://en.wikipedia.org/wiki/Orchestration_(computing)) -- it allows LangChain to handle run-time execution of chains in an optimized way.\\n\\nWhile we have seen users run chains with hundreds of steps in production, we generally recommend using LCEL for simpler orchestration tasks. When the application requires complex state management, branching, cycles or multiple agents, we recommend that users take advantage of [LangGraph](/docs/concepts/architecture#langgraph).\\n\\nIn LangGraph, users define graphs that specify the application\\'s flow. This allows users to keep using LCEL within individual nodes when LCEL is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.\\n\\nHere are some guidelines:\\n\\n* If you are making a single LLM call, you don\\'t need LCEL; instead call the underlying [chat model](/docs/concepts/chat_models) directly.\\n* If you have a simple chain (e.g., prompt + llm + parser, simple retrieval set up etc.), LCEL is a reasonable fit, if you\\'re taking advantage of the LCEL benefits.\\n* If you\\'re building a complex chain (e.g., with branching, cycles, multiple agents, etc.) use [LangGraph](/docs/concepts/architecture#langgraph) instead. Remember that you can always use LCEL within individual nodes in LangGraph.\\n\\n## Composition Primitives\\n\\n`LCEL` chains are built by composing existing `Runnables` together. The two main composition primitives are [RunnableSequence](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableSequence.html#langchain_core.runnables.base.RunnableSequence) and [RunnableParallel](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableParallel.html#langchain_core.runnables.base.RunnableParallel).\\n\\nMany other composition primitives (e.g., [RunnableAssign](\\nhttps://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnableAssign.html#langchain_core.runnables.passthrough.RunnableAssign\\n)) can be thought of as variations of these two primitives.\\n\\n:::note\\nYou can find a list of all composition primitives in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html).\\n:::\\n\\n### RunnableSequence\\n\\n`RunnableSequence` is a composition primitive that allows you \"chain\" multiple runnables sequentially, with the output of one runnable serving as the input to the next.\\n\\n```python\\nfrom langchain_core.runnables import RunnableSequence\\nchain = RunnableSequence([runnable1, runnable2])\\n```\\n\\nInvoking the `chain` with some input:\\n\\n```python\\nfinal_output = chain.invoke(some_input)\\n```\\n\\ncorresponds to the following:\\n\\n```python\\noutput1 = runnable1.invoke(some_input)\\nfinal_output = runnable2.invoke(output1)\\n```\\n\\n:::note\\n`runnable1` and `runnable2` are placeholders for any `Runnable` that you want to chain together.\\n:::\\n\\n### RunnableParallel\\n\\n`RunnableParallel` is a composition primitive that allows you to run multiple runnables concurrently, with the same input provided to each.\\n\\n```python\\nfrom langchain_core.runnables import RunnableParallel\\nchain = RunnableParallel({\\n    \"key1\": runnable1,\\n    \"key2\": runnable2,\\n})\\n```\\n\\nInvoking the `chain` with some input:\\n\\n```python\\nfinal_output = chain.invoke(some_input)\\n```\\n\\nWill yield a `final_output` dictionary with the same keys as the input dictionary, but with the values replaced by the output of the corresponding runnable.\\n\\n```python\\n{\\n    \"key1\": runnable1.invoke(some_input),\\n    \"key2\": runnable2.invoke(some_input),\\n}\\n```\\n\\nRecall, that the runnables are executed in parallel, so while the result is the same as\\ndictionary comprehension shown above, the execution time is much faster.\\n\\n:::note\\n`RunnableParallel`supports both synchronous and asynchronous execution (as all `Runnables` do).\\n\\n* For synchronous execution, `RunnableParallel` uses a [ThreadPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor) to run the runnables concurrently.\\n* For asynchronous execution, `RunnableParallel` uses [asyncio.gather](https://docs.python.org/3/library/asyncio.html#asyncio.gather) to run the runnables concurrently.\\n:::\\n\\n## Composition Syntax\\n\\nThe usage of `RunnableSequence` and `RunnableParallel` is so common that we created a shorthand syntax for using them. This helps\\nto make the code more readable and concise.\\n\\n### The `|` operator\\n\\nWe have [overloaded](https://docs.python.org/3/reference/datamodel.html#special-method-names) the `|` operator to create a `RunnableSequence` from two `Runnables`.\\n\\n```python\\nchain = runnable1 | runnable2\\n```\\n\\nis Equivalent to:\\n\\n```python\\nchain = RunnableSequence([runnable1, runnable2])\\n```\\n\\n### The `.pipe` method\\n\\nIf you have moral qualms with operator overloading, you can use the `.pipe` method instead. This is equivalent to the `|` operator.\\n\\n```python\\nchain = runnable1.pipe(runnable2)\\n```\\n\\n### Coercion\\n\\nLCEL applies automatic type coercion to make it easier to compose chains.\\n\\nIf you do not understand the type coercion, you can always use the `RunnableSequence` and `RunnableParallel` classes directly.\\n\\nThis will make the code more verbose, but it will also make it more explicit.\\n\\n#### Dictionary to RunnableParallel\\n\\nInside an LCEL expression, a dictionary is automatically converted to a `RunnableParallel`.\\n\\nFor example, the following code:\\n\\n```python\\nmapping = {\\n    \"key1\": runnable1,\\n    \"key2\": runnable2,\\n}\\n\\nchain = mapping | runnable3\\n```\\n\\nIt gets automatically converted to the following:\\n\\n```python\\nchain = RunnableSequence([RunnableParallel(mapping), runnable3])\\n```\\n\\n:::caution\\nYou have to be careful because the `mapping` dictionary is not a `RunnableParallel` object, it is just a dictionary. This means that the following code will raise an `AttributeError`:\\n\\n```python\\nmapping.invoke(some_input)\\n```\\n:::\\n\\n#### Function to RunnableLambda\\n\\nInside an LCEL expression, a function is automatically converted to a `RunnableLambda`.\\n\\n```\\ndef some_func(x):\\n    return x\\n\\nchain = some_func | runnable1\\n```\\n\\nIt gets automatically converted to the following:\\n\\n```python\\nchain = RunnableSequence([RunnableLambda(some_func), runnable1])\\n```\\n\\n:::caution\\nYou have to be careful because the lambda function is not a `RunnableLambda` object, it is just a function. This means that the following code will raise an `AttributeError`:\\n\\n```python\\nlambda x: x + 1.invoke(some_input)\\n```\\n:::\\n\\n## Legacy chains\\n\\nLCEL aims to provide consistency around behavior and customization over legacy subclassed chains such as `LLMChain` and\\n`ConversationalRetrievalChain`. Many of these legacy chains hide important details like prompts, and as a wider variety\\nof viable models emerge, customization has become more and more important.\\n\\nIf you are currently using one of these legacy chains, please see [this guide for guidance on how to migrate](/docs/versions/migrating_chains).\\n\\nFor guides on how to do specific tasks with LCEL, check out [the relevant how-to guides](/docs/how_to/#langchain-expression-language-lcel).\\n'), Document(metadata={'file_name': 'index.mdx', 'file_path': 'docs\\\\docs\\\\how_to\\\\index.mdx', 'file_type': '.mdx', 'source': 'docs\\\\docs\\\\how_to\\\\index.mdx'}, page_content='---\\nsidebar_position: 0\\nsidebar_class_name: hidden\\n---\\n\\n# How-to guides\\n\\nHere you’ll find answers to “How do I….?” types of questions.\\nThese guides are *goal-oriented* and *concrete*; they\\'re meant to help you complete a specific task.\\nFor conceptual explanations see the [Conceptual guide](/docs/concepts/).\\nFor end-to-end walkthroughs see [Tutorials](/docs/tutorials).\\nFor comprehensive descriptions of every class and function see the [API Reference](https://python.langchain.com/api_reference/).\\n\\n## Installation\\n\\n- [How to: install LangChain packages](/docs/how_to/installation/)\\n- [How to: use LangChain with different Pydantic versions](/docs/how_to/pydantic_compatibility)\\n\\n## Key features\\n\\nThis highlights functionality that is core to using LangChain.\\n\\n- [How to: return structured data from a model](/docs/how_to/structured_output/)\\n- [How to: use a model to call tools](/docs/how_to/tool_calling)\\n- [How to: stream runnables](/docs/how_to/streaming)\\n- [How to: debug your LLM apps](/docs/how_to/debugging/)\\n\\n## Components\\n\\nThese are the core building blocks you can use when building applications.\\n\\n### Chat models\\n\\n[Chat Models](/docs/concepts/chat_models) are newer forms of language models that take messages in and output a message.\\nSee [supported integrations](/docs/integrations/chat/) for details on getting started with chat models from a specific provider.\\n\\n- [How to: do function/tool calling](/docs/how_to/tool_calling)\\n- [How to: get models to return structured output](/docs/how_to/structured_output)\\n- [How to: cache model responses](/docs/how_to/chat_model_caching)\\n- [How to: get log probabilities](/docs/how_to/logprobs)\\n- [How to: create a custom chat model class](/docs/how_to/custom_chat_model)\\n- [How to: stream a response back](/docs/how_to/chat_streaming)\\n- [How to: track token usage](/docs/how_to/chat_token_usage_tracking)\\n- [How to: track response metadata across providers](/docs/how_to/response_metadata)\\n- [How to: use chat model to call tools](/docs/how_to/tool_calling)\\n- [How to: stream tool calls](/docs/how_to/tool_streaming)\\n- [How to: handle rate limits](/docs/how_to/chat_model_rate_limiting)\\n- [How to: few shot prompt tool behavior](/docs/how_to/tools_few_shot)\\n- [How to: bind model-specific formatted tools](/docs/how_to/tools_model_specific)\\n- [How to: force a specific tool call](/docs/how_to/tool_choice)\\n- [How to: work with local models](/docs/how_to/local_llms)\\n- [How to: init any model in one line](/docs/how_to/chat_models_universal_init/)\\n- [How to: pass multimodal data directly to models](/docs/how_to/multimodal_inputs/)\\n\\n### Messages\\n\\n[Messages](/docs/concepts/messages) are the input and output of chat models. They have some `content` and a `role`, which describes the source of the message.\\n\\n- [How to: trim messages](/docs/how_to/trim_messages/)\\n- [How to: filter messages](/docs/how_to/filter_messages/)\\n- [How to: merge consecutive messages of the same type](/docs/how_to/merge_message_runs/)\\n\\n### Prompt templates\\n\\n[Prompt Templates](/docs/concepts/prompt_templates) are responsible for formatting user input into a format that can be passed to a language model.\\n\\n- [How to: use few shot examples](/docs/how_to/few_shot_examples)\\n- [How to: use few shot examples in chat models](/docs/how_to/few_shot_examples_chat/)\\n- [How to: partially format prompt templates](/docs/how_to/prompts_partial)\\n- [How to: compose prompts together](/docs/how_to/prompts_composition)\\n- [How to: use multimodal prompts](/docs/how_to/multimodal_prompts/)\\n\\n### Example selectors\\n\\n[Example Selectors](/docs/concepts/example_selectors) are responsible for selecting the correct few shot examples to pass to the prompt.\\n\\n- [How to: use example selectors](/docs/how_to/example_selectors)\\n- [How to: select examples by length](/docs/how_to/example_selectors_length_based)\\n- [How to: select examples by semantic similarity](/docs/how_to/example_selectors_similarity)\\n- [How to: select examples by semantic ngram overlap](/docs/how_to/example_selectors_ngram)\\n- [How to: select examples by maximal marginal relevance](/docs/how_to/example_selectors_mmr)\\n- [How to: select examples from LangSmith few-shot datasets](/docs/how_to/example_selectors_langsmith/)\\n\\n### LLMs\\n\\nWhat LangChain calls [LLMs](/docs/concepts/text_llms) are older forms of language models that take a string in and output a string.\\n\\n- [How to: cache model responses](/docs/how_to/llm_caching)\\n- [How to: create a custom LLM class](/docs/how_to/custom_llm)\\n- [How to: stream a response back](/docs/how_to/streaming_llm)\\n- [How to: track token usage](/docs/how_to/llm_token_usage_tracking)\\n- [How to: work with local models](/docs/how_to/local_llms)\\n\\n### Output parsers\\n\\n[Output Parsers](/docs/concepts/output_parsers) are responsible for taking the output of an LLM and parsing into more structured format.\\n\\n- [How to: parse text from message objects](/docs/how_to/output_parser_string)\\n- [How to: use output parsers to parse an LLM response into structured format](/docs/how_to/output_parser_structured)\\n- [How to: parse JSON output](/docs/how_to/output_parser_json)\\n- [How to: parse XML output](/docs/how_to/output_parser_xml)\\n- [How to: parse YAML output](/docs/how_to/output_parser_yaml)\\n- [How to: retry when output parsing errors occur](/docs/how_to/output_parser_retry)\\n- [How to: try to fix errors in output parsing](/docs/how_to/output_parser_fixing)\\n- [How to: write a custom output parser class](/docs/how_to/output_parser_custom)\\n\\n### Document loaders\\n\\n[Document Loaders](/docs/concepts/document_loaders) are responsible for loading documents from a variety of sources.\\n\\n- [How to: load PDF files](/docs/how_to/document_loader_pdf)\\n- [How to: load web pages](/docs/how_to/document_loader_web)\\n- [How to: load CSV data](/docs/how_to/document_loader_csv)\\n- [How to: load data from a directory](/docs/how_to/document_loader_directory)\\n- [How to: load HTML data](/docs/how_to/document_loader_html)\\n- [How to: load JSON data](/docs/how_to/document_loader_json)\\n- [How to: load Markdown data](/docs/how_to/document_loader_markdown)\\n- [How to: load Microsoft Office data](/docs/how_to/document_loader_office_file)\\n- [How to: write a custom document loader](/docs/how_to/document_loader_custom)\\n\\n### Text splitters\\n\\n[Text Splitters](/docs/concepts/text_splitters) take a document and split into chunks that can be used for retrieval.\\n\\n- [How to: recursively split text](/docs/how_to/recursive_text_splitter)\\n- [How to: split HTML](/docs/how_to/split_html)\\n- [How to: split by character](/docs/how_to/character_text_splitter)\\n- [How to: split code](/docs/how_to/code_splitter)\\n- [How to: split Markdown by headers](/docs/how_to/markdown_header_metadata_splitter)\\n- [How to: recursively split JSON](/docs/how_to/recursive_json_splitter)\\n- [How to: split text into semantic chunks](/docs/how_to/semantic-chunker)\\n- [How to: split by tokens](/docs/how_to/split_by_token)\\n\\n### Embedding models\\n\\n[Embedding Models](/docs/concepts/embedding_models) take a piece of text and create a numerical representation of it.\\nSee [supported integrations](/docs/integrations/text_embedding/) for details on getting started with embedding models from a specific provider.\\n\\n- [How to: embed text data](/docs/how_to/embed_text)\\n- [How to: cache embedding results](/docs/how_to/caching_embeddings)\\n- [How to: create a custom embeddings class](/docs/how_to/custom_embeddings)\\n\\n### Vector stores\\n\\n[Vector stores](/docs/concepts/vectorstores) are databases that can efficiently store and retrieve embeddings.\\nSee [supported integrations](/docs/integrations/vectorstores/) for details on getting started with vector stores from a specific provider.\\n\\n- [How to: use a vector store to retrieve data](/docs/how_to/vectorstores)\\n\\n### Retrievers\\n\\n[Retrievers](/docs/concepts/retrievers) are responsible for taking a query and returning relevant documents.\\n\\n- [How to: use a vector store to retrieve data](/docs/how_to/vectorstore_retriever)\\n- [How to: generate multiple queries to retrieve data for](/docs/how_to/MultiQueryRetriever)\\n- [How to: use contextual compression to compress the data retrieved](/docs/how_to/contextual_compression)\\n- [How to: write a custom retriever class](/docs/how_to/custom_retriever)\\n- [How to: add similarity scores to retriever results](/docs/how_to/add_scores_retriever)\\n- [How to: combine the results from multiple retrievers](/docs/how_to/ensemble_retriever)\\n- [How to: reorder retrieved results to mitigate the \"lost in the middle\" effect](/docs/how_to/long_context_reorder)\\n- [How to: generate multiple embeddings per document](/docs/how_to/multi_vector)\\n- [How to: retrieve the whole document for a chunk](/docs/how_to/parent_document_retriever)\\n- [How to: generate metadata filters](/docs/how_to/self_query)\\n- [How to: create a time-weighted retriever](/docs/how_to/time_weighted_vectorstore)\\n- [How to: use hybrid vector and keyword retrieval](/docs/how_to/hybrid)\\n\\n### Indexing\\n\\nIndexing is the process of keeping your vectorstore in-sync with the underlying data source.\\n\\n- [How to: reindex data to keep your vectorstore in-sync with the underlying data source](/docs/how_to/indexing)\\n\\n### Tools\\n\\nLangChain [Tools](/docs/concepts/tools) contain a description of the tool (to pass to the language model) as well as the implementation of the function to call. Refer [here](/docs/integrations/tools/) for a list of pre-built tools. \\n\\n- [How to: create tools](/docs/how_to/custom_tools)\\n- [How to: use built-in tools and toolkits](/docs/how_to/tools_builtin)\\n- [How to: use chat models to call tools](/docs/how_to/tool_calling)\\n- [How to: pass tool outputs to chat models](/docs/how_to/tool_results_pass_to_model)\\n- [How to: pass run time values to tools](/docs/how_to/tool_runtime)\\n- [How to: add a human-in-the-loop for tools](/docs/how_to/tools_human)\\n- [How to: handle tool errors](/docs/how_to/tools_error)\\n- [How to: force models to call a tool](/docs/how_to/tool_choice)\\n- [How to: disable parallel tool calling](/docs/how_to/tool_calling_parallel)\\n- [How to: access the `RunnableConfig` from a tool](/docs/how_to/tool_configure)\\n- [How to: stream events from a tool](/docs/how_to/tool_stream_events)\\n- [How to: return artifacts from a tool](/docs/how_to/tool_artifacts/)\\n- [How to: convert Runnables to tools](/docs/how_to/convert_runnable_to_tool)\\n- [How to: add ad-hoc tool calling capability to models](/docs/how_to/tools_prompting)\\n- [How to: pass in runtime secrets](/docs/how_to/runnable_runtime_secrets)\\n\\n### Multimodal\\n\\n- [How to: pass multimodal data directly to models](/docs/how_to/multimodal_inputs/)\\n- [How to: use multimodal prompts](/docs/how_to/multimodal_prompts/)\\n\\n\\n### Agents\\n\\n:::note\\n\\nFor in depth how-to guides for agents, please check out [LangGraph](https://langchain-ai.github.io/langgraph/) documentation.\\n\\n:::\\n\\n- [How to: use legacy LangChain Agents (AgentExecutor)](/docs/how_to/agent_executor)\\n- [How to: migrate from legacy LangChain agents to LangGraph](/docs/how_to/migrate_agent)\\n\\n### Callbacks\\n\\n[Callbacks](/docs/concepts/callbacks) allow you to hook into the various stages of your LLM application\\'s execution.\\n\\n- [How to: pass in callbacks at runtime](/docs/how_to/callbacks_runtime)\\n- [How to: attach callbacks to a module](/docs/how_to/callbacks_attach)\\n- [How to: pass callbacks into a module constructor](/docs/how_to/callbacks_constructor)\\n- [How to: create custom callback handlers](/docs/how_to/custom_callbacks)\\n- [How to: use callbacks in async environments](/docs/how_to/callbacks_async)\\n- [How to: dispatch custom callback events](/docs/how_to/callbacks_custom_events)\\n\\n### Custom\\n\\nAll of LangChain components can easily be extended to support your own versions.\\n\\n- [How to: create a custom chat model class](/docs/how_to/custom_chat_model)\\n- [How to: create a custom LLM class](/docs/how_to/custom_llm)\\n- [How to: create a custom embeddings class](/docs/how_to/custom_embeddings)\\n- [How to: write a custom retriever class](/docs/how_to/custom_retriever)\\n- [How to: write a custom document loader](/docs/how_to/document_loader_custom)\\n- [How to: write a custom output parser class](/docs/how_to/output_parser_custom)\\n- [How to: create custom callback handlers](/docs/how_to/custom_callbacks)\\n- [How to: define a custom tool](/docs/how_to/custom_tools)\\n- [How to: dispatch custom callback events](/docs/how_to/callbacks_custom_events)\\n\\n### Serialization\\n- [How to: save and load LangChain objects](/docs/how_to/serialization)\\n\\n## Use cases\\n\\nThese guides cover use-case specific details.\\n\\n### Q&A with RAG\\n\\nRetrieval Augmented Generation (RAG) is a way to connect LLMs to external sources of data.\\nFor a high-level tutorial on RAG, check out [this guide](/docs/tutorials/rag/).\\n\\n- [How to: add chat history](/docs/how_to/qa_chat_history_how_to/)\\n- [How to: stream](/docs/how_to/qa_streaming/)\\n- [How to: return sources](/docs/how_to/qa_sources/)\\n- [How to: return citations](/docs/how_to/qa_citations/)\\n- [How to: do per-user retrieval](/docs/how_to/qa_per_user/)\\n\\n\\n### Extraction\\n\\nExtraction is when you use LLMs to extract structured information from unstructured text.\\nFor a high level tutorial on extraction, check out [this guide](/docs/tutorials/extraction/).\\n\\n- [How to: use reference examples](/docs/how_to/extraction_examples/)\\n- [How to: handle long text](/docs/how_to/extraction_long_text/)\\n- [How to: do extraction without using function calling](/docs/how_to/extraction_parse)\\n\\n### Chatbots\\n\\nChatbots involve using an LLM to have a conversation.\\nFor a high-level tutorial on building chatbots, check out [this guide](/docs/tutorials/chatbot/).\\n\\n- [How to: manage memory](/docs/how_to/chatbots_memory)\\n- [How to: do retrieval](/docs/how_to/chatbots_retrieval)\\n- [How to: use tools](/docs/how_to/chatbots_tools)\\n- [How to: manage large chat history](/docs/how_to/trim_messages/)\\n\\n### Query analysis\\n\\nQuery Analysis is the task of using an LLM to generate a query to send to a retriever.\\nFor a high-level tutorial on query analysis, check out [this guide](/docs/tutorials/rag/#query-analysis).\\n\\n- [How to: add examples to the prompt](/docs/how_to/query_few_shot)\\n- [How to: handle cases where no queries are generated](/docs/how_to/query_no_queries)\\n- [How to: handle multiple queries](/docs/how_to/query_multiple_queries)\\n- [How to: handle multiple retrievers](/docs/how_to/query_multiple_retrievers)\\n- [How to: construct filters](/docs/how_to/query_constructing_filters)\\n- [How to: deal with high cardinality categorical variables](/docs/how_to/query_high_cardinality)\\n\\n### Q&A over SQL + CSV\\n\\nYou can use LLMs to do question answering over tabular data.\\nFor a high-level tutorial, check out [this guide](/docs/tutorials/sql_qa/).\\n\\n- [How to: use prompting to improve results](/docs/how_to/sql_prompting)\\n- [How to: do query validation](/docs/how_to/sql_query_checking)\\n- [How to: deal with large databases](/docs/how_to/sql_large_db)\\n- [How to: deal with CSV files](/docs/how_to/sql_csv)\\n\\n### Q&A over graph databases\\n\\nYou can use an LLM to do question answering over graph databases.\\nFor a high-level tutorial, check out [this guide](/docs/tutorials/graph/).\\n\\n- [How to: add a semantic layer over the database](/docs/how_to/graph_semantic)\\n- [How to: construct knowledge graphs](/docs/how_to/graph_constructing)\\n\\n### Summarization\\n\\nLLMs can summarize and otherwise distill desired information from text, including\\nlarge volumes of text. For a high-level tutorial, check out [this guide](/docs/tutorials/summarization).\\n\\n- [How to: summarize text in a single LLM call](/docs/how_to/summarize_stuff)\\n- [How to: summarize text through parallelization](/docs/how_to/summarize_map_reduce)\\n- [How to: summarize text through iterative refinement](/docs/how_to/summarize_refine)\\n\\n## LangChain Expression Language (LCEL)\\n\\n:::note Should I use LCEL?\\n\\nLCEL is an orchestration solution. See our\\n[concepts page](/docs/concepts/lcel/#should-i-use-lcel) for recommendations on when to\\nuse LCEL.\\n\\n:::\\n\\n[LangChain Expression Language](/docs/concepts/lcel) is a way to create arbitrary custom chains. It is built on the [Runnable](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html) protocol.\\n\\n[**LCEL cheatsheet**](/docs/how_to/lcel_cheatsheet/): For a quick overview of how to use the main LCEL primitives.\\n\\n[**Migration guide**](/docs/versions/migrating_chains): For migrating legacy chain abstractions to LCEL.\\n\\n- [How to: chain runnables](/docs/how_to/sequence)\\n- [How to: stream runnables](/docs/how_to/streaming)\\n- [How to: invoke runnables in parallel](/docs/how_to/parallel/)\\n- [How to: add default invocation args to runnables](/docs/how_to/binding/)\\n- [How to: turn any function into a runnable](/docs/how_to/functions)\\n- [How to: pass through inputs from one chain step to the next](/docs/how_to/passthrough)\\n- [How to: configure runnable behavior at runtime](/docs/how_to/configure)\\n- [How to: add message history (memory) to a chain](/docs/how_to/message_history)\\n- [How to: route between sub-chains](/docs/how_to/routing)\\n- [How to: create a dynamic (self-constructing) chain](/docs/how_to/dynamic_chain/)\\n- [How to: inspect runnables](/docs/how_to/inspect)\\n- [How to: add fallbacks to a runnable](/docs/how_to/fallbacks)\\n- [How to: pass runtime secrets to a runnable](/docs/how_to/runnable_runtime_secrets)\\n\\n## [LangGraph](https://langchain-ai.github.io/langgraph)\\n\\nLangGraph is an extension of LangChain aimed at\\nbuilding robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.\\n\\nLangGraph documentation is currently hosted on a separate site.\\nYou can peruse [LangGraph how-to guides here](https://langchain-ai.github.io/langgraph/how-tos/).\\n\\n## [LangSmith](https://docs.smith.langchain.com/)\\n\\nLangSmith allows you to closely trace, monitor and evaluate your LLM application.\\nIt seamlessly integrates with LangChain and LangGraph, and you can use it to inspect and debug individual steps of your chains and agents as you build.\\n\\nLangSmith documentation is hosted on a separate site.\\nYou can peruse [LangSmith how-to guides here](https://docs.smith.langchain.com/), but we\\'ll highlight a few sections that are particularly\\nrelevant to LangChain below:\\n\\n### Evaluation\\n<span data-heading-keywords=\"evaluation,evaluate\"></span>\\n\\nEvaluating performance is a vital part of building LLM-powered applications.\\nLangSmith helps with every step of the process from creating a dataset to defining metrics to running evaluators.\\n\\nTo learn more, check out the [LangSmith evaluation how-to guides](https://docs.smith.langchain.com/how_to_guides#evaluation).\\n\\n### Tracing\\n<span data-heading-keywords=\"trace,tracing\"></span>\\n\\nTracing gives you observability inside your chains and agents, and is vital in diagnosing issues.\\n\\n- [How to: trace with LangChain](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain)\\n- [How to: add metadata and tags to traces](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#add-metadata-and-tags-to-traces)\\n\\nYou can see general tracing-related how-tos [in this section of the LangSmith docs](https://docs.smith.langchain.com/how_to_guides/tracing).\\n')]\n",
      "rank:0\n",
      "content_score_mapping:{'# Why LangChain?\\n\\nThe goal of `langchain` the Python package and LangChain the company is to make it as easy as possible for developers to build applications that reason.\\nWhile LangChain originally started as a single open source package, it has evolved into a company and a whole ecosystem.\\nThis page will talk about the LangChain ecosystem as a whole.\\nMost of the components within the LangChain ecosystem can be used by themselves - so if you feel particularly drawn to certain components but not others, that is totally fine! Pick and choose whichever components you like best for your own use case!\\n\\n## Features\\n\\nThere are several primary needs that LangChain aims to address:\\n\\n1. **Standardized component interfaces:** The growing number of [models](/docs/integrations/chat/) and [related components](/docs/integrations/vectorstores/) for AI applications has resulted in a wide variety of different APIs that developers need to learn and use.\\nThis diversity can make it challenging for developers to switch between providers or combine components when building applications.\\nLangChain exposes a standard interface for key components, making it easy to switch between providers.\\n\\n2. **Orchestration:** As applications become more complex, combining multiple components and models, there\\'s [a growing need to efficiently connect these elements into control flows](https://lilianweng.github.io/posts/2023-06-23-agent/) that can [accomplish diverse tasks](https://www.sequoiacap.com/article/generative-ais-act-o1/).\\n[Orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)) is crucial for building such applications.\\n\\n3. **Observability and evaluation:** As applications become more complex, it becomes increasingly difficult to understand what is happening within them.\\nFurthermore, the pace of development can become rate-limited by the [paradox of choice](https://en.wikipedia.org/wiki/Paradox_of_choice).\\nFor example, developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\n[Observability](https://en.wikipedia.org/wiki/Observability) and evaluations can help developers monitor their applications and rapidly answer these types of questions with confidence.\\n\\n\\n## Standardized component interfaces\\n\\nLangChain provides common interfaces for components that are central to many AI applications.\\nAs an example, all [chat models](/docs/concepts/chat_models/) implement the [BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) interface.\\nThis provides a standard way to interact with chat models, supporting important but often provider-specific features like [tool calling](/docs/concepts/tool_calling/) and [structured outputs](/docs/concepts/structured_outputs/).\\n\\n\\n### Example: chat models \\n\\nMany [model providers](/docs/concepts/chat_models/) support [tool calling](/docs/concepts/tool_calling/), a critical feature for many applications (e.g., [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/)), that allows a developer to request model responses that match a particular schema.\\nThe APIs for each provider differ. \\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to bind [tools](/docs/concepts/tools) to a model in order to support [tool calling](/docs/concepts/tool_calling/):\\n\\n```python\\n# Tool creation\\ntools = [my_tool]\\n# Tool binding\\nmodel_with_tools = model.bind_tools(tools)\\n```\\n\\nSimilarly, getting models to produce [structured outputs](/docs/concepts/structured_outputs/) is an extremely common use case. \\nProviders support different approaches for this, including [JSON mode or tool calling](https://platform.openai.com/docs/guides/structured-outputs), with different APIs.\\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to produce structured outputs using the `with_structured_output()` method:\\n\\n```python\\n# Define schema\\nschema = ...\\n# Bind schema to model\\nmodel_with_structure = model.with_structured_output(schema)\\n```\\n\\n### Example: retrievers\\n\\nIn the context of [RAG](/docs/concepts/rag/) and LLM application components, LangChain\\'s [retriever](/docs/concepts/retrievers/) interface provides a standard way to connect to many different types of data services or databases (e.g., [vector stores](/docs/concepts/vectorstores) or databases).\\nThe underlying implementation of the retriever depends on the type of data store or database you are connecting to, but all retrievers implement the [runnable interface](/docs/concepts/runnables/), meaning they can be invoked in a common manner.\\n\\n```python\\ndocuments = my_retriever.invoke(\"What is the meaning of life?\")\\n```\\n\\n## Orchestration \\n\\nWhile standardization for individual components is useful, we\\'ve increasingly seen that developers want to *combine* components into more complex applications. \\nThis motivates the need for [orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)).\\nThere are several common characteristics of LLM applications that this orchestration layer should support:\\n\\n* **Complex control flow:** The application requires complex patterns such as cycles (e.g., a loop that reiterates until a condition is met).\\n* **[Persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/):** The application needs to maintain [short-term and / or long-term memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n* **[Human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/):** The application needs human interaction, e.g., pausing, reviewing, editing, approving certain steps.\\n\\nThe recommended way to orchestrate components for complex applications is [LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/).\\nLangGraph is a library that gives developers a high degree of control by expressing the flow of the application as a set of nodes and edges.\\nLangGraph comes with built-in support for [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), [memory](https://langchain-ai.github.io/langgraph/concepts/memory/), and other features.\\nIt\\'s particularly well suited for building [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/) or [multi-agent](https://langchain-ai.github.io/langgraph/concepts/multi_agent/) applications. \\nImportantly, individual LangChain components can be used as LangGraph nodes, but you can also use LangGraph **without** using LangChain components.\\n\\n:::info[Further reading]\\n\\nHave a look at our free course, [Introduction to LangGraph](https://academy.langchain.com/courses/intro-to-langgraph), to learn more about how to use LangGraph to build complex applications.\\n\\n:::\\n\\n## Observability and evaluation\\n\\nThe pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. \\nDevelopers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\nHigh quality tracing and evaluations can help you rapidly answer these types of questions with confidence.\\n[LangSmith](https://docs.smith.langchain.com/) is our platform that supports observability and evaluation for AI applications.\\nSee our conceptual guides on [evaluations](https://docs.smith.langchain.com/concepts/evaluation) and [tracing](https://docs.smith.langchain.com/concepts/tracing) for more details.\\n\\n:::info[Further reading]\\n\\nSee our video playlist on [LangSmith tracing and evaluations](https://youtube.com/playlist?list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S&feature=shared) for more details.\\n\\n:::\\n\\n## Conclusion\\n\\nLangChain offers standard interfaces for components that are central to many AI applications, which offers a few specific advantages:\\n- **Ease of swapping providers:** It allows you to swap out different component providers without having to change the underlying code.\\n- **Advanced features:** It provides common methods for more advanced features, such as [streaming](/docs/concepts/streaming) and [tool calling](/docs/concepts/tool_calling/).\\n\\n[LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/) makes it possible to orchestrate complex applications (e.g., [agents](/docs/concepts/agents/)) and provide features like including [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), or [memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n\\n[LangSmith](https://docs.smith.langchain.com/) makes it possible to iterate with confidence on your applications, by providing LLM-specific observability and framework for testing and evaluating your application.\\n': 0.05, '---\\nsidebar_position: 0\\nsidebar_class_name: hidden\\n---\\n\\n# Introduction\\n\\n**LangChain** is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:\\n- **Development**: Build your applications using LangChain\\'s open-source [components](/docs/concepts) and [third-party integrations](/docs/integrations/providers/).\\nUse [LangGraph](/docs/concepts/architecture/#langgraph) to build stateful agents with first-class streaming and human-in-the-loop support.\\n- **Productionization**: Use [LangSmith](https://docs.smith.langchain.com/) to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\n- **Deployment**: Turn your LangGraph applications into production-ready APIs and Assistants with [LangGraph Platform](https://langchain-ai.github.io/langgraph/cloud/).\\n\\nimport ThemedImage from \\'@theme/ThemedImage\\';\\nimport useBaseUrl from \\'@docusaurus/useBaseUrl\\';\\n\\n<ThemedImage\\n  alt=\"Diagram outlining the hierarchical organization of the LangChain framework, displaying the interconnected parts across multiple layers.\"\\n  sources={{\\n    light: useBaseUrl(\\'/svg/langchain_stack_112024.svg\\'),\\n    dark: useBaseUrl(\\'/svg/langchain_stack_112024_dark.svg\\'),\\n  }}\\n  style={{ width: \"100%\" }}\\n  title=\"LangChain Framework Overview\"\\n/>\\n\\nLangChain implements a standard interface for large language models and related\\ntechnologies, such as embedding models and vector stores, and integrates with\\nhundreds of providers. See the [integrations](/docs/integrations/providers/) page for\\nmore.\\n\\nimport ChatModelTabs from \"@theme/ChatModelTabs\";\\n\\n<ChatModelTabs/>\\n\\n```python\\nmodel.invoke(\"Hello, world!\")\\n```\\n\\n:::note\\n\\nThese docs focus on the Python LangChain library. [Head here](https://js.langchain.com) for docs on the JavaScript LangChain library.\\n\\n:::\\n\\n## Architecture\\n\\nThe LangChain framework consists of multiple open-source libraries. Read more in the\\n[Architecture](/docs/concepts/architecture/) page.\\n\\n- **`langchain-core`**: Base abstractions for chat models and other components.\\n- **Integration packages** (e.g. `langchain-openai`, `langchain-anthropic`, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\\n- **`langchain`**: Chains, agents, and retrieval strategies that make up an application\\'s cognitive architecture.\\n- **`langchain-community`**: Third-party integrations that are community maintained.\\n- **`langgraph`**: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See [LangGraph documentation](https://langchain-ai.github.io/langgraph/).\\n\\n## Guides\\n\\n### [Tutorials](/docs/tutorials)\\n\\nIf you\\'re looking to build something specific or are more of a hands-on learner, check out our [tutorials section](/docs/tutorials).\\nThis is the best place to get started.\\n\\nThese are the best ones to get started with:\\n\\n- [Build a Simple LLM Application](/docs/tutorials/llm_chain)\\n- [Build a Chatbot](/docs/tutorials/chatbot)\\n- [Build an Agent](/docs/tutorials/agents)\\n- [Introduction to LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/)\\n\\nExplore the full list of LangChain tutorials [here](/docs/tutorials), and check out other [LangGraph tutorials here](https://langchain-ai.github.io/langgraph/tutorials/). To learn more about LangGraph, check out our first LangChain Academy course, *Introduction to LangGraph*, available [here](https://academy.langchain.com/courses/intro-to-langgraph).\\n\\n\\n### [How-to guides](/docs/how_to)\\n\\n[Here](/docs/how_to) you’ll find short answers to “How do I….?” types of questions.\\nThese how-to guides don’t cover topics in depth – you’ll find that material in the [Tutorials](/docs/tutorials) and the [API Reference](https://python.langchain.com/api_reference/).\\nHowever, these guides will help you quickly accomplish common tasks using [chat models](/docs/how_to/#chat-models),\\n[vector stores](/docs/how_to/#vector-stores), and other common LangChain components.\\n\\nCheck out [LangGraph-specific how-tos here](https://langchain-ai.github.io/langgraph/how-tos/).\\n\\n### [Conceptual guide](/docs/concepts)\\n\\nIntroductions to all the key parts of LangChain you’ll need to know! [Here](/docs/concepts) you\\'ll find high level explanations of all LangChain concepts.\\n\\nFor a deeper dive into LangGraph concepts, check out [this page](https://langchain-ai.github.io/langgraph/concepts/).\\n\\n### [Integrations](integrations/providers/index.mdx)\\n\\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\\nIf you\\'re looking to get up and running quickly with [chat models](/docs/integrations/chat/), [vector stores](/docs/integrations/vectorstores/),\\nor other LangChain components from a specific provider, check out our growing list of [integrations](/docs/integrations/providers/).\\n\\n\\n### [API reference](https://python.langchain.com/api_reference/)\\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\\n\\n## Ecosystem\\n\\n### [🦜🛠️ LangSmith](https://docs.smith.langchain.com)\\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\\n\\n### [🦜🕸️ LangGraph](https://langchain-ai.github.io/langgraph)\\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by Linkedin, Uber, Klarna, GitLab, and many more.\\n\\n## Additional resources\\n\\n### [Versions](/docs/versions/v0_3/)\\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\\n\\n### [Security](/docs/security)\\nRead up on [security](/docs/security) best practices to make sure you\\'re developing safely with LangChain.\\n\\n### [Contributing](contributing/index.mdx)\\nCheck out the developer\\'s guide for guidelines on contributing and help getting your dev environment set up.\\n': 0.03252247488101534, '# LangChain Expression Language (LCEL)\\n\\n:::info Prerequisites\\n* [Runnable Interface](/docs/concepts/runnables)\\n:::\\n\\nThe **L**ang**C**hain **E**xpression **L**anguage (LCEL) takes a [declarative](https://en.wikipedia.org/wiki/Declarative_programming) approach to building new [Runnables](/docs/concepts/runnables) from existing Runnables.\\n\\nThis means that you describe what *should* happen, rather than *how* it should happen, allowing LangChain to optimize the run-time execution of the chains.\\n\\nWe often refer to a `Runnable` created using LCEL as a \"chain\". It\\'s important to remember that a \"chain\" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).\\n\\n:::note\\n* The [LCEL cheatsheet](/docs/how_to/lcel_cheatsheet/) shows common patterns that involve the Runnable interface and LCEL expressions.\\n* Please see the following list of [how-to guides](/docs/how_to/#langchain-expression-language-lcel) that cover common tasks with LCEL.\\n* A list of built-in `Runnables` can be found in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html). Many of these Runnables are useful when composing custom \"chains\" in LangChain using LCEL.\\n:::\\n\\n## Benefits of LCEL\\n\\nLangChain optimizes the run-time execution of chains built with LCEL in a number of ways:\\n\\n- **Optimized parallel execution**: Run Runnables in parallel using [RunnableParallel](#runnableparallel) or run multiple inputs through a given chain in parallel using the [Runnable Batch API](/docs/concepts/runnables/#optimized-parallel-execution-batch). Parallel execution can significantly reduce the latency as processing can be done in parallel instead of sequentially.\\n- **Guaranteed Async support**: Any chain built with LCEL can be run asynchronously using the [Runnable Async API](/docs/concepts/runnables/#asynchronous-support). This can be useful when running chains in a server environment where you want to handle large number of requests concurrently.\\n- **Simplify streaming**: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(time elapsed until the first chunk of output from a [chat model](/docs/concepts/chat_models) or [llm](/docs/concepts/text_llms) comes out).\\n\\nOther benefits include:\\n\\n- [**Seamless LangSmith tracing**](https://docs.smith.langchain.com)\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, **all** steps are automatically logged to [LangSmith](https://docs.smith.langchain.com/) for maximum observability and debuggability.\\n- **Standard API**: Because all chains are built using the Runnable interface, they can be used in the same way as any other Runnable.\\n- [**Deployable with LangServe**](/docs/concepts/architecture#langserve): Chains built with LCEL can be deployed using for production use.\\n\\n## Should I use LCEL?\\n\\nLCEL is an [orchestration solution](https://en.wikipedia.org/wiki/Orchestration_(computing)) -- it allows LangChain to handle run-time execution of chains in an optimized way.\\n\\nWhile we have seen users run chains with hundreds of steps in production, we generally recommend using LCEL for simpler orchestration tasks. When the application requires complex state management, branching, cycles or multiple agents, we recommend that users take advantage of [LangGraph](/docs/concepts/architecture#langgraph).\\n\\nIn LangGraph, users define graphs that specify the application\\'s flow. This allows users to keep using LCEL within individual nodes when LCEL is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.\\n\\nHere are some guidelines:\\n\\n* If you are making a single LLM call, you don\\'t need LCEL; instead call the underlying [chat model](/docs/concepts/chat_models) directly.\\n* If you have a simple chain (e.g., prompt + llm + parser, simple retrieval set up etc.), LCEL is a reasonable fit, if you\\'re taking advantage of the LCEL benefits.\\n* If you\\'re building a complex chain (e.g., with branching, cycles, multiple agents, etc.) use [LangGraph](/docs/concepts/architecture#langgraph) instead. Remember that you can always use LCEL within individual nodes in LangGraph.\\n\\n## Composition Primitives\\n\\n`LCEL` chains are built by composing existing `Runnables` together. The two main composition primitives are [RunnableSequence](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableSequence.html#langchain_core.runnables.base.RunnableSequence) and [RunnableParallel](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableParallel.html#langchain_core.runnables.base.RunnableParallel).\\n\\nMany other composition primitives (e.g., [RunnableAssign](\\nhttps://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnableAssign.html#langchain_core.runnables.passthrough.RunnableAssign\\n)) can be thought of as variations of these two primitives.\\n\\n:::note\\nYou can find a list of all composition primitives in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html).\\n:::\\n\\n### RunnableSequence\\n\\n`RunnableSequence` is a composition primitive that allows you \"chain\" multiple runnables sequentially, with the output of one runnable serving as the input to the next.\\n\\n```python\\nfrom langchain_core.runnables import RunnableSequence\\nchain = RunnableSequence([runnable1, runnable2])\\n```\\n\\nInvoking the `chain` with some input:\\n\\n```python\\nfinal_output = chain.invoke(some_input)\\n```\\n\\ncorresponds to the following:\\n\\n```python\\noutput1 = runnable1.invoke(some_input)\\nfinal_output = runnable2.invoke(output1)\\n```\\n\\n:::note\\n`runnable1` and `runnable2` are placeholders for any `Runnable` that you want to chain together.\\n:::\\n\\n### RunnableParallel\\n\\n`RunnableParallel` is a composition primitive that allows you to run multiple runnables concurrently, with the same input provided to each.\\n\\n```python\\nfrom langchain_core.runnables import RunnableParallel\\nchain = RunnableParallel({\\n    \"key1\": runnable1,\\n    \"key2\": runnable2,\\n})\\n```\\n\\nInvoking the `chain` with some input:\\n\\n```python\\nfinal_output = chain.invoke(some_input)\\n```\\n\\nWill yield a `final_output` dictionary with the same keys as the input dictionary, but with the values replaced by the output of the corresponding runnable.\\n\\n```python\\n{\\n    \"key1\": runnable1.invoke(some_input),\\n    \"key2\": runnable2.invoke(some_input),\\n}\\n```\\n\\nRecall, that the runnables are executed in parallel, so while the result is the same as\\ndictionary comprehension shown above, the execution time is much faster.\\n\\n:::note\\n`RunnableParallel`supports both synchronous and asynchronous execution (as all `Runnables` do).\\n\\n* For synchronous execution, `RunnableParallel` uses a [ThreadPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor) to run the runnables concurrently.\\n* For asynchronous execution, `RunnableParallel` uses [asyncio.gather](https://docs.python.org/3/library/asyncio.html#asyncio.gather) to run the runnables concurrently.\\n:::\\n\\n## Composition Syntax\\n\\nThe usage of `RunnableSequence` and `RunnableParallel` is so common that we created a shorthand syntax for using them. This helps\\nto make the code more readable and concise.\\n\\n### The `|` operator\\n\\nWe have [overloaded](https://docs.python.org/3/reference/datamodel.html#special-method-names) the `|` operator to create a `RunnableSequence` from two `Runnables`.\\n\\n```python\\nchain = runnable1 | runnable2\\n```\\n\\nis Equivalent to:\\n\\n```python\\nchain = RunnableSequence([runnable1, runnable2])\\n```\\n\\n### The `.pipe` method\\n\\nIf you have moral qualms with operator overloading, you can use the `.pipe` method instead. This is equivalent to the `|` operator.\\n\\n```python\\nchain = runnable1.pipe(runnable2)\\n```\\n\\n### Coercion\\n\\nLCEL applies automatic type coercion to make it easier to compose chains.\\n\\nIf you do not understand the type coercion, you can always use the `RunnableSequence` and `RunnableParallel` classes directly.\\n\\nThis will make the code more verbose, but it will also make it more explicit.\\n\\n#### Dictionary to RunnableParallel\\n\\nInside an LCEL expression, a dictionary is automatically converted to a `RunnableParallel`.\\n\\nFor example, the following code:\\n\\n```python\\nmapping = {\\n    \"key1\": runnable1,\\n    \"key2\": runnable2,\\n}\\n\\nchain = mapping | runnable3\\n```\\n\\nIt gets automatically converted to the following:\\n\\n```python\\nchain = RunnableSequence([RunnableParallel(mapping), runnable3])\\n```\\n\\n:::caution\\nYou have to be careful because the `mapping` dictionary is not a `RunnableParallel` object, it is just a dictionary. This means that the following code will raise an `AttributeError`:\\n\\n```python\\nmapping.invoke(some_input)\\n```\\n:::\\n\\n#### Function to RunnableLambda\\n\\nInside an LCEL expression, a function is automatically converted to a `RunnableLambda`.\\n\\n```\\ndef some_func(x):\\n    return x\\n\\nchain = some_func | runnable1\\n```\\n\\nIt gets automatically converted to the following:\\n\\n```python\\nchain = RunnableSequence([RunnableLambda(some_func), runnable1])\\n```\\n\\n:::caution\\nYou have to be careful because the lambda function is not a `RunnableLambda` object, it is just a function. This means that the following code will raise an `AttributeError`:\\n\\n```python\\nlambda x: x + 1.invoke(some_input)\\n```\\n:::\\n\\n## Legacy chains\\n\\nLCEL aims to provide consistency around behavior and customization over legacy subclassed chains such as `LLMChain` and\\n`ConversationalRetrievalChain`. Many of these legacy chains hide important details like prompts, and as a wider variety\\nof viable models emerge, customization has become more and more important.\\n\\nIf you are currently using one of these legacy chains, please see [this guide for guidance on how to migrate](/docs/versions/migrating_chains).\\n\\nFor guides on how to do specific tasks with LCEL, check out [the relevant how-to guides](/docs/how_to/#langchain-expression-language-lcel).\\n': 0.03252247488101534, '# Conceptual guide\\n\\nThis guide provides explanations of the key concepts behind the LangChain framework and AI applications more broadly.\\n\\nWe recommend that you go through at least one of the [Tutorials](/docs/tutorials) before diving into the conceptual guide. This will provide practical context that will make it easier to understand the concepts discussed here.\\n\\nThe conceptual guide does not cover step-by-step instructions or specific implementation examples — those are found in the [How-to guides](/docs/how_to/) and [Tutorials](/docs/tutorials). For detailed reference material, please see the [API reference](https://python.langchain.com/api_reference/).\\n\\n## High level\\n\\n- **[Why LangChain?](/docs/concepts/why_langchain)**: Overview of the value that LangChain provides.\\n- **[Architecture](/docs/concepts/architecture)**: How packages are organized in the LangChain ecosystem.\\n\\n## Concepts\\n\\n- **[Chat models](/docs/concepts/chat_models)**: LLMs exposed via a chat API that process sequences of messages as input and output a message.\\n- **[Messages](/docs/concepts/messages)**: The unit of communication in chat models, used to represent model input and output.\\n- **[Chat history](/docs/concepts/chat_history)**: A conversation represented as a sequence of messages, alternating between user messages and model responses.\\n- **[Tools](/docs/concepts/tools)**: A function with an associated schema defining the function\\'s name, description, and the arguments it accepts.\\n- **[Tool calling](/docs/concepts/tool_calling)**: A type of chat model API that accepts tool schemas, along with messages, as input and returns invocations of those tools as part of the output message.\\n- **[Structured output](/docs/concepts/structured_outputs)**: A technique to make a chat model respond in a structured format, such as JSON that matches a given schema.\\n- **[Memory](https://langchain-ai.github.io/langgraph/concepts/memory/)**: Information about a conversation that is persisted so that it can be used in future conversations.\\n- **[Multimodality](/docs/concepts/multimodality)**: The ability to work with data that comes in different forms, such as text, audio, images, and video.\\n- **[Runnable interface](/docs/concepts/runnables)**: The base abstraction that many LangChain components and the LangChain Expression Language are built on.\\n- **[Streaming](/docs/concepts/streaming)**: LangChain streaming APIs for surfacing results as they are generated.\\n- **[LangChain Expression Language (LCEL)](/docs/concepts/lcel)**: A syntax for orchestrating LangChain components. Most useful for simpler applications.\\n- **[Document loaders](/docs/concepts/document_loaders)**: Load a source as a list of documents.\\n- **[Retrieval](/docs/concepts/retrieval)**: Information retrieval systems can retrieve structured or unstructured data from a datasource in response to a query.\\n- **[Text splitters](/docs/concepts/text_splitters)**: Split long text into smaller chunks that can be individually indexed to enable granular retrieval.\\n- **[Embedding models](/docs/concepts/embedding_models)**: Models that represent data such as text or images in a vector space.\\n- **[Vector stores](/docs/concepts/vectorstores)**: Storage of and efficient search over vectors and associated metadata.\\n- **[Retriever](/docs/concepts/retrievers)**: A component that returns relevant documents from a knowledge base in response to a query.\\n- **[Retrieval Augmented Generation (RAG)](/docs/concepts/rag)**: A technique that enhances language models by combining them with external knowledge bases.\\n- **[Agents](/docs/concepts/agents)**: Use a [language model](/docs/concepts/chat_models) to choose a sequence of actions to take. Agents can interact with external resources via [tool](/docs/concepts/tools).\\n- **[Prompt templates](/docs/concepts/prompt_templates)**: Component for factoring out the static parts of a model \"prompt\" (usually a sequence of messages). Useful for serializing, versioning, and reusing these static parts.\\n- **[Output parsers](/docs/concepts/output_parsers)**: Responsible for taking the output of a model and transforming it into a more suitable format for downstream tasks. Output parsers were primarily useful prior to the general availability of [tool calling](/docs/concepts/tool_calling) and [structured outputs](/docs/concepts/structured_outputs).\\n- **[Few-shot prompting](/docs/concepts/few_shot_prompting)**: A technique for improving model performance by providing a few examples of the task to perform in the prompt.\\n- **[Example selectors](/docs/concepts/example_selectors)**: Used to select the most relevant examples from a dataset based on a given input. Example selectors are used in few-shot prompting to select examples for a prompt.\\n- **[Async programming](/docs/concepts/async)**: The basics that one should know to use LangChain in an asynchronous context.\\n- **[Callbacks](/docs/concepts/callbacks)**: Callbacks enable the execution of custom auxiliary code in built-in components. Callbacks are used to stream outputs from LLMs in LangChain, trace the intermediate steps of an application, and more.\\n- **[Tracing](/docs/concepts/tracing)**: The process of recording the steps that an application takes to go from input to output. Tracing is essential for debugging and diagnosing issues in complex applications.\\n- **[Evaluation](/docs/concepts/evaluation)**: The process of assessing the performance and effectiveness of AI applications. This involves testing the model\\'s responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose. This process is vital for building reliable applications.\\n- **[Testing](/docs/concepts/testing)**: The process of verifying that a component of an integration or application works as expected. Testing is essential for ensuring that the application behaves correctly and that changes to the codebase do not introduce new bugs.\\n\\n## Glossary\\n\\n- **[AIMessageChunk](/docs/concepts/messages#aimessagechunk)**: A partial response from an AI message. Used when streaming responses from a chat model.\\n- **[AIMessage](/docs/concepts/messages#aimessage)**: Represents a complete response from an AI model.\\n- **[astream_events](/docs/concepts/chat_models#key-methods)**: Stream granular information from [LCEL](/docs/concepts/lcel) chains.\\n- **[BaseTool](/docs/concepts/tools/#tool-interface)**: The base class for all tools in LangChain.\\n- **[batch](/docs/concepts/runnables)**: Use to execute a runnable with batch inputs.\\n- **[bind_tools](/docs/concepts/tool_calling/#tool-binding)**: Allows models to interact with tools.\\n- **[Caching](/docs/concepts/chat_models#caching)**: Storing results to avoid redundant calls to a chat model.\\n- **[Chat models](/docs/concepts/multimodality/#multimodality-in-chat-models)**: Chat models that handle multiple data modalities.\\n- **[Configurable runnables](/docs/concepts/runnables/#configurable-runnables)**: Creating configurable Runnables.\\n- **[Context window](/docs/concepts/chat_models#context-window)**: The maximum size of input a chat model can process.\\n- **[Conversation patterns](/docs/concepts/chat_history#conversation-patterns)**: Common patterns in chat interactions.\\n- **[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)**: LangChain\\'s representation of a document.\\n- **[Embedding models](/docs/concepts/multimodality/#multimodality-in-embedding-models)**: Models that generate vector embeddings for various data types.\\n- **[HumanMessage](/docs/concepts/messages#humanmessage)**: Represents a message from a human user.\\n- **[InjectedState](/docs/concepts/tools#injectedstate)**: A state injected into a tool function.\\n- **[InjectedStore](/docs/concepts/tools#injectedstore)**: A store that can be injected into a tool for data persistence.\\n- **[InjectedToolArg](/docs/concepts/tools#injectedtoolarg)**: Mechanism to inject arguments into tool functions.\\n- **[input and output types](/docs/concepts/runnables#input-and-output-types)**: Types used for input and output in Runnables.\\n- **[Integration packages](/docs/concepts/architecture/#integration-packages)**: Third-party packages that integrate with LangChain.\\n- **[Integration tests](/docs/concepts/testing#integration-tests)**: Tests that verify the correctness of the interaction between components, usually run with access to the underlying API that powers an integration.\\n- **[invoke](/docs/concepts/runnables)**: A standard method to invoke a Runnable.\\n- **[JSON mode](/docs/concepts/structured_outputs#json-mode)**: Returning responses in JSON format.\\n- **[langchain-community](/docs/concepts/architecture#langchain-community)**: Community-driven components for LangChain.\\n- **[langchain-core](/docs/concepts/architecture#langchain-core)**: Core langchain package. Includes base interfaces and in-memory implementations.\\n- **[langchain](/docs/concepts/architecture#langchain)**: A package for higher level components (e.g., some pre-built chains).\\n- **[langgraph](/docs/concepts/architecture#langgraph)**: Powerful orchestration layer for LangChain. Use to build complex pipelines and workflows.\\n- **[langserve](/docs/concepts/architecture#langserve)**: Used to deploy LangChain Runnables as REST endpoints. Uses FastAPI. Works primarily for LangChain Runnables, does not currently integrate with LangGraph.\\n- **[LLMs (legacy)](/docs/concepts/text_llms)**: Older language models that take a string as input and return a string as output.\\n- **[Managing chat history](/docs/concepts/chat_history#managing-chat-history)**: Techniques to maintain and manage the chat history.\\n- **[OpenAI format](/docs/concepts/messages#openai-format)**: OpenAI\\'s message format for chat models.\\n- **[Propagation of RunnableConfig](/docs/concepts/runnables/#propagation-of-runnableconfig)**: Propagating configuration through Runnables. Read if working with python 3.9, 3.10 and async.\\n- **[rate-limiting](/docs/concepts/chat_models#rate-limiting)**: Client side rate limiting for chat models.\\n- **[RemoveMessage](/docs/concepts/messages/#removemessage)**: An abstraction used to remove a message from chat history, used primarily in LangGraph.\\n- **[role](/docs/concepts/messages#role)**: Represents the role (e.g., user, assistant) of a chat message.\\n- **[RunnableConfig](/docs/concepts/runnables/#runnableconfig)**: Use to pass run time information to Runnables (e.g., `run_name`, `run_id`, `tags`, `metadata`, `max_concurrency`, `recursion_limit`, `configurable`).\\n- **[Standard parameters for chat models](/docs/concepts/chat_models#standard-parameters)**: Parameters such as API key, `temperature`, and `max_tokens`.\\n- **[Standard tests](/docs/concepts/testing#standard-tests)**: A defined set of unit and integration tests that all integrations must pass.\\n- **[stream](/docs/concepts/streaming)**: Use to stream output from a Runnable or a graph.\\n- **[Tokenization](/docs/concepts/tokens)**: The process of converting data into tokens and vice versa.\\n- **[Tokens](/docs/concepts/tokens)**: The basic unit that a language model reads, processes, and generates under the hood.\\n- **[Tool artifacts](/docs/concepts/tools#tool-artifacts)**: Add artifacts to the output of a tool that will not be sent to the model, but will be available for downstream processing.\\n- **[Tool binding](/docs/concepts/tool_calling#tool-binding)**: Binding tools to models.\\n- **[@tool](/docs/concepts/tools/#create-tools-using-the-tool-decorator)**: Decorator for creating tools in LangChain.\\n- **[Toolkits](/docs/concepts/tools#toolkits)**: A collection of tools that can be used together.\\n- **[ToolMessage](/docs/concepts/messages#toolmessage)**: Represents a message that contains the results of a tool execution.\\n- **[Unit tests](/docs/concepts/testing#unit-tests)**: Tests that verify the correctness of individual components, run in isolation without access to the Internet.\\n- **[Vector stores](/docs/concepts/vectorstores)**: Datastores specialized for storing and efficiently searching vector embeddings.\\n- **[with_structured_output](/docs/concepts/structured_outputs/#structured-output-method)**: A helper method for chat models that natively support [tool calling](/docs/concepts/tool_calling) to get structured output matching a given schema specified via Pydantic, JSON schema or a function.\\n- **[with_types](/docs/concepts/runnables#with_types)**: Method to overwrite the input and output types of a runnable. Useful when working with complex LCEL chains and deploying with LangServe.\\n': 0.015873015873015872, '---\\nsidebar_position: 0\\nsidebar_class_name: hidden\\n---\\n\\n# How-to guides\\n\\nHere you’ll find answers to “How do I….?” types of questions.\\nThese guides are *goal-oriented* and *concrete*; they\\'re meant to help you complete a specific task.\\nFor conceptual explanations see the [Conceptual guide](/docs/concepts/).\\nFor end-to-end walkthroughs see [Tutorials](/docs/tutorials).\\nFor comprehensive descriptions of every class and function see the [API Reference](https://python.langchain.com/api_reference/).\\n\\n## Installation\\n\\n- [How to: install LangChain packages](/docs/how_to/installation/)\\n- [How to: use LangChain with different Pydantic versions](/docs/how_to/pydantic_compatibility)\\n\\n## Key features\\n\\nThis highlights functionality that is core to using LangChain.\\n\\n- [How to: return structured data from a model](/docs/how_to/structured_output/)\\n- [How to: use a model to call tools](/docs/how_to/tool_calling)\\n- [How to: stream runnables](/docs/how_to/streaming)\\n- [How to: debug your LLM apps](/docs/how_to/debugging/)\\n\\n## Components\\n\\nThese are the core building blocks you can use when building applications.\\n\\n### Chat models\\n\\n[Chat Models](/docs/concepts/chat_models) are newer forms of language models that take messages in and output a message.\\nSee [supported integrations](/docs/integrations/chat/) for details on getting started with chat models from a specific provider.\\n\\n- [How to: do function/tool calling](/docs/how_to/tool_calling)\\n- [How to: get models to return structured output](/docs/how_to/structured_output)\\n- [How to: cache model responses](/docs/how_to/chat_model_caching)\\n- [How to: get log probabilities](/docs/how_to/logprobs)\\n- [How to: create a custom chat model class](/docs/how_to/custom_chat_model)\\n- [How to: stream a response back](/docs/how_to/chat_streaming)\\n- [How to: track token usage](/docs/how_to/chat_token_usage_tracking)\\n- [How to: track response metadata across providers](/docs/how_to/response_metadata)\\n- [How to: use chat model to call tools](/docs/how_to/tool_calling)\\n- [How to: stream tool calls](/docs/how_to/tool_streaming)\\n- [How to: handle rate limits](/docs/how_to/chat_model_rate_limiting)\\n- [How to: few shot prompt tool behavior](/docs/how_to/tools_few_shot)\\n- [How to: bind model-specific formatted tools](/docs/how_to/tools_model_specific)\\n- [How to: force a specific tool call](/docs/how_to/tool_choice)\\n- [How to: work with local models](/docs/how_to/local_llms)\\n- [How to: init any model in one line](/docs/how_to/chat_models_universal_init/)\\n- [How to: pass multimodal data directly to models](/docs/how_to/multimodal_inputs/)\\n\\n### Messages\\n\\n[Messages](/docs/concepts/messages) are the input and output of chat models. They have some `content` and a `role`, which describes the source of the message.\\n\\n- [How to: trim messages](/docs/how_to/trim_messages/)\\n- [How to: filter messages](/docs/how_to/filter_messages/)\\n- [How to: merge consecutive messages of the same type](/docs/how_to/merge_message_runs/)\\n\\n### Prompt templates\\n\\n[Prompt Templates](/docs/concepts/prompt_templates) are responsible for formatting user input into a format that can be passed to a language model.\\n\\n- [How to: use few shot examples](/docs/how_to/few_shot_examples)\\n- [How to: use few shot examples in chat models](/docs/how_to/few_shot_examples_chat/)\\n- [How to: partially format prompt templates](/docs/how_to/prompts_partial)\\n- [How to: compose prompts together](/docs/how_to/prompts_composition)\\n- [How to: use multimodal prompts](/docs/how_to/multimodal_prompts/)\\n\\n### Example selectors\\n\\n[Example Selectors](/docs/concepts/example_selectors) are responsible for selecting the correct few shot examples to pass to the prompt.\\n\\n- [How to: use example selectors](/docs/how_to/example_selectors)\\n- [How to: select examples by length](/docs/how_to/example_selectors_length_based)\\n- [How to: select examples by semantic similarity](/docs/how_to/example_selectors_similarity)\\n- [How to: select examples by semantic ngram overlap](/docs/how_to/example_selectors_ngram)\\n- [How to: select examples by maximal marginal relevance](/docs/how_to/example_selectors_mmr)\\n- [How to: select examples from LangSmith few-shot datasets](/docs/how_to/example_selectors_langsmith/)\\n\\n### LLMs\\n\\nWhat LangChain calls [LLMs](/docs/concepts/text_llms) are older forms of language models that take a string in and output a string.\\n\\n- [How to: cache model responses](/docs/how_to/llm_caching)\\n- [How to: create a custom LLM class](/docs/how_to/custom_llm)\\n- [How to: stream a response back](/docs/how_to/streaming_llm)\\n- [How to: track token usage](/docs/how_to/llm_token_usage_tracking)\\n- [How to: work with local models](/docs/how_to/local_llms)\\n\\n### Output parsers\\n\\n[Output Parsers](/docs/concepts/output_parsers) are responsible for taking the output of an LLM and parsing into more structured format.\\n\\n- [How to: parse text from message objects](/docs/how_to/output_parser_string)\\n- [How to: use output parsers to parse an LLM response into structured format](/docs/how_to/output_parser_structured)\\n- [How to: parse JSON output](/docs/how_to/output_parser_json)\\n- [How to: parse XML output](/docs/how_to/output_parser_xml)\\n- [How to: parse YAML output](/docs/how_to/output_parser_yaml)\\n- [How to: retry when output parsing errors occur](/docs/how_to/output_parser_retry)\\n- [How to: try to fix errors in output parsing](/docs/how_to/output_parser_fixing)\\n- [How to: write a custom output parser class](/docs/how_to/output_parser_custom)\\n\\n### Document loaders\\n\\n[Document Loaders](/docs/concepts/document_loaders) are responsible for loading documents from a variety of sources.\\n\\n- [How to: load PDF files](/docs/how_to/document_loader_pdf)\\n- [How to: load web pages](/docs/how_to/document_loader_web)\\n- [How to: load CSV data](/docs/how_to/document_loader_csv)\\n- [How to: load data from a directory](/docs/how_to/document_loader_directory)\\n- [How to: load HTML data](/docs/how_to/document_loader_html)\\n- [How to: load JSON data](/docs/how_to/document_loader_json)\\n- [How to: load Markdown data](/docs/how_to/document_loader_markdown)\\n- [How to: load Microsoft Office data](/docs/how_to/document_loader_office_file)\\n- [How to: write a custom document loader](/docs/how_to/document_loader_custom)\\n\\n### Text splitters\\n\\n[Text Splitters](/docs/concepts/text_splitters) take a document and split into chunks that can be used for retrieval.\\n\\n- [How to: recursively split text](/docs/how_to/recursive_text_splitter)\\n- [How to: split HTML](/docs/how_to/split_html)\\n- [How to: split by character](/docs/how_to/character_text_splitter)\\n- [How to: split code](/docs/how_to/code_splitter)\\n- [How to: split Markdown by headers](/docs/how_to/markdown_header_metadata_splitter)\\n- [How to: recursively split JSON](/docs/how_to/recursive_json_splitter)\\n- [How to: split text into semantic chunks](/docs/how_to/semantic-chunker)\\n- [How to: split by tokens](/docs/how_to/split_by_token)\\n\\n### Embedding models\\n\\n[Embedding Models](/docs/concepts/embedding_models) take a piece of text and create a numerical representation of it.\\nSee [supported integrations](/docs/integrations/text_embedding/) for details on getting started with embedding models from a specific provider.\\n\\n- [How to: embed text data](/docs/how_to/embed_text)\\n- [How to: cache embedding results](/docs/how_to/caching_embeddings)\\n- [How to: create a custom embeddings class](/docs/how_to/custom_embeddings)\\n\\n### Vector stores\\n\\n[Vector stores](/docs/concepts/vectorstores) are databases that can efficiently store and retrieve embeddings.\\nSee [supported integrations](/docs/integrations/vectorstores/) for details on getting started with vector stores from a specific provider.\\n\\n- [How to: use a vector store to retrieve data](/docs/how_to/vectorstores)\\n\\n### Retrievers\\n\\n[Retrievers](/docs/concepts/retrievers) are responsible for taking a query and returning relevant documents.\\n\\n- [How to: use a vector store to retrieve data](/docs/how_to/vectorstore_retriever)\\n- [How to: generate multiple queries to retrieve data for](/docs/how_to/MultiQueryRetriever)\\n- [How to: use contextual compression to compress the data retrieved](/docs/how_to/contextual_compression)\\n- [How to: write a custom retriever class](/docs/how_to/custom_retriever)\\n- [How to: add similarity scores to retriever results](/docs/how_to/add_scores_retriever)\\n- [How to: combine the results from multiple retrievers](/docs/how_to/ensemble_retriever)\\n- [How to: reorder retrieved results to mitigate the \"lost in the middle\" effect](/docs/how_to/long_context_reorder)\\n- [How to: generate multiple embeddings per document](/docs/how_to/multi_vector)\\n- [How to: retrieve the whole document for a chunk](/docs/how_to/parent_document_retriever)\\n- [How to: generate metadata filters](/docs/how_to/self_query)\\n- [How to: create a time-weighted retriever](/docs/how_to/time_weighted_vectorstore)\\n- [How to: use hybrid vector and keyword retrieval](/docs/how_to/hybrid)\\n\\n### Indexing\\n\\nIndexing is the process of keeping your vectorstore in-sync with the underlying data source.\\n\\n- [How to: reindex data to keep your vectorstore in-sync with the underlying data source](/docs/how_to/indexing)\\n\\n### Tools\\n\\nLangChain [Tools](/docs/concepts/tools) contain a description of the tool (to pass to the language model) as well as the implementation of the function to call. Refer [here](/docs/integrations/tools/) for a list of pre-built tools. \\n\\n- [How to: create tools](/docs/how_to/custom_tools)\\n- [How to: use built-in tools and toolkits](/docs/how_to/tools_builtin)\\n- [How to: use chat models to call tools](/docs/how_to/tool_calling)\\n- [How to: pass tool outputs to chat models](/docs/how_to/tool_results_pass_to_model)\\n- [How to: pass run time values to tools](/docs/how_to/tool_runtime)\\n- [How to: add a human-in-the-loop for tools](/docs/how_to/tools_human)\\n- [How to: handle tool errors](/docs/how_to/tools_error)\\n- [How to: force models to call a tool](/docs/how_to/tool_choice)\\n- [How to: disable parallel tool calling](/docs/how_to/tool_calling_parallel)\\n- [How to: access the `RunnableConfig` from a tool](/docs/how_to/tool_configure)\\n- [How to: stream events from a tool](/docs/how_to/tool_stream_events)\\n- [How to: return artifacts from a tool](/docs/how_to/tool_artifacts/)\\n- [How to: convert Runnables to tools](/docs/how_to/convert_runnable_to_tool)\\n- [How to: add ad-hoc tool calling capability to models](/docs/how_to/tools_prompting)\\n- [How to: pass in runtime secrets](/docs/how_to/runnable_runtime_secrets)\\n\\n### Multimodal\\n\\n- [How to: pass multimodal data directly to models](/docs/how_to/multimodal_inputs/)\\n- [How to: use multimodal prompts](/docs/how_to/multimodal_prompts/)\\n\\n\\n### Agents\\n\\n:::note\\n\\nFor in depth how-to guides for agents, please check out [LangGraph](https://langchain-ai.github.io/langgraph/) documentation.\\n\\n:::\\n\\n- [How to: use legacy LangChain Agents (AgentExecutor)](/docs/how_to/agent_executor)\\n- [How to: migrate from legacy LangChain agents to LangGraph](/docs/how_to/migrate_agent)\\n\\n### Callbacks\\n\\n[Callbacks](/docs/concepts/callbacks) allow you to hook into the various stages of your LLM application\\'s execution.\\n\\n- [How to: pass in callbacks at runtime](/docs/how_to/callbacks_runtime)\\n- [How to: attach callbacks to a module](/docs/how_to/callbacks_attach)\\n- [How to: pass callbacks into a module constructor](/docs/how_to/callbacks_constructor)\\n- [How to: create custom callback handlers](/docs/how_to/custom_callbacks)\\n- [How to: use callbacks in async environments](/docs/how_to/callbacks_async)\\n- [How to: dispatch custom callback events](/docs/how_to/callbacks_custom_events)\\n\\n### Custom\\n\\nAll of LangChain components can easily be extended to support your own versions.\\n\\n- [How to: create a custom chat model class](/docs/how_to/custom_chat_model)\\n- [How to: create a custom LLM class](/docs/how_to/custom_llm)\\n- [How to: create a custom embeddings class](/docs/how_to/custom_embeddings)\\n- [How to: write a custom retriever class](/docs/how_to/custom_retriever)\\n- [How to: write a custom document loader](/docs/how_to/document_loader_custom)\\n- [How to: write a custom output parser class](/docs/how_to/output_parser_custom)\\n- [How to: create custom callback handlers](/docs/how_to/custom_callbacks)\\n- [How to: define a custom tool](/docs/how_to/custom_tools)\\n- [How to: dispatch custom callback events](/docs/how_to/callbacks_custom_events)\\n\\n### Serialization\\n- [How to: save and load LangChain objects](/docs/how_to/serialization)\\n\\n## Use cases\\n\\nThese guides cover use-case specific details.\\n\\n### Q&A with RAG\\n\\nRetrieval Augmented Generation (RAG) is a way to connect LLMs to external sources of data.\\nFor a high-level tutorial on RAG, check out [this guide](/docs/tutorials/rag/).\\n\\n- [How to: add chat history](/docs/how_to/qa_chat_history_how_to/)\\n- [How to: stream](/docs/how_to/qa_streaming/)\\n- [How to: return sources](/docs/how_to/qa_sources/)\\n- [How to: return citations](/docs/how_to/qa_citations/)\\n- [How to: do per-user retrieval](/docs/how_to/qa_per_user/)\\n\\n\\n### Extraction\\n\\nExtraction is when you use LLMs to extract structured information from unstructured text.\\nFor a high level tutorial on extraction, check out [this guide](/docs/tutorials/extraction/).\\n\\n- [How to: use reference examples](/docs/how_to/extraction_examples/)\\n- [How to: handle long text](/docs/how_to/extraction_long_text/)\\n- [How to: do extraction without using function calling](/docs/how_to/extraction_parse)\\n\\n### Chatbots\\n\\nChatbots involve using an LLM to have a conversation.\\nFor a high-level tutorial on building chatbots, check out [this guide](/docs/tutorials/chatbot/).\\n\\n- [How to: manage memory](/docs/how_to/chatbots_memory)\\n- [How to: do retrieval](/docs/how_to/chatbots_retrieval)\\n- [How to: use tools](/docs/how_to/chatbots_tools)\\n- [How to: manage large chat history](/docs/how_to/trim_messages/)\\n\\n### Query analysis\\n\\nQuery Analysis is the task of using an LLM to generate a query to send to a retriever.\\nFor a high-level tutorial on query analysis, check out [this guide](/docs/tutorials/rag/#query-analysis).\\n\\n- [How to: add examples to the prompt](/docs/how_to/query_few_shot)\\n- [How to: handle cases where no queries are generated](/docs/how_to/query_no_queries)\\n- [How to: handle multiple queries](/docs/how_to/query_multiple_queries)\\n- [How to: handle multiple retrievers](/docs/how_to/query_multiple_retrievers)\\n- [How to: construct filters](/docs/how_to/query_constructing_filters)\\n- [How to: deal with high cardinality categorical variables](/docs/how_to/query_high_cardinality)\\n\\n### Q&A over SQL + CSV\\n\\nYou can use LLMs to do question answering over tabular data.\\nFor a high-level tutorial, check out [this guide](/docs/tutorials/sql_qa/).\\n\\n- [How to: use prompting to improve results](/docs/how_to/sql_prompting)\\n- [How to: do query validation](/docs/how_to/sql_query_checking)\\n- [How to: deal with large databases](/docs/how_to/sql_large_db)\\n- [How to: deal with CSV files](/docs/how_to/sql_csv)\\n\\n### Q&A over graph databases\\n\\nYou can use an LLM to do question answering over graph databases.\\nFor a high-level tutorial, check out [this guide](/docs/tutorials/graph/).\\n\\n- [How to: add a semantic layer over the database](/docs/how_to/graph_semantic)\\n- [How to: construct knowledge graphs](/docs/how_to/graph_constructing)\\n\\n### Summarization\\n\\nLLMs can summarize and otherwise distill desired information from text, including\\nlarge volumes of text. For a high-level tutorial, check out [this guide](/docs/tutorials/summarization).\\n\\n- [How to: summarize text in a single LLM call](/docs/how_to/summarize_stuff)\\n- [How to: summarize text through parallelization](/docs/how_to/summarize_map_reduce)\\n- [How to: summarize text through iterative refinement](/docs/how_to/summarize_refine)\\n\\n## LangChain Expression Language (LCEL)\\n\\n:::note Should I use LCEL?\\n\\nLCEL is an orchestration solution. See our\\n[concepts page](/docs/concepts/lcel/#should-i-use-lcel) for recommendations on when to\\nuse LCEL.\\n\\n:::\\n\\n[LangChain Expression Language](/docs/concepts/lcel) is a way to create arbitrary custom chains. It is built on the [Runnable](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html) protocol.\\n\\n[**LCEL cheatsheet**](/docs/how_to/lcel_cheatsheet/): For a quick overview of how to use the main LCEL primitives.\\n\\n[**Migration guide**](/docs/versions/migrating_chains): For migrating legacy chain abstractions to LCEL.\\n\\n- [How to: chain runnables](/docs/how_to/sequence)\\n- [How to: stream runnables](/docs/how_to/streaming)\\n- [How to: invoke runnables in parallel](/docs/how_to/parallel/)\\n- [How to: add default invocation args to runnables](/docs/how_to/binding/)\\n- [How to: turn any function into a runnable](/docs/how_to/functions)\\n- [How to: pass through inputs from one chain step to the next](/docs/how_to/passthrough)\\n- [How to: configure runnable behavior at runtime](/docs/how_to/configure)\\n- [How to: add message history (memory) to a chain](/docs/how_to/message_history)\\n- [How to: route between sub-chains](/docs/how_to/routing)\\n- [How to: create a dynamic (self-constructing) chain](/docs/how_to/dynamic_chain/)\\n- [How to: inspect runnables](/docs/how_to/inspect)\\n- [How to: add fallbacks to a runnable](/docs/how_to/fallbacks)\\n- [How to: pass runtime secrets to a runnable](/docs/how_to/runnable_runtime_secrets)\\n\\n## [LangGraph](https://langchain-ai.github.io/langgraph)\\n\\nLangGraph is an extension of LangChain aimed at\\nbuilding robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.\\n\\nLangGraph documentation is currently hosted on a separate site.\\nYou can peruse [LangGraph how-to guides here](https://langchain-ai.github.io/langgraph/how-tos/).\\n\\n## [LangSmith](https://docs.smith.langchain.com/)\\n\\nLangSmith allows you to closely trace, monitor and evaluate your LLM application.\\nIt seamlessly integrates with LangChain and LangGraph, and you can use it to inspect and debug individual steps of your chains and agents as you build.\\n\\nLangSmith documentation is hosted on a separate site.\\nYou can peruse [LangSmith how-to guides here](https://docs.smith.langchain.com/), but we\\'ll highlight a few sections that are particularly\\nrelevant to LangChain below:\\n\\n### Evaluation\\n<span data-heading-keywords=\"evaluation,evaluate\"></span>\\n\\nEvaluating performance is a vital part of building LLM-powered applications.\\nLangSmith helps with every step of the process from creating a dataset to defining metrics to running evaluators.\\n\\nTo learn more, check out the [LangSmith evaluation how-to guides](https://docs.smith.langchain.com/how_to_guides#evaluation).\\n\\n### Tracing\\n<span data-heading-keywords=\"trace,tracing\"></span>\\n\\nTracing gives you observability inside your chains and agents, and is vital in diagnosing issues.\\n\\n- [How to: trace with LangChain](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain)\\n- [How to: add metadata and tags to traces](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#add-metadata-and-tags-to-traces)\\n\\nYou can see general tracing-related how-tos [in this section of the LangSmith docs](https://docs.smith.langchain.com/how_to_guides/tracing).\\n': 0.015873015873015872}\n",
      "rank:1\n",
      "content_score_mapping:{'# Why LangChain?\\n\\nThe goal of `langchain` the Python package and LangChain the company is to make it as easy as possible for developers to build applications that reason.\\nWhile LangChain originally started as a single open source package, it has evolved into a company and a whole ecosystem.\\nThis page will talk about the LangChain ecosystem as a whole.\\nMost of the components within the LangChain ecosystem can be used by themselves - so if you feel particularly drawn to certain components but not others, that is totally fine! Pick and choose whichever components you like best for your own use case!\\n\\n## Features\\n\\nThere are several primary needs that LangChain aims to address:\\n\\n1. **Standardized component interfaces:** The growing number of [models](/docs/integrations/chat/) and [related components](/docs/integrations/vectorstores/) for AI applications has resulted in a wide variety of different APIs that developers need to learn and use.\\nThis diversity can make it challenging for developers to switch between providers or combine components when building applications.\\nLangChain exposes a standard interface for key components, making it easy to switch between providers.\\n\\n2. **Orchestration:** As applications become more complex, combining multiple components and models, there\\'s [a growing need to efficiently connect these elements into control flows](https://lilianweng.github.io/posts/2023-06-23-agent/) that can [accomplish diverse tasks](https://www.sequoiacap.com/article/generative-ais-act-o1/).\\n[Orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)) is crucial for building such applications.\\n\\n3. **Observability and evaluation:** As applications become more complex, it becomes increasingly difficult to understand what is happening within them.\\nFurthermore, the pace of development can become rate-limited by the [paradox of choice](https://en.wikipedia.org/wiki/Paradox_of_choice).\\nFor example, developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\n[Observability](https://en.wikipedia.org/wiki/Observability) and evaluations can help developers monitor their applications and rapidly answer these types of questions with confidence.\\n\\n\\n## Standardized component interfaces\\n\\nLangChain provides common interfaces for components that are central to many AI applications.\\nAs an example, all [chat models](/docs/concepts/chat_models/) implement the [BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) interface.\\nThis provides a standard way to interact with chat models, supporting important but often provider-specific features like [tool calling](/docs/concepts/tool_calling/) and [structured outputs](/docs/concepts/structured_outputs/).\\n\\n\\n### Example: chat models \\n\\nMany [model providers](/docs/concepts/chat_models/) support [tool calling](/docs/concepts/tool_calling/), a critical feature for many applications (e.g., [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/)), that allows a developer to request model responses that match a particular schema.\\nThe APIs for each provider differ. \\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to bind [tools](/docs/concepts/tools) to a model in order to support [tool calling](/docs/concepts/tool_calling/):\\n\\n```python\\n# Tool creation\\ntools = [my_tool]\\n# Tool binding\\nmodel_with_tools = model.bind_tools(tools)\\n```\\n\\nSimilarly, getting models to produce [structured outputs](/docs/concepts/structured_outputs/) is an extremely common use case. \\nProviders support different approaches for this, including [JSON mode or tool calling](https://platform.openai.com/docs/guides/structured-outputs), with different APIs.\\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to produce structured outputs using the `with_structured_output()` method:\\n\\n```python\\n# Define schema\\nschema = ...\\n# Bind schema to model\\nmodel_with_structure = model.with_structured_output(schema)\\n```\\n\\n### Example: retrievers\\n\\nIn the context of [RAG](/docs/concepts/rag/) and LLM application components, LangChain\\'s [retriever](/docs/concepts/retrievers/) interface provides a standard way to connect to many different types of data services or databases (e.g., [vector stores](/docs/concepts/vectorstores) or databases).\\nThe underlying implementation of the retriever depends on the type of data store or database you are connecting to, but all retrievers implement the [runnable interface](/docs/concepts/runnables/), meaning they can be invoked in a common manner.\\n\\n```python\\ndocuments = my_retriever.invoke(\"What is the meaning of life?\")\\n```\\n\\n## Orchestration \\n\\nWhile standardization for individual components is useful, we\\'ve increasingly seen that developers want to *combine* components into more complex applications. \\nThis motivates the need for [orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)).\\nThere are several common characteristics of LLM applications that this orchestration layer should support:\\n\\n* **Complex control flow:** The application requires complex patterns such as cycles (e.g., a loop that reiterates until a condition is met).\\n* **[Persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/):** The application needs to maintain [short-term and / or long-term memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n* **[Human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/):** The application needs human interaction, e.g., pausing, reviewing, editing, approving certain steps.\\n\\nThe recommended way to orchestrate components for complex applications is [LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/).\\nLangGraph is a library that gives developers a high degree of control by expressing the flow of the application as a set of nodes and edges.\\nLangGraph comes with built-in support for [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), [memory](https://langchain-ai.github.io/langgraph/concepts/memory/), and other features.\\nIt\\'s particularly well suited for building [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/) or [multi-agent](https://langchain-ai.github.io/langgraph/concepts/multi_agent/) applications. \\nImportantly, individual LangChain components can be used as LangGraph nodes, but you can also use LangGraph **without** using LangChain components.\\n\\n:::info[Further reading]\\n\\nHave a look at our free course, [Introduction to LangGraph](https://academy.langchain.com/courses/intro-to-langgraph), to learn more about how to use LangGraph to build complex applications.\\n\\n:::\\n\\n## Observability and evaluation\\n\\nThe pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. \\nDevelopers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\nHigh quality tracing and evaluations can help you rapidly answer these types of questions with confidence.\\n[LangSmith](https://docs.smith.langchain.com/) is our platform that supports observability and evaluation for AI applications.\\nSee our conceptual guides on [evaluations](https://docs.smith.langchain.com/concepts/evaluation) and [tracing](https://docs.smith.langchain.com/concepts/tracing) for more details.\\n\\n:::info[Further reading]\\n\\nSee our video playlist on [LangSmith tracing and evaluations](https://youtube.com/playlist?list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S&feature=shared) for more details.\\n\\n:::\\n\\n## Conclusion\\n\\nLangChain offers standard interfaces for components that are central to many AI applications, which offers a few specific advantages:\\n- **Ease of swapping providers:** It allows you to swap out different component providers without having to change the underlying code.\\n- **Advanced features:** It provides common methods for more advanced features, such as [streaming](/docs/concepts/streaming) and [tool calling](/docs/concepts/tool_calling/).\\n\\n[LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/) makes it possible to orchestrate complex applications (e.g., [agents](/docs/concepts/agents/)) and provide features like including [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), or [memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n\\n[LangSmith](https://docs.smith.langchain.com/) makes it possible to iterate with confidence on your applications, by providing LLM-specific observability and framework for testing and evaluating your application.\\n': 0.05, '---\\nsidebar_position: 0\\nsidebar_class_name: hidden\\n---\\n\\n# Introduction\\n\\n**LangChain** is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:\\n- **Development**: Build your applications using LangChain\\'s open-source [components](/docs/concepts) and [third-party integrations](/docs/integrations/providers/).\\nUse [LangGraph](/docs/concepts/architecture/#langgraph) to build stateful agents with first-class streaming and human-in-the-loop support.\\n- **Productionization**: Use [LangSmith](https://docs.smith.langchain.com/) to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\n- **Deployment**: Turn your LangGraph applications into production-ready APIs and Assistants with [LangGraph Platform](https://langchain-ai.github.io/langgraph/cloud/).\\n\\nimport ThemedImage from \\'@theme/ThemedImage\\';\\nimport useBaseUrl from \\'@docusaurus/useBaseUrl\\';\\n\\n<ThemedImage\\n  alt=\"Diagram outlining the hierarchical organization of the LangChain framework, displaying the interconnected parts across multiple layers.\"\\n  sources={{\\n    light: useBaseUrl(\\'/svg/langchain_stack_112024.svg\\'),\\n    dark: useBaseUrl(\\'/svg/langchain_stack_112024_dark.svg\\'),\\n  }}\\n  style={{ width: \"100%\" }}\\n  title=\"LangChain Framework Overview\"\\n/>\\n\\nLangChain implements a standard interface for large language models and related\\ntechnologies, such as embedding models and vector stores, and integrates with\\nhundreds of providers. See the [integrations](/docs/integrations/providers/) page for\\nmore.\\n\\nimport ChatModelTabs from \"@theme/ChatModelTabs\";\\n\\n<ChatModelTabs/>\\n\\n```python\\nmodel.invoke(\"Hello, world!\")\\n```\\n\\n:::note\\n\\nThese docs focus on the Python LangChain library. [Head here](https://js.langchain.com) for docs on the JavaScript LangChain library.\\n\\n:::\\n\\n## Architecture\\n\\nThe LangChain framework consists of multiple open-source libraries. Read more in the\\n[Architecture](/docs/concepts/architecture/) page.\\n\\n- **`langchain-core`**: Base abstractions for chat models and other components.\\n- **Integration packages** (e.g. `langchain-openai`, `langchain-anthropic`, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\\n- **`langchain`**: Chains, agents, and retrieval strategies that make up an application\\'s cognitive architecture.\\n- **`langchain-community`**: Third-party integrations that are community maintained.\\n- **`langgraph`**: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See [LangGraph documentation](https://langchain-ai.github.io/langgraph/).\\n\\n## Guides\\n\\n### [Tutorials](/docs/tutorials)\\n\\nIf you\\'re looking to build something specific or are more of a hands-on learner, check out our [tutorials section](/docs/tutorials).\\nThis is the best place to get started.\\n\\nThese are the best ones to get started with:\\n\\n- [Build a Simple LLM Application](/docs/tutorials/llm_chain)\\n- [Build a Chatbot](/docs/tutorials/chatbot)\\n- [Build an Agent](/docs/tutorials/agents)\\n- [Introduction to LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/)\\n\\nExplore the full list of LangChain tutorials [here](/docs/tutorials), and check out other [LangGraph tutorials here](https://langchain-ai.github.io/langgraph/tutorials/). To learn more about LangGraph, check out our first LangChain Academy course, *Introduction to LangGraph*, available [here](https://academy.langchain.com/courses/intro-to-langgraph).\\n\\n\\n### [How-to guides](/docs/how_to)\\n\\n[Here](/docs/how_to) you’ll find short answers to “How do I….?” types of questions.\\nThese how-to guides don’t cover topics in depth – you’ll find that material in the [Tutorials](/docs/tutorials) and the [API Reference](https://python.langchain.com/api_reference/).\\nHowever, these guides will help you quickly accomplish common tasks using [chat models](/docs/how_to/#chat-models),\\n[vector stores](/docs/how_to/#vector-stores), and other common LangChain components.\\n\\nCheck out [LangGraph-specific how-tos here](https://langchain-ai.github.io/langgraph/how-tos/).\\n\\n### [Conceptual guide](/docs/concepts)\\n\\nIntroductions to all the key parts of LangChain you’ll need to know! [Here](/docs/concepts) you\\'ll find high level explanations of all LangChain concepts.\\n\\nFor a deeper dive into LangGraph concepts, check out [this page](https://langchain-ai.github.io/langgraph/concepts/).\\n\\n### [Integrations](integrations/providers/index.mdx)\\n\\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\\nIf you\\'re looking to get up and running quickly with [chat models](/docs/integrations/chat/), [vector stores](/docs/integrations/vectorstores/),\\nor other LangChain components from a specific provider, check out our growing list of [integrations](/docs/integrations/providers/).\\n\\n\\n### [API reference](https://python.langchain.com/api_reference/)\\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\\n\\n## Ecosystem\\n\\n### [🦜🛠️ LangSmith](https://docs.smith.langchain.com)\\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\\n\\n### [🦜🕸️ LangGraph](https://langchain-ai.github.io/langgraph)\\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by Linkedin, Uber, Klarna, GitLab, and many more.\\n\\n## Additional resources\\n\\n### [Versions](/docs/versions/v0_3/)\\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\\n\\n### [Security](/docs/security)\\nRead up on [security](/docs/security) best practices to make sure you\\'re developing safely with LangChain.\\n\\n### [Contributing](contributing/index.mdx)\\nCheck out the developer\\'s guide for guidelines on contributing and help getting your dev environment set up.\\n': 0.048915917503966164, '# LangChain Expression Language (LCEL)\\n\\n:::info Prerequisites\\n* [Runnable Interface](/docs/concepts/runnables)\\n:::\\n\\nThe **L**ang**C**hain **E**xpression **L**anguage (LCEL) takes a [declarative](https://en.wikipedia.org/wiki/Declarative_programming) approach to building new [Runnables](/docs/concepts/runnables) from existing Runnables.\\n\\nThis means that you describe what *should* happen, rather than *how* it should happen, allowing LangChain to optimize the run-time execution of the chains.\\n\\nWe often refer to a `Runnable` created using LCEL as a \"chain\". It\\'s important to remember that a \"chain\" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).\\n\\n:::note\\n* The [LCEL cheatsheet](/docs/how_to/lcel_cheatsheet/) shows common patterns that involve the Runnable interface and LCEL expressions.\\n* Please see the following list of [how-to guides](/docs/how_to/#langchain-expression-language-lcel) that cover common tasks with LCEL.\\n* A list of built-in `Runnables` can be found in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html). Many of these Runnables are useful when composing custom \"chains\" in LangChain using LCEL.\\n:::\\n\\n## Benefits of LCEL\\n\\nLangChain optimizes the run-time execution of chains built with LCEL in a number of ways:\\n\\n- **Optimized parallel execution**: Run Runnables in parallel using [RunnableParallel](#runnableparallel) or run multiple inputs through a given chain in parallel using the [Runnable Batch API](/docs/concepts/runnables/#optimized-parallel-execution-batch). Parallel execution can significantly reduce the latency as processing can be done in parallel instead of sequentially.\\n- **Guaranteed Async support**: Any chain built with LCEL can be run asynchronously using the [Runnable Async API](/docs/concepts/runnables/#asynchronous-support). This can be useful when running chains in a server environment where you want to handle large number of requests concurrently.\\n- **Simplify streaming**: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(time elapsed until the first chunk of output from a [chat model](/docs/concepts/chat_models) or [llm](/docs/concepts/text_llms) comes out).\\n\\nOther benefits include:\\n\\n- [**Seamless LangSmith tracing**](https://docs.smith.langchain.com)\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, **all** steps are automatically logged to [LangSmith](https://docs.smith.langchain.com/) for maximum observability and debuggability.\\n- **Standard API**: Because all chains are built using the Runnable interface, they can be used in the same way as any other Runnable.\\n- [**Deployable with LangServe**](/docs/concepts/architecture#langserve): Chains built with LCEL can be deployed using for production use.\\n\\n## Should I use LCEL?\\n\\nLCEL is an [orchestration solution](https://en.wikipedia.org/wiki/Orchestration_(computing)) -- it allows LangChain to handle run-time execution of chains in an optimized way.\\n\\nWhile we have seen users run chains with hundreds of steps in production, we generally recommend using LCEL for simpler orchestration tasks. When the application requires complex state management, branching, cycles or multiple agents, we recommend that users take advantage of [LangGraph](/docs/concepts/architecture#langgraph).\\n\\nIn LangGraph, users define graphs that specify the application\\'s flow. This allows users to keep using LCEL within individual nodes when LCEL is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.\\n\\nHere are some guidelines:\\n\\n* If you are making a single LLM call, you don\\'t need LCEL; instead call the underlying [chat model](/docs/concepts/chat_models) directly.\\n* If you have a simple chain (e.g., prompt + llm + parser, simple retrieval set up etc.), LCEL is a reasonable fit, if you\\'re taking advantage of the LCEL benefits.\\n* If you\\'re building a complex chain (e.g., with branching, cycles, multiple agents, etc.) use [LangGraph](/docs/concepts/architecture#langgraph) instead. Remember that you can always use LCEL within individual nodes in LangGraph.\\n\\n## Composition Primitives\\n\\n`LCEL` chains are built by composing existing `Runnables` together. The two main composition primitives are [RunnableSequence](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableSequence.html#langchain_core.runnables.base.RunnableSequence) and [RunnableParallel](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableParallel.html#langchain_core.runnables.base.RunnableParallel).\\n\\nMany other composition primitives (e.g., [RunnableAssign](\\nhttps://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnableAssign.html#langchain_core.runnables.passthrough.RunnableAssign\\n)) can be thought of as variations of these two primitives.\\n\\n:::note\\nYou can find a list of all composition primitives in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html).\\n:::\\n\\n### RunnableSequence\\n\\n`RunnableSequence` is a composition primitive that allows you \"chain\" multiple runnables sequentially, with the output of one runnable serving as the input to the next.\\n\\n```python\\nfrom langchain_core.runnables import RunnableSequence\\nchain = RunnableSequence([runnable1, runnable2])\\n```\\n\\nInvoking the `chain` with some input:\\n\\n```python\\nfinal_output = chain.invoke(some_input)\\n```\\n\\ncorresponds to the following:\\n\\n```python\\noutput1 = runnable1.invoke(some_input)\\nfinal_output = runnable2.invoke(output1)\\n```\\n\\n:::note\\n`runnable1` and `runnable2` are placeholders for any `Runnable` that you want to chain together.\\n:::\\n\\n### RunnableParallel\\n\\n`RunnableParallel` is a composition primitive that allows you to run multiple runnables concurrently, with the same input provided to each.\\n\\n```python\\nfrom langchain_core.runnables import RunnableParallel\\nchain = RunnableParallel({\\n    \"key1\": runnable1,\\n    \"key2\": runnable2,\\n})\\n```\\n\\nInvoking the `chain` with some input:\\n\\n```python\\nfinal_output = chain.invoke(some_input)\\n```\\n\\nWill yield a `final_output` dictionary with the same keys as the input dictionary, but with the values replaced by the output of the corresponding runnable.\\n\\n```python\\n{\\n    \"key1\": runnable1.invoke(some_input),\\n    \"key2\": runnable2.invoke(some_input),\\n}\\n```\\n\\nRecall, that the runnables are executed in parallel, so while the result is the same as\\ndictionary comprehension shown above, the execution time is much faster.\\n\\n:::note\\n`RunnableParallel`supports both synchronous and asynchronous execution (as all `Runnables` do).\\n\\n* For synchronous execution, `RunnableParallel` uses a [ThreadPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor) to run the runnables concurrently.\\n* For asynchronous execution, `RunnableParallel` uses [asyncio.gather](https://docs.python.org/3/library/asyncio.html#asyncio.gather) to run the runnables concurrently.\\n:::\\n\\n## Composition Syntax\\n\\nThe usage of `RunnableSequence` and `RunnableParallel` is so common that we created a shorthand syntax for using them. This helps\\nto make the code more readable and concise.\\n\\n### The `|` operator\\n\\nWe have [overloaded](https://docs.python.org/3/reference/datamodel.html#special-method-names) the `|` operator to create a `RunnableSequence` from two `Runnables`.\\n\\n```python\\nchain = runnable1 | runnable2\\n```\\n\\nis Equivalent to:\\n\\n```python\\nchain = RunnableSequence([runnable1, runnable2])\\n```\\n\\n### The `.pipe` method\\n\\nIf you have moral qualms with operator overloading, you can use the `.pipe` method instead. This is equivalent to the `|` operator.\\n\\n```python\\nchain = runnable1.pipe(runnable2)\\n```\\n\\n### Coercion\\n\\nLCEL applies automatic type coercion to make it easier to compose chains.\\n\\nIf you do not understand the type coercion, you can always use the `RunnableSequence` and `RunnableParallel` classes directly.\\n\\nThis will make the code more verbose, but it will also make it more explicit.\\n\\n#### Dictionary to RunnableParallel\\n\\nInside an LCEL expression, a dictionary is automatically converted to a `RunnableParallel`.\\n\\nFor example, the following code:\\n\\n```python\\nmapping = {\\n    \"key1\": runnable1,\\n    \"key2\": runnable2,\\n}\\n\\nchain = mapping | runnable3\\n```\\n\\nIt gets automatically converted to the following:\\n\\n```python\\nchain = RunnableSequence([RunnableParallel(mapping), runnable3])\\n```\\n\\n:::caution\\nYou have to be careful because the `mapping` dictionary is not a `RunnableParallel` object, it is just a dictionary. This means that the following code will raise an `AttributeError`:\\n\\n```python\\nmapping.invoke(some_input)\\n```\\n:::\\n\\n#### Function to RunnableLambda\\n\\nInside an LCEL expression, a function is automatically converted to a `RunnableLambda`.\\n\\n```\\ndef some_func(x):\\n    return x\\n\\nchain = some_func | runnable1\\n```\\n\\nIt gets automatically converted to the following:\\n\\n```python\\nchain = RunnableSequence([RunnableLambda(some_func), runnable1])\\n```\\n\\n:::caution\\nYou have to be careful because the lambda function is not a `RunnableLambda` object, it is just a function. This means that the following code will raise an `AttributeError`:\\n\\n```python\\nlambda x: x + 1.invoke(some_input)\\n```\\n:::\\n\\n## Legacy chains\\n\\nLCEL aims to provide consistency around behavior and customization over legacy subclassed chains such as `LLMChain` and\\n`ConversationalRetrievalChain`. Many of these legacy chains hide important details like prompts, and as a wider variety\\nof viable models emerge, customization has become more and more important.\\n\\nIf you are currently using one of these legacy chains, please see [this guide for guidance on how to migrate](/docs/versions/migrating_chains).\\n\\nFor guides on how to do specific tasks with LCEL, check out [the relevant how-to guides](/docs/how_to/#langchain-expression-language-lcel).\\n': 0.03252247488101534, '# Conceptual guide\\n\\nThis guide provides explanations of the key concepts behind the LangChain framework and AI applications more broadly.\\n\\nWe recommend that you go through at least one of the [Tutorials](/docs/tutorials) before diving into the conceptual guide. This will provide practical context that will make it easier to understand the concepts discussed here.\\n\\nThe conceptual guide does not cover step-by-step instructions or specific implementation examples — those are found in the [How-to guides](/docs/how_to/) and [Tutorials](/docs/tutorials). For detailed reference material, please see the [API reference](https://python.langchain.com/api_reference/).\\n\\n## High level\\n\\n- **[Why LangChain?](/docs/concepts/why_langchain)**: Overview of the value that LangChain provides.\\n- **[Architecture](/docs/concepts/architecture)**: How packages are organized in the LangChain ecosystem.\\n\\n## Concepts\\n\\n- **[Chat models](/docs/concepts/chat_models)**: LLMs exposed via a chat API that process sequences of messages as input and output a message.\\n- **[Messages](/docs/concepts/messages)**: The unit of communication in chat models, used to represent model input and output.\\n- **[Chat history](/docs/concepts/chat_history)**: A conversation represented as a sequence of messages, alternating between user messages and model responses.\\n- **[Tools](/docs/concepts/tools)**: A function with an associated schema defining the function\\'s name, description, and the arguments it accepts.\\n- **[Tool calling](/docs/concepts/tool_calling)**: A type of chat model API that accepts tool schemas, along with messages, as input and returns invocations of those tools as part of the output message.\\n- **[Structured output](/docs/concepts/structured_outputs)**: A technique to make a chat model respond in a structured format, such as JSON that matches a given schema.\\n- **[Memory](https://langchain-ai.github.io/langgraph/concepts/memory/)**: Information about a conversation that is persisted so that it can be used in future conversations.\\n- **[Multimodality](/docs/concepts/multimodality)**: The ability to work with data that comes in different forms, such as text, audio, images, and video.\\n- **[Runnable interface](/docs/concepts/runnables)**: The base abstraction that many LangChain components and the LangChain Expression Language are built on.\\n- **[Streaming](/docs/concepts/streaming)**: LangChain streaming APIs for surfacing results as they are generated.\\n- **[LangChain Expression Language (LCEL)](/docs/concepts/lcel)**: A syntax for orchestrating LangChain components. Most useful for simpler applications.\\n- **[Document loaders](/docs/concepts/document_loaders)**: Load a source as a list of documents.\\n- **[Retrieval](/docs/concepts/retrieval)**: Information retrieval systems can retrieve structured or unstructured data from a datasource in response to a query.\\n- **[Text splitters](/docs/concepts/text_splitters)**: Split long text into smaller chunks that can be individually indexed to enable granular retrieval.\\n- **[Embedding models](/docs/concepts/embedding_models)**: Models that represent data such as text or images in a vector space.\\n- **[Vector stores](/docs/concepts/vectorstores)**: Storage of and efficient search over vectors and associated metadata.\\n- **[Retriever](/docs/concepts/retrievers)**: A component that returns relevant documents from a knowledge base in response to a query.\\n- **[Retrieval Augmented Generation (RAG)](/docs/concepts/rag)**: A technique that enhances language models by combining them with external knowledge bases.\\n- **[Agents](/docs/concepts/agents)**: Use a [language model](/docs/concepts/chat_models) to choose a sequence of actions to take. Agents can interact with external resources via [tool](/docs/concepts/tools).\\n- **[Prompt templates](/docs/concepts/prompt_templates)**: Component for factoring out the static parts of a model \"prompt\" (usually a sequence of messages). Useful for serializing, versioning, and reusing these static parts.\\n- **[Output parsers](/docs/concepts/output_parsers)**: Responsible for taking the output of a model and transforming it into a more suitable format for downstream tasks. Output parsers were primarily useful prior to the general availability of [tool calling](/docs/concepts/tool_calling) and [structured outputs](/docs/concepts/structured_outputs).\\n- **[Few-shot prompting](/docs/concepts/few_shot_prompting)**: A technique for improving model performance by providing a few examples of the task to perform in the prompt.\\n- **[Example selectors](/docs/concepts/example_selectors)**: Used to select the most relevant examples from a dataset based on a given input. Example selectors are used in few-shot prompting to select examples for a prompt.\\n- **[Async programming](/docs/concepts/async)**: The basics that one should know to use LangChain in an asynchronous context.\\n- **[Callbacks](/docs/concepts/callbacks)**: Callbacks enable the execution of custom auxiliary code in built-in components. Callbacks are used to stream outputs from LLMs in LangChain, trace the intermediate steps of an application, and more.\\n- **[Tracing](/docs/concepts/tracing)**: The process of recording the steps that an application takes to go from input to output. Tracing is essential for debugging and diagnosing issues in complex applications.\\n- **[Evaluation](/docs/concepts/evaluation)**: The process of assessing the performance and effectiveness of AI applications. This involves testing the model\\'s responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose. This process is vital for building reliable applications.\\n- **[Testing](/docs/concepts/testing)**: The process of verifying that a component of an integration or application works as expected. Testing is essential for ensuring that the application behaves correctly and that changes to the codebase do not introduce new bugs.\\n\\n## Glossary\\n\\n- **[AIMessageChunk](/docs/concepts/messages#aimessagechunk)**: A partial response from an AI message. Used when streaming responses from a chat model.\\n- **[AIMessage](/docs/concepts/messages#aimessage)**: Represents a complete response from an AI model.\\n- **[astream_events](/docs/concepts/chat_models#key-methods)**: Stream granular information from [LCEL](/docs/concepts/lcel) chains.\\n- **[BaseTool](/docs/concepts/tools/#tool-interface)**: The base class for all tools in LangChain.\\n- **[batch](/docs/concepts/runnables)**: Use to execute a runnable with batch inputs.\\n- **[bind_tools](/docs/concepts/tool_calling/#tool-binding)**: Allows models to interact with tools.\\n- **[Caching](/docs/concepts/chat_models#caching)**: Storing results to avoid redundant calls to a chat model.\\n- **[Chat models](/docs/concepts/multimodality/#multimodality-in-chat-models)**: Chat models that handle multiple data modalities.\\n- **[Configurable runnables](/docs/concepts/runnables/#configurable-runnables)**: Creating configurable Runnables.\\n- **[Context window](/docs/concepts/chat_models#context-window)**: The maximum size of input a chat model can process.\\n- **[Conversation patterns](/docs/concepts/chat_history#conversation-patterns)**: Common patterns in chat interactions.\\n- **[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)**: LangChain\\'s representation of a document.\\n- **[Embedding models](/docs/concepts/multimodality/#multimodality-in-embedding-models)**: Models that generate vector embeddings for various data types.\\n- **[HumanMessage](/docs/concepts/messages#humanmessage)**: Represents a message from a human user.\\n- **[InjectedState](/docs/concepts/tools#injectedstate)**: A state injected into a tool function.\\n- **[InjectedStore](/docs/concepts/tools#injectedstore)**: A store that can be injected into a tool for data persistence.\\n- **[InjectedToolArg](/docs/concepts/tools#injectedtoolarg)**: Mechanism to inject arguments into tool functions.\\n- **[input and output types](/docs/concepts/runnables#input-and-output-types)**: Types used for input and output in Runnables.\\n- **[Integration packages](/docs/concepts/architecture/#integration-packages)**: Third-party packages that integrate with LangChain.\\n- **[Integration tests](/docs/concepts/testing#integration-tests)**: Tests that verify the correctness of the interaction between components, usually run with access to the underlying API that powers an integration.\\n- **[invoke](/docs/concepts/runnables)**: A standard method to invoke a Runnable.\\n- **[JSON mode](/docs/concepts/structured_outputs#json-mode)**: Returning responses in JSON format.\\n- **[langchain-community](/docs/concepts/architecture#langchain-community)**: Community-driven components for LangChain.\\n- **[langchain-core](/docs/concepts/architecture#langchain-core)**: Core langchain package. Includes base interfaces and in-memory implementations.\\n- **[langchain](/docs/concepts/architecture#langchain)**: A package for higher level components (e.g., some pre-built chains).\\n- **[langgraph](/docs/concepts/architecture#langgraph)**: Powerful orchestration layer for LangChain. Use to build complex pipelines and workflows.\\n- **[langserve](/docs/concepts/architecture#langserve)**: Used to deploy LangChain Runnables as REST endpoints. Uses FastAPI. Works primarily for LangChain Runnables, does not currently integrate with LangGraph.\\n- **[LLMs (legacy)](/docs/concepts/text_llms)**: Older language models that take a string as input and return a string as output.\\n- **[Managing chat history](/docs/concepts/chat_history#managing-chat-history)**: Techniques to maintain and manage the chat history.\\n- **[OpenAI format](/docs/concepts/messages#openai-format)**: OpenAI\\'s message format for chat models.\\n- **[Propagation of RunnableConfig](/docs/concepts/runnables/#propagation-of-runnableconfig)**: Propagating configuration through Runnables. Read if working with python 3.9, 3.10 and async.\\n- **[rate-limiting](/docs/concepts/chat_models#rate-limiting)**: Client side rate limiting for chat models.\\n- **[RemoveMessage](/docs/concepts/messages/#removemessage)**: An abstraction used to remove a message from chat history, used primarily in LangGraph.\\n- **[role](/docs/concepts/messages#role)**: Represents the role (e.g., user, assistant) of a chat message.\\n- **[RunnableConfig](/docs/concepts/runnables/#runnableconfig)**: Use to pass run time information to Runnables (e.g., `run_name`, `run_id`, `tags`, `metadata`, `max_concurrency`, `recursion_limit`, `configurable`).\\n- **[Standard parameters for chat models](/docs/concepts/chat_models#standard-parameters)**: Parameters such as API key, `temperature`, and `max_tokens`.\\n- **[Standard tests](/docs/concepts/testing#standard-tests)**: A defined set of unit and integration tests that all integrations must pass.\\n- **[stream](/docs/concepts/streaming)**: Use to stream output from a Runnable or a graph.\\n- **[Tokenization](/docs/concepts/tokens)**: The process of converting data into tokens and vice versa.\\n- **[Tokens](/docs/concepts/tokens)**: The basic unit that a language model reads, processes, and generates under the hood.\\n- **[Tool artifacts](/docs/concepts/tools#tool-artifacts)**: Add artifacts to the output of a tool that will not be sent to the model, but will be available for downstream processing.\\n- **[Tool binding](/docs/concepts/tool_calling#tool-binding)**: Binding tools to models.\\n- **[@tool](/docs/concepts/tools/#create-tools-using-the-tool-decorator)**: Decorator for creating tools in LangChain.\\n- **[Toolkits](/docs/concepts/tools#toolkits)**: A collection of tools that can be used together.\\n- **[ToolMessage](/docs/concepts/messages#toolmessage)**: Represents a message that contains the results of a tool execution.\\n- **[Unit tests](/docs/concepts/testing#unit-tests)**: Tests that verify the correctness of individual components, run in isolation without access to the Internet.\\n- **[Vector stores](/docs/concepts/vectorstores)**: Datastores specialized for storing and efficiently searching vector embeddings.\\n- **[with_structured_output](/docs/concepts/structured_outputs/#structured-output-method)**: A helper method for chat models that natively support [tool calling](/docs/concepts/tool_calling) to get structured output matching a given schema specified via Pydantic, JSON schema or a function.\\n- **[with_types](/docs/concepts/runnables#with_types)**: Method to overwrite the input and output types of a runnable. Useful when working with complex LCEL chains and deploying with LangServe.\\n': 0.015873015873015872, '---\\nsidebar_position: 0\\nsidebar_class_name: hidden\\n---\\n\\n# How-to guides\\n\\nHere you’ll find answers to “How do I….?” types of questions.\\nThese guides are *goal-oriented* and *concrete*; they\\'re meant to help you complete a specific task.\\nFor conceptual explanations see the [Conceptual guide](/docs/concepts/).\\nFor end-to-end walkthroughs see [Tutorials](/docs/tutorials).\\nFor comprehensive descriptions of every class and function see the [API Reference](https://python.langchain.com/api_reference/).\\n\\n## Installation\\n\\n- [How to: install LangChain packages](/docs/how_to/installation/)\\n- [How to: use LangChain with different Pydantic versions](/docs/how_to/pydantic_compatibility)\\n\\n## Key features\\n\\nThis highlights functionality that is core to using LangChain.\\n\\n- [How to: return structured data from a model](/docs/how_to/structured_output/)\\n- [How to: use a model to call tools](/docs/how_to/tool_calling)\\n- [How to: stream runnables](/docs/how_to/streaming)\\n- [How to: debug your LLM apps](/docs/how_to/debugging/)\\n\\n## Components\\n\\nThese are the core building blocks you can use when building applications.\\n\\n### Chat models\\n\\n[Chat Models](/docs/concepts/chat_models) are newer forms of language models that take messages in and output a message.\\nSee [supported integrations](/docs/integrations/chat/) for details on getting started with chat models from a specific provider.\\n\\n- [How to: do function/tool calling](/docs/how_to/tool_calling)\\n- [How to: get models to return structured output](/docs/how_to/structured_output)\\n- [How to: cache model responses](/docs/how_to/chat_model_caching)\\n- [How to: get log probabilities](/docs/how_to/logprobs)\\n- [How to: create a custom chat model class](/docs/how_to/custom_chat_model)\\n- [How to: stream a response back](/docs/how_to/chat_streaming)\\n- [How to: track token usage](/docs/how_to/chat_token_usage_tracking)\\n- [How to: track response metadata across providers](/docs/how_to/response_metadata)\\n- [How to: use chat model to call tools](/docs/how_to/tool_calling)\\n- [How to: stream tool calls](/docs/how_to/tool_streaming)\\n- [How to: handle rate limits](/docs/how_to/chat_model_rate_limiting)\\n- [How to: few shot prompt tool behavior](/docs/how_to/tools_few_shot)\\n- [How to: bind model-specific formatted tools](/docs/how_to/tools_model_specific)\\n- [How to: force a specific tool call](/docs/how_to/tool_choice)\\n- [How to: work with local models](/docs/how_to/local_llms)\\n- [How to: init any model in one line](/docs/how_to/chat_models_universal_init/)\\n- [How to: pass multimodal data directly to models](/docs/how_to/multimodal_inputs/)\\n\\n### Messages\\n\\n[Messages](/docs/concepts/messages) are the input and output of chat models. They have some `content` and a `role`, which describes the source of the message.\\n\\n- [How to: trim messages](/docs/how_to/trim_messages/)\\n- [How to: filter messages](/docs/how_to/filter_messages/)\\n- [How to: merge consecutive messages of the same type](/docs/how_to/merge_message_runs/)\\n\\n### Prompt templates\\n\\n[Prompt Templates](/docs/concepts/prompt_templates) are responsible for formatting user input into a format that can be passed to a language model.\\n\\n- [How to: use few shot examples](/docs/how_to/few_shot_examples)\\n- [How to: use few shot examples in chat models](/docs/how_to/few_shot_examples_chat/)\\n- [How to: partially format prompt templates](/docs/how_to/prompts_partial)\\n- [How to: compose prompts together](/docs/how_to/prompts_composition)\\n- [How to: use multimodal prompts](/docs/how_to/multimodal_prompts/)\\n\\n### Example selectors\\n\\n[Example Selectors](/docs/concepts/example_selectors) are responsible for selecting the correct few shot examples to pass to the prompt.\\n\\n- [How to: use example selectors](/docs/how_to/example_selectors)\\n- [How to: select examples by length](/docs/how_to/example_selectors_length_based)\\n- [How to: select examples by semantic similarity](/docs/how_to/example_selectors_similarity)\\n- [How to: select examples by semantic ngram overlap](/docs/how_to/example_selectors_ngram)\\n- [How to: select examples by maximal marginal relevance](/docs/how_to/example_selectors_mmr)\\n- [How to: select examples from LangSmith few-shot datasets](/docs/how_to/example_selectors_langsmith/)\\n\\n### LLMs\\n\\nWhat LangChain calls [LLMs](/docs/concepts/text_llms) are older forms of language models that take a string in and output a string.\\n\\n- [How to: cache model responses](/docs/how_to/llm_caching)\\n- [How to: create a custom LLM class](/docs/how_to/custom_llm)\\n- [How to: stream a response back](/docs/how_to/streaming_llm)\\n- [How to: track token usage](/docs/how_to/llm_token_usage_tracking)\\n- [How to: work with local models](/docs/how_to/local_llms)\\n\\n### Output parsers\\n\\n[Output Parsers](/docs/concepts/output_parsers) are responsible for taking the output of an LLM and parsing into more structured format.\\n\\n- [How to: parse text from message objects](/docs/how_to/output_parser_string)\\n- [How to: use output parsers to parse an LLM response into structured format](/docs/how_to/output_parser_structured)\\n- [How to: parse JSON output](/docs/how_to/output_parser_json)\\n- [How to: parse XML output](/docs/how_to/output_parser_xml)\\n- [How to: parse YAML output](/docs/how_to/output_parser_yaml)\\n- [How to: retry when output parsing errors occur](/docs/how_to/output_parser_retry)\\n- [How to: try to fix errors in output parsing](/docs/how_to/output_parser_fixing)\\n- [How to: write a custom output parser class](/docs/how_to/output_parser_custom)\\n\\n### Document loaders\\n\\n[Document Loaders](/docs/concepts/document_loaders) are responsible for loading documents from a variety of sources.\\n\\n- [How to: load PDF files](/docs/how_to/document_loader_pdf)\\n- [How to: load web pages](/docs/how_to/document_loader_web)\\n- [How to: load CSV data](/docs/how_to/document_loader_csv)\\n- [How to: load data from a directory](/docs/how_to/document_loader_directory)\\n- [How to: load HTML data](/docs/how_to/document_loader_html)\\n- [How to: load JSON data](/docs/how_to/document_loader_json)\\n- [How to: load Markdown data](/docs/how_to/document_loader_markdown)\\n- [How to: load Microsoft Office data](/docs/how_to/document_loader_office_file)\\n- [How to: write a custom document loader](/docs/how_to/document_loader_custom)\\n\\n### Text splitters\\n\\n[Text Splitters](/docs/concepts/text_splitters) take a document and split into chunks that can be used for retrieval.\\n\\n- [How to: recursively split text](/docs/how_to/recursive_text_splitter)\\n- [How to: split HTML](/docs/how_to/split_html)\\n- [How to: split by character](/docs/how_to/character_text_splitter)\\n- [How to: split code](/docs/how_to/code_splitter)\\n- [How to: split Markdown by headers](/docs/how_to/markdown_header_metadata_splitter)\\n- [How to: recursively split JSON](/docs/how_to/recursive_json_splitter)\\n- [How to: split text into semantic chunks](/docs/how_to/semantic-chunker)\\n- [How to: split by tokens](/docs/how_to/split_by_token)\\n\\n### Embedding models\\n\\n[Embedding Models](/docs/concepts/embedding_models) take a piece of text and create a numerical representation of it.\\nSee [supported integrations](/docs/integrations/text_embedding/) for details on getting started with embedding models from a specific provider.\\n\\n- [How to: embed text data](/docs/how_to/embed_text)\\n- [How to: cache embedding results](/docs/how_to/caching_embeddings)\\n- [How to: create a custom embeddings class](/docs/how_to/custom_embeddings)\\n\\n### Vector stores\\n\\n[Vector stores](/docs/concepts/vectorstores) are databases that can efficiently store and retrieve embeddings.\\nSee [supported integrations](/docs/integrations/vectorstores/) for details on getting started with vector stores from a specific provider.\\n\\n- [How to: use a vector store to retrieve data](/docs/how_to/vectorstores)\\n\\n### Retrievers\\n\\n[Retrievers](/docs/concepts/retrievers) are responsible for taking a query and returning relevant documents.\\n\\n- [How to: use a vector store to retrieve data](/docs/how_to/vectorstore_retriever)\\n- [How to: generate multiple queries to retrieve data for](/docs/how_to/MultiQueryRetriever)\\n- [How to: use contextual compression to compress the data retrieved](/docs/how_to/contextual_compression)\\n- [How to: write a custom retriever class](/docs/how_to/custom_retriever)\\n- [How to: add similarity scores to retriever results](/docs/how_to/add_scores_retriever)\\n- [How to: combine the results from multiple retrievers](/docs/how_to/ensemble_retriever)\\n- [How to: reorder retrieved results to mitigate the \"lost in the middle\" effect](/docs/how_to/long_context_reorder)\\n- [How to: generate multiple embeddings per document](/docs/how_to/multi_vector)\\n- [How to: retrieve the whole document for a chunk](/docs/how_to/parent_document_retriever)\\n- [How to: generate metadata filters](/docs/how_to/self_query)\\n- [How to: create a time-weighted retriever](/docs/how_to/time_weighted_vectorstore)\\n- [How to: use hybrid vector and keyword retrieval](/docs/how_to/hybrid)\\n\\n### Indexing\\n\\nIndexing is the process of keeping your vectorstore in-sync with the underlying data source.\\n\\n- [How to: reindex data to keep your vectorstore in-sync with the underlying data source](/docs/how_to/indexing)\\n\\n### Tools\\n\\nLangChain [Tools](/docs/concepts/tools) contain a description of the tool (to pass to the language model) as well as the implementation of the function to call. Refer [here](/docs/integrations/tools/) for a list of pre-built tools. \\n\\n- [How to: create tools](/docs/how_to/custom_tools)\\n- [How to: use built-in tools and toolkits](/docs/how_to/tools_builtin)\\n- [How to: use chat models to call tools](/docs/how_to/tool_calling)\\n- [How to: pass tool outputs to chat models](/docs/how_to/tool_results_pass_to_model)\\n- [How to: pass run time values to tools](/docs/how_to/tool_runtime)\\n- [How to: add a human-in-the-loop for tools](/docs/how_to/tools_human)\\n- [How to: handle tool errors](/docs/how_to/tools_error)\\n- [How to: force models to call a tool](/docs/how_to/tool_choice)\\n- [How to: disable parallel tool calling](/docs/how_to/tool_calling_parallel)\\n- [How to: access the `RunnableConfig` from a tool](/docs/how_to/tool_configure)\\n- [How to: stream events from a tool](/docs/how_to/tool_stream_events)\\n- [How to: return artifacts from a tool](/docs/how_to/tool_artifacts/)\\n- [How to: convert Runnables to tools](/docs/how_to/convert_runnable_to_tool)\\n- [How to: add ad-hoc tool calling capability to models](/docs/how_to/tools_prompting)\\n- [How to: pass in runtime secrets](/docs/how_to/runnable_runtime_secrets)\\n\\n### Multimodal\\n\\n- [How to: pass multimodal data directly to models](/docs/how_to/multimodal_inputs/)\\n- [How to: use multimodal prompts](/docs/how_to/multimodal_prompts/)\\n\\n\\n### Agents\\n\\n:::note\\n\\nFor in depth how-to guides for agents, please check out [LangGraph](https://langchain-ai.github.io/langgraph/) documentation.\\n\\n:::\\n\\n- [How to: use legacy LangChain Agents (AgentExecutor)](/docs/how_to/agent_executor)\\n- [How to: migrate from legacy LangChain agents to LangGraph](/docs/how_to/migrate_agent)\\n\\n### Callbacks\\n\\n[Callbacks](/docs/concepts/callbacks) allow you to hook into the various stages of your LLM application\\'s execution.\\n\\n- [How to: pass in callbacks at runtime](/docs/how_to/callbacks_runtime)\\n- [How to: attach callbacks to a module](/docs/how_to/callbacks_attach)\\n- [How to: pass callbacks into a module constructor](/docs/how_to/callbacks_constructor)\\n- [How to: create custom callback handlers](/docs/how_to/custom_callbacks)\\n- [How to: use callbacks in async environments](/docs/how_to/callbacks_async)\\n- [How to: dispatch custom callback events](/docs/how_to/callbacks_custom_events)\\n\\n### Custom\\n\\nAll of LangChain components can easily be extended to support your own versions.\\n\\n- [How to: create a custom chat model class](/docs/how_to/custom_chat_model)\\n- [How to: create a custom LLM class](/docs/how_to/custom_llm)\\n- [How to: create a custom embeddings class](/docs/how_to/custom_embeddings)\\n- [How to: write a custom retriever class](/docs/how_to/custom_retriever)\\n- [How to: write a custom document loader](/docs/how_to/document_loader_custom)\\n- [How to: write a custom output parser class](/docs/how_to/output_parser_custom)\\n- [How to: create custom callback handlers](/docs/how_to/custom_callbacks)\\n- [How to: define a custom tool](/docs/how_to/custom_tools)\\n- [How to: dispatch custom callback events](/docs/how_to/callbacks_custom_events)\\n\\n### Serialization\\n- [How to: save and load LangChain objects](/docs/how_to/serialization)\\n\\n## Use cases\\n\\nThese guides cover use-case specific details.\\n\\n### Q&A with RAG\\n\\nRetrieval Augmented Generation (RAG) is a way to connect LLMs to external sources of data.\\nFor a high-level tutorial on RAG, check out [this guide](/docs/tutorials/rag/).\\n\\n- [How to: add chat history](/docs/how_to/qa_chat_history_how_to/)\\n- [How to: stream](/docs/how_to/qa_streaming/)\\n- [How to: return sources](/docs/how_to/qa_sources/)\\n- [How to: return citations](/docs/how_to/qa_citations/)\\n- [How to: do per-user retrieval](/docs/how_to/qa_per_user/)\\n\\n\\n### Extraction\\n\\nExtraction is when you use LLMs to extract structured information from unstructured text.\\nFor a high level tutorial on extraction, check out [this guide](/docs/tutorials/extraction/).\\n\\n- [How to: use reference examples](/docs/how_to/extraction_examples/)\\n- [How to: handle long text](/docs/how_to/extraction_long_text/)\\n- [How to: do extraction without using function calling](/docs/how_to/extraction_parse)\\n\\n### Chatbots\\n\\nChatbots involve using an LLM to have a conversation.\\nFor a high-level tutorial on building chatbots, check out [this guide](/docs/tutorials/chatbot/).\\n\\n- [How to: manage memory](/docs/how_to/chatbots_memory)\\n- [How to: do retrieval](/docs/how_to/chatbots_retrieval)\\n- [How to: use tools](/docs/how_to/chatbots_tools)\\n- [How to: manage large chat history](/docs/how_to/trim_messages/)\\n\\n### Query analysis\\n\\nQuery Analysis is the task of using an LLM to generate a query to send to a retriever.\\nFor a high-level tutorial on query analysis, check out [this guide](/docs/tutorials/rag/#query-analysis).\\n\\n- [How to: add examples to the prompt](/docs/how_to/query_few_shot)\\n- [How to: handle cases where no queries are generated](/docs/how_to/query_no_queries)\\n- [How to: handle multiple queries](/docs/how_to/query_multiple_queries)\\n- [How to: handle multiple retrievers](/docs/how_to/query_multiple_retrievers)\\n- [How to: construct filters](/docs/how_to/query_constructing_filters)\\n- [How to: deal with high cardinality categorical variables](/docs/how_to/query_high_cardinality)\\n\\n### Q&A over SQL + CSV\\n\\nYou can use LLMs to do question answering over tabular data.\\nFor a high-level tutorial, check out [this guide](/docs/tutorials/sql_qa/).\\n\\n- [How to: use prompting to improve results](/docs/how_to/sql_prompting)\\n- [How to: do query validation](/docs/how_to/sql_query_checking)\\n- [How to: deal with large databases](/docs/how_to/sql_large_db)\\n- [How to: deal with CSV files](/docs/how_to/sql_csv)\\n\\n### Q&A over graph databases\\n\\nYou can use an LLM to do question answering over graph databases.\\nFor a high-level tutorial, check out [this guide](/docs/tutorials/graph/).\\n\\n- [How to: add a semantic layer over the database](/docs/how_to/graph_semantic)\\n- [How to: construct knowledge graphs](/docs/how_to/graph_constructing)\\n\\n### Summarization\\n\\nLLMs can summarize and otherwise distill desired information from text, including\\nlarge volumes of text. For a high-level tutorial, check out [this guide](/docs/tutorials/summarization).\\n\\n- [How to: summarize text in a single LLM call](/docs/how_to/summarize_stuff)\\n- [How to: summarize text through parallelization](/docs/how_to/summarize_map_reduce)\\n- [How to: summarize text through iterative refinement](/docs/how_to/summarize_refine)\\n\\n## LangChain Expression Language (LCEL)\\n\\n:::note Should I use LCEL?\\n\\nLCEL is an orchestration solution. See our\\n[concepts page](/docs/concepts/lcel/#should-i-use-lcel) for recommendations on when to\\nuse LCEL.\\n\\n:::\\n\\n[LangChain Expression Language](/docs/concepts/lcel) is a way to create arbitrary custom chains. It is built on the [Runnable](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html) protocol.\\n\\n[**LCEL cheatsheet**](/docs/how_to/lcel_cheatsheet/): For a quick overview of how to use the main LCEL primitives.\\n\\n[**Migration guide**](/docs/versions/migrating_chains): For migrating legacy chain abstractions to LCEL.\\n\\n- [How to: chain runnables](/docs/how_to/sequence)\\n- [How to: stream runnables](/docs/how_to/streaming)\\n- [How to: invoke runnables in parallel](/docs/how_to/parallel/)\\n- [How to: add default invocation args to runnables](/docs/how_to/binding/)\\n- [How to: turn any function into a runnable](/docs/how_to/functions)\\n- [How to: pass through inputs from one chain step to the next](/docs/how_to/passthrough)\\n- [How to: configure runnable behavior at runtime](/docs/how_to/configure)\\n- [How to: add message history (memory) to a chain](/docs/how_to/message_history)\\n- [How to: route between sub-chains](/docs/how_to/routing)\\n- [How to: create a dynamic (self-constructing) chain](/docs/how_to/dynamic_chain/)\\n- [How to: inspect runnables](/docs/how_to/inspect)\\n- [How to: add fallbacks to a runnable](/docs/how_to/fallbacks)\\n- [How to: pass runtime secrets to a runnable](/docs/how_to/runnable_runtime_secrets)\\n\\n## [LangGraph](https://langchain-ai.github.io/langgraph)\\n\\nLangGraph is an extension of LangChain aimed at\\nbuilding robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.\\n\\nLangGraph documentation is currently hosted on a separate site.\\nYou can peruse [LangGraph how-to guides here](https://langchain-ai.github.io/langgraph/how-tos/).\\n\\n## [LangSmith](https://docs.smith.langchain.com/)\\n\\nLangSmith allows you to closely trace, monitor and evaluate your LLM application.\\nIt seamlessly integrates with LangChain and LangGraph, and you can use it to inspect and debug individual steps of your chains and agents as you build.\\n\\nLangSmith documentation is hosted on a separate site.\\nYou can peruse [LangSmith how-to guides here](https://docs.smith.langchain.com/), but we\\'ll highlight a few sections that are particularly\\nrelevant to LangChain below:\\n\\n### Evaluation\\n<span data-heading-keywords=\"evaluation,evaluate\"></span>\\n\\nEvaluating performance is a vital part of building LLM-powered applications.\\nLangSmith helps with every step of the process from creating a dataset to defining metrics to running evaluators.\\n\\nTo learn more, check out the [LangSmith evaluation how-to guides](https://docs.smith.langchain.com/how_to_guides#evaluation).\\n\\n### Tracing\\n<span data-heading-keywords=\"trace,tracing\"></span>\\n\\nTracing gives you observability inside your chains and agents, and is vital in diagnosing issues.\\n\\n- [How to: trace with LangChain](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain)\\n- [How to: add metadata and tags to traces](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#add-metadata-and-tags-to-traces)\\n\\nYou can see general tracing-related how-tos [in this section of the LangSmith docs](https://docs.smith.langchain.com/how_to_guides/tracing).\\n': 0.015873015873015872}\n",
      "rank:2\n",
      "content_score_mapping:{'# Why LangChain?\\n\\nThe goal of `langchain` the Python package and LangChain the company is to make it as easy as possible for developers to build applications that reason.\\nWhile LangChain originally started as a single open source package, it has evolved into a company and a whole ecosystem.\\nThis page will talk about the LangChain ecosystem as a whole.\\nMost of the components within the LangChain ecosystem can be used by themselves - so if you feel particularly drawn to certain components but not others, that is totally fine! Pick and choose whichever components you like best for your own use case!\\n\\n## Features\\n\\nThere are several primary needs that LangChain aims to address:\\n\\n1. **Standardized component interfaces:** The growing number of [models](/docs/integrations/chat/) and [related components](/docs/integrations/vectorstores/) for AI applications has resulted in a wide variety of different APIs that developers need to learn and use.\\nThis diversity can make it challenging for developers to switch between providers or combine components when building applications.\\nLangChain exposes a standard interface for key components, making it easy to switch between providers.\\n\\n2. **Orchestration:** As applications become more complex, combining multiple components and models, there\\'s [a growing need to efficiently connect these elements into control flows](https://lilianweng.github.io/posts/2023-06-23-agent/) that can [accomplish diverse tasks](https://www.sequoiacap.com/article/generative-ais-act-o1/).\\n[Orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)) is crucial for building such applications.\\n\\n3. **Observability and evaluation:** As applications become more complex, it becomes increasingly difficult to understand what is happening within them.\\nFurthermore, the pace of development can become rate-limited by the [paradox of choice](https://en.wikipedia.org/wiki/Paradox_of_choice).\\nFor example, developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\n[Observability](https://en.wikipedia.org/wiki/Observability) and evaluations can help developers monitor their applications and rapidly answer these types of questions with confidence.\\n\\n\\n## Standardized component interfaces\\n\\nLangChain provides common interfaces for components that are central to many AI applications.\\nAs an example, all [chat models](/docs/concepts/chat_models/) implement the [BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) interface.\\nThis provides a standard way to interact with chat models, supporting important but often provider-specific features like [tool calling](/docs/concepts/tool_calling/) and [structured outputs](/docs/concepts/structured_outputs/).\\n\\n\\n### Example: chat models \\n\\nMany [model providers](/docs/concepts/chat_models/) support [tool calling](/docs/concepts/tool_calling/), a critical feature for many applications (e.g., [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/)), that allows a developer to request model responses that match a particular schema.\\nThe APIs for each provider differ. \\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to bind [tools](/docs/concepts/tools) to a model in order to support [tool calling](/docs/concepts/tool_calling/):\\n\\n```python\\n# Tool creation\\ntools = [my_tool]\\n# Tool binding\\nmodel_with_tools = model.bind_tools(tools)\\n```\\n\\nSimilarly, getting models to produce [structured outputs](/docs/concepts/structured_outputs/) is an extremely common use case. \\nProviders support different approaches for this, including [JSON mode or tool calling](https://platform.openai.com/docs/guides/structured-outputs), with different APIs.\\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to produce structured outputs using the `with_structured_output()` method:\\n\\n```python\\n# Define schema\\nschema = ...\\n# Bind schema to model\\nmodel_with_structure = model.with_structured_output(schema)\\n```\\n\\n### Example: retrievers\\n\\nIn the context of [RAG](/docs/concepts/rag/) and LLM application components, LangChain\\'s [retriever](/docs/concepts/retrievers/) interface provides a standard way to connect to many different types of data services or databases (e.g., [vector stores](/docs/concepts/vectorstores) or databases).\\nThe underlying implementation of the retriever depends on the type of data store or database you are connecting to, but all retrievers implement the [runnable interface](/docs/concepts/runnables/), meaning they can be invoked in a common manner.\\n\\n```python\\ndocuments = my_retriever.invoke(\"What is the meaning of life?\")\\n```\\n\\n## Orchestration \\n\\nWhile standardization for individual components is useful, we\\'ve increasingly seen that developers want to *combine* components into more complex applications. \\nThis motivates the need for [orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)).\\nThere are several common characteristics of LLM applications that this orchestration layer should support:\\n\\n* **Complex control flow:** The application requires complex patterns such as cycles (e.g., a loop that reiterates until a condition is met).\\n* **[Persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/):** The application needs to maintain [short-term and / or long-term memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n* **[Human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/):** The application needs human interaction, e.g., pausing, reviewing, editing, approving certain steps.\\n\\nThe recommended way to orchestrate components for complex applications is [LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/).\\nLangGraph is a library that gives developers a high degree of control by expressing the flow of the application as a set of nodes and edges.\\nLangGraph comes with built-in support for [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), [memory](https://langchain-ai.github.io/langgraph/concepts/memory/), and other features.\\nIt\\'s particularly well suited for building [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/) or [multi-agent](https://langchain-ai.github.io/langgraph/concepts/multi_agent/) applications. \\nImportantly, individual LangChain components can be used as LangGraph nodes, but you can also use LangGraph **without** using LangChain components.\\n\\n:::info[Further reading]\\n\\nHave a look at our free course, [Introduction to LangGraph](https://academy.langchain.com/courses/intro-to-langgraph), to learn more about how to use LangGraph to build complex applications.\\n\\n:::\\n\\n## Observability and evaluation\\n\\nThe pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. \\nDevelopers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\nHigh quality tracing and evaluations can help you rapidly answer these types of questions with confidence.\\n[LangSmith](https://docs.smith.langchain.com/) is our platform that supports observability and evaluation for AI applications.\\nSee our conceptual guides on [evaluations](https://docs.smith.langchain.com/concepts/evaluation) and [tracing](https://docs.smith.langchain.com/concepts/tracing) for more details.\\n\\n:::info[Further reading]\\n\\nSee our video playlist on [LangSmith tracing and evaluations](https://youtube.com/playlist?list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S&feature=shared) for more details.\\n\\n:::\\n\\n## Conclusion\\n\\nLangChain offers standard interfaces for components that are central to many AI applications, which offers a few specific advantages:\\n- **Ease of swapping providers:** It allows you to swap out different component providers without having to change the underlying code.\\n- **Advanced features:** It provides common methods for more advanced features, such as [streaming](/docs/concepts/streaming) and [tool calling](/docs/concepts/tool_calling/).\\n\\n[LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/) makes it possible to orchestrate complex applications (e.g., [agents](/docs/concepts/agents/)) and provide features like including [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), or [memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n\\n[LangSmith](https://docs.smith.langchain.com/) makes it possible to iterate with confidence on your applications, by providing LLM-specific observability and framework for testing and evaluating your application.\\n': 0.05, '---\\nsidebar_position: 0\\nsidebar_class_name: hidden\\n---\\n\\n# Introduction\\n\\n**LangChain** is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:\\n- **Development**: Build your applications using LangChain\\'s open-source [components](/docs/concepts) and [third-party integrations](/docs/integrations/providers/).\\nUse [LangGraph](/docs/concepts/architecture/#langgraph) to build stateful agents with first-class streaming and human-in-the-loop support.\\n- **Productionization**: Use [LangSmith](https://docs.smith.langchain.com/) to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\n- **Deployment**: Turn your LangGraph applications into production-ready APIs and Assistants with [LangGraph Platform](https://langchain-ai.github.io/langgraph/cloud/).\\n\\nimport ThemedImage from \\'@theme/ThemedImage\\';\\nimport useBaseUrl from \\'@docusaurus/useBaseUrl\\';\\n\\n<ThemedImage\\n  alt=\"Diagram outlining the hierarchical organization of the LangChain framework, displaying the interconnected parts across multiple layers.\"\\n  sources={{\\n    light: useBaseUrl(\\'/svg/langchain_stack_112024.svg\\'),\\n    dark: useBaseUrl(\\'/svg/langchain_stack_112024_dark.svg\\'),\\n  }}\\n  style={{ width: \"100%\" }}\\n  title=\"LangChain Framework Overview\"\\n/>\\n\\nLangChain implements a standard interface for large language models and related\\ntechnologies, such as embedding models and vector stores, and integrates with\\nhundreds of providers. See the [integrations](/docs/integrations/providers/) page for\\nmore.\\n\\nimport ChatModelTabs from \"@theme/ChatModelTabs\";\\n\\n<ChatModelTabs/>\\n\\n```python\\nmodel.invoke(\"Hello, world!\")\\n```\\n\\n:::note\\n\\nThese docs focus on the Python LangChain library. [Head here](https://js.langchain.com) for docs on the JavaScript LangChain library.\\n\\n:::\\n\\n## Architecture\\n\\nThe LangChain framework consists of multiple open-source libraries. Read more in the\\n[Architecture](/docs/concepts/architecture/) page.\\n\\n- **`langchain-core`**: Base abstractions for chat models and other components.\\n- **Integration packages** (e.g. `langchain-openai`, `langchain-anthropic`, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\\n- **`langchain`**: Chains, agents, and retrieval strategies that make up an application\\'s cognitive architecture.\\n- **`langchain-community`**: Third-party integrations that are community maintained.\\n- **`langgraph`**: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See [LangGraph documentation](https://langchain-ai.github.io/langgraph/).\\n\\n## Guides\\n\\n### [Tutorials](/docs/tutorials)\\n\\nIf you\\'re looking to build something specific or are more of a hands-on learner, check out our [tutorials section](/docs/tutorials).\\nThis is the best place to get started.\\n\\nThese are the best ones to get started with:\\n\\n- [Build a Simple LLM Application](/docs/tutorials/llm_chain)\\n- [Build a Chatbot](/docs/tutorials/chatbot)\\n- [Build an Agent](/docs/tutorials/agents)\\n- [Introduction to LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/)\\n\\nExplore the full list of LangChain tutorials [here](/docs/tutorials), and check out other [LangGraph tutorials here](https://langchain-ai.github.io/langgraph/tutorials/). To learn more about LangGraph, check out our first LangChain Academy course, *Introduction to LangGraph*, available [here](https://academy.langchain.com/courses/intro-to-langgraph).\\n\\n\\n### [How-to guides](/docs/how_to)\\n\\n[Here](/docs/how_to) you’ll find short answers to “How do I….?” types of questions.\\nThese how-to guides don’t cover topics in depth – you’ll find that material in the [Tutorials](/docs/tutorials) and the [API Reference](https://python.langchain.com/api_reference/).\\nHowever, these guides will help you quickly accomplish common tasks using [chat models](/docs/how_to/#chat-models),\\n[vector stores](/docs/how_to/#vector-stores), and other common LangChain components.\\n\\nCheck out [LangGraph-specific how-tos here](https://langchain-ai.github.io/langgraph/how-tos/).\\n\\n### [Conceptual guide](/docs/concepts)\\n\\nIntroductions to all the key parts of LangChain you’ll need to know! [Here](/docs/concepts) you\\'ll find high level explanations of all LangChain concepts.\\n\\nFor a deeper dive into LangGraph concepts, check out [this page](https://langchain-ai.github.io/langgraph/concepts/).\\n\\n### [Integrations](integrations/providers/index.mdx)\\n\\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\\nIf you\\'re looking to get up and running quickly with [chat models](/docs/integrations/chat/), [vector stores](/docs/integrations/vectorstores/),\\nor other LangChain components from a specific provider, check out our growing list of [integrations](/docs/integrations/providers/).\\n\\n\\n### [API reference](https://python.langchain.com/api_reference/)\\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\\n\\n## Ecosystem\\n\\n### [🦜🛠️ LangSmith](https://docs.smith.langchain.com)\\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\\n\\n### [🦜🕸️ LangGraph](https://langchain-ai.github.io/langgraph)\\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by Linkedin, Uber, Klarna, GitLab, and many more.\\n\\n## Additional resources\\n\\n### [Versions](/docs/versions/v0_3/)\\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\\n\\n### [Security](/docs/security)\\nRead up on [security](/docs/security) best practices to make sure you\\'re developing safely with LangChain.\\n\\n### [Contributing](contributing/index.mdx)\\nCheck out the developer\\'s guide for guidelines on contributing and help getting your dev environment set up.\\n': 0.048915917503966164, '# LangChain Expression Language (LCEL)\\n\\n:::info Prerequisites\\n* [Runnable Interface](/docs/concepts/runnables)\\n:::\\n\\nThe **L**ang**C**hain **E**xpression **L**anguage (LCEL) takes a [declarative](https://en.wikipedia.org/wiki/Declarative_programming) approach to building new [Runnables](/docs/concepts/runnables) from existing Runnables.\\n\\nThis means that you describe what *should* happen, rather than *how* it should happen, allowing LangChain to optimize the run-time execution of the chains.\\n\\nWe often refer to a `Runnable` created using LCEL as a \"chain\". It\\'s important to remember that a \"chain\" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).\\n\\n:::note\\n* The [LCEL cheatsheet](/docs/how_to/lcel_cheatsheet/) shows common patterns that involve the Runnable interface and LCEL expressions.\\n* Please see the following list of [how-to guides](/docs/how_to/#langchain-expression-language-lcel) that cover common tasks with LCEL.\\n* A list of built-in `Runnables` can be found in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html). Many of these Runnables are useful when composing custom \"chains\" in LangChain using LCEL.\\n:::\\n\\n## Benefits of LCEL\\n\\nLangChain optimizes the run-time execution of chains built with LCEL in a number of ways:\\n\\n- **Optimized parallel execution**: Run Runnables in parallel using [RunnableParallel](#runnableparallel) or run multiple inputs through a given chain in parallel using the [Runnable Batch API](/docs/concepts/runnables/#optimized-parallel-execution-batch). Parallel execution can significantly reduce the latency as processing can be done in parallel instead of sequentially.\\n- **Guaranteed Async support**: Any chain built with LCEL can be run asynchronously using the [Runnable Async API](/docs/concepts/runnables/#asynchronous-support). This can be useful when running chains in a server environment where you want to handle large number of requests concurrently.\\n- **Simplify streaming**: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(time elapsed until the first chunk of output from a [chat model](/docs/concepts/chat_models) or [llm](/docs/concepts/text_llms) comes out).\\n\\nOther benefits include:\\n\\n- [**Seamless LangSmith tracing**](https://docs.smith.langchain.com)\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, **all** steps are automatically logged to [LangSmith](https://docs.smith.langchain.com/) for maximum observability and debuggability.\\n- **Standard API**: Because all chains are built using the Runnable interface, they can be used in the same way as any other Runnable.\\n- [**Deployable with LangServe**](/docs/concepts/architecture#langserve): Chains built with LCEL can be deployed using for production use.\\n\\n## Should I use LCEL?\\n\\nLCEL is an [orchestration solution](https://en.wikipedia.org/wiki/Orchestration_(computing)) -- it allows LangChain to handle run-time execution of chains in an optimized way.\\n\\nWhile we have seen users run chains with hundreds of steps in production, we generally recommend using LCEL for simpler orchestration tasks. When the application requires complex state management, branching, cycles or multiple agents, we recommend that users take advantage of [LangGraph](/docs/concepts/architecture#langgraph).\\n\\nIn LangGraph, users define graphs that specify the application\\'s flow. This allows users to keep using LCEL within individual nodes when LCEL is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.\\n\\nHere are some guidelines:\\n\\n* If you are making a single LLM call, you don\\'t need LCEL; instead call the underlying [chat model](/docs/concepts/chat_models) directly.\\n* If you have a simple chain (e.g., prompt + llm + parser, simple retrieval set up etc.), LCEL is a reasonable fit, if you\\'re taking advantage of the LCEL benefits.\\n* If you\\'re building a complex chain (e.g., with branching, cycles, multiple agents, etc.) use [LangGraph](/docs/concepts/architecture#langgraph) instead. Remember that you can always use LCEL within individual nodes in LangGraph.\\n\\n## Composition Primitives\\n\\n`LCEL` chains are built by composing existing `Runnables` together. The two main composition primitives are [RunnableSequence](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableSequence.html#langchain_core.runnables.base.RunnableSequence) and [RunnableParallel](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableParallel.html#langchain_core.runnables.base.RunnableParallel).\\n\\nMany other composition primitives (e.g., [RunnableAssign](\\nhttps://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnableAssign.html#langchain_core.runnables.passthrough.RunnableAssign\\n)) can be thought of as variations of these two primitives.\\n\\n:::note\\nYou can find a list of all composition primitives in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html).\\n:::\\n\\n### RunnableSequence\\n\\n`RunnableSequence` is a composition primitive that allows you \"chain\" multiple runnables sequentially, with the output of one runnable serving as the input to the next.\\n\\n```python\\nfrom langchain_core.runnables import RunnableSequence\\nchain = RunnableSequence([runnable1, runnable2])\\n```\\n\\nInvoking the `chain` with some input:\\n\\n```python\\nfinal_output = chain.invoke(some_input)\\n```\\n\\ncorresponds to the following:\\n\\n```python\\noutput1 = runnable1.invoke(some_input)\\nfinal_output = runnable2.invoke(output1)\\n```\\n\\n:::note\\n`runnable1` and `runnable2` are placeholders for any `Runnable` that you want to chain together.\\n:::\\n\\n### RunnableParallel\\n\\n`RunnableParallel` is a composition primitive that allows you to run multiple runnables concurrently, with the same input provided to each.\\n\\n```python\\nfrom langchain_core.runnables import RunnableParallel\\nchain = RunnableParallel({\\n    \"key1\": runnable1,\\n    \"key2\": runnable2,\\n})\\n```\\n\\nInvoking the `chain` with some input:\\n\\n```python\\nfinal_output = chain.invoke(some_input)\\n```\\n\\nWill yield a `final_output` dictionary with the same keys as the input dictionary, but with the values replaced by the output of the corresponding runnable.\\n\\n```python\\n{\\n    \"key1\": runnable1.invoke(some_input),\\n    \"key2\": runnable2.invoke(some_input),\\n}\\n```\\n\\nRecall, that the runnables are executed in parallel, so while the result is the same as\\ndictionary comprehension shown above, the execution time is much faster.\\n\\n:::note\\n`RunnableParallel`supports both synchronous and asynchronous execution (as all `Runnables` do).\\n\\n* For synchronous execution, `RunnableParallel` uses a [ThreadPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor) to run the runnables concurrently.\\n* For asynchronous execution, `RunnableParallel` uses [asyncio.gather](https://docs.python.org/3/library/asyncio.html#asyncio.gather) to run the runnables concurrently.\\n:::\\n\\n## Composition Syntax\\n\\nThe usage of `RunnableSequence` and `RunnableParallel` is so common that we created a shorthand syntax for using them. This helps\\nto make the code more readable and concise.\\n\\n### The `|` operator\\n\\nWe have [overloaded](https://docs.python.org/3/reference/datamodel.html#special-method-names) the `|` operator to create a `RunnableSequence` from two `Runnables`.\\n\\n```python\\nchain = runnable1 | runnable2\\n```\\n\\nis Equivalent to:\\n\\n```python\\nchain = RunnableSequence([runnable1, runnable2])\\n```\\n\\n### The `.pipe` method\\n\\nIf you have moral qualms with operator overloading, you can use the `.pipe` method instead. This is equivalent to the `|` operator.\\n\\n```python\\nchain = runnable1.pipe(runnable2)\\n```\\n\\n### Coercion\\n\\nLCEL applies automatic type coercion to make it easier to compose chains.\\n\\nIf you do not understand the type coercion, you can always use the `RunnableSequence` and `RunnableParallel` classes directly.\\n\\nThis will make the code more verbose, but it will also make it more explicit.\\n\\n#### Dictionary to RunnableParallel\\n\\nInside an LCEL expression, a dictionary is automatically converted to a `RunnableParallel`.\\n\\nFor example, the following code:\\n\\n```python\\nmapping = {\\n    \"key1\": runnable1,\\n    \"key2\": runnable2,\\n}\\n\\nchain = mapping | runnable3\\n```\\n\\nIt gets automatically converted to the following:\\n\\n```python\\nchain = RunnableSequence([RunnableParallel(mapping), runnable3])\\n```\\n\\n:::caution\\nYou have to be careful because the `mapping` dictionary is not a `RunnableParallel` object, it is just a dictionary. This means that the following code will raise an `AttributeError`:\\n\\n```python\\nmapping.invoke(some_input)\\n```\\n:::\\n\\n#### Function to RunnableLambda\\n\\nInside an LCEL expression, a function is automatically converted to a `RunnableLambda`.\\n\\n```\\ndef some_func(x):\\n    return x\\n\\nchain = some_func | runnable1\\n```\\n\\nIt gets automatically converted to the following:\\n\\n```python\\nchain = RunnableSequence([RunnableLambda(some_func), runnable1])\\n```\\n\\n:::caution\\nYou have to be careful because the lambda function is not a `RunnableLambda` object, it is just a function. This means that the following code will raise an `AttributeError`:\\n\\n```python\\nlambda x: x + 1.invoke(some_input)\\n```\\n:::\\n\\n## Legacy chains\\n\\nLCEL aims to provide consistency around behavior and customization over legacy subclassed chains such as `LLMChain` and\\n`ConversationalRetrievalChain`. Many of these legacy chains hide important details like prompts, and as a wider variety\\nof viable models emerge, customization has become more and more important.\\n\\nIf you are currently using one of these legacy chains, please see [this guide for guidance on how to migrate](/docs/versions/migrating_chains).\\n\\nFor guides on how to do specific tasks with LCEL, check out [the relevant how-to guides](/docs/how_to/#langchain-expression-language-lcel).\\n': 0.048651507139079855, '# Conceptual guide\\n\\nThis guide provides explanations of the key concepts behind the LangChain framework and AI applications more broadly.\\n\\nWe recommend that you go through at least one of the [Tutorials](/docs/tutorials) before diving into the conceptual guide. This will provide practical context that will make it easier to understand the concepts discussed here.\\n\\nThe conceptual guide does not cover step-by-step instructions or specific implementation examples — those are found in the [How-to guides](/docs/how_to/) and [Tutorials](/docs/tutorials). For detailed reference material, please see the [API reference](https://python.langchain.com/api_reference/).\\n\\n## High level\\n\\n- **[Why LangChain?](/docs/concepts/why_langchain)**: Overview of the value that LangChain provides.\\n- **[Architecture](/docs/concepts/architecture)**: How packages are organized in the LangChain ecosystem.\\n\\n## Concepts\\n\\n- **[Chat models](/docs/concepts/chat_models)**: LLMs exposed via a chat API that process sequences of messages as input and output a message.\\n- **[Messages](/docs/concepts/messages)**: The unit of communication in chat models, used to represent model input and output.\\n- **[Chat history](/docs/concepts/chat_history)**: A conversation represented as a sequence of messages, alternating between user messages and model responses.\\n- **[Tools](/docs/concepts/tools)**: A function with an associated schema defining the function\\'s name, description, and the arguments it accepts.\\n- **[Tool calling](/docs/concepts/tool_calling)**: A type of chat model API that accepts tool schemas, along with messages, as input and returns invocations of those tools as part of the output message.\\n- **[Structured output](/docs/concepts/structured_outputs)**: A technique to make a chat model respond in a structured format, such as JSON that matches a given schema.\\n- **[Memory](https://langchain-ai.github.io/langgraph/concepts/memory/)**: Information about a conversation that is persisted so that it can be used in future conversations.\\n- **[Multimodality](/docs/concepts/multimodality)**: The ability to work with data that comes in different forms, such as text, audio, images, and video.\\n- **[Runnable interface](/docs/concepts/runnables)**: The base abstraction that many LangChain components and the LangChain Expression Language are built on.\\n- **[Streaming](/docs/concepts/streaming)**: LangChain streaming APIs for surfacing results as they are generated.\\n- **[LangChain Expression Language (LCEL)](/docs/concepts/lcel)**: A syntax for orchestrating LangChain components. Most useful for simpler applications.\\n- **[Document loaders](/docs/concepts/document_loaders)**: Load a source as a list of documents.\\n- **[Retrieval](/docs/concepts/retrieval)**: Information retrieval systems can retrieve structured or unstructured data from a datasource in response to a query.\\n- **[Text splitters](/docs/concepts/text_splitters)**: Split long text into smaller chunks that can be individually indexed to enable granular retrieval.\\n- **[Embedding models](/docs/concepts/embedding_models)**: Models that represent data such as text or images in a vector space.\\n- **[Vector stores](/docs/concepts/vectorstores)**: Storage of and efficient search over vectors and associated metadata.\\n- **[Retriever](/docs/concepts/retrievers)**: A component that returns relevant documents from a knowledge base in response to a query.\\n- **[Retrieval Augmented Generation (RAG)](/docs/concepts/rag)**: A technique that enhances language models by combining them with external knowledge bases.\\n- **[Agents](/docs/concepts/agents)**: Use a [language model](/docs/concepts/chat_models) to choose a sequence of actions to take. Agents can interact with external resources via [tool](/docs/concepts/tools).\\n- **[Prompt templates](/docs/concepts/prompt_templates)**: Component for factoring out the static parts of a model \"prompt\" (usually a sequence of messages). Useful for serializing, versioning, and reusing these static parts.\\n- **[Output parsers](/docs/concepts/output_parsers)**: Responsible for taking the output of a model and transforming it into a more suitable format for downstream tasks. Output parsers were primarily useful prior to the general availability of [tool calling](/docs/concepts/tool_calling) and [structured outputs](/docs/concepts/structured_outputs).\\n- **[Few-shot prompting](/docs/concepts/few_shot_prompting)**: A technique for improving model performance by providing a few examples of the task to perform in the prompt.\\n- **[Example selectors](/docs/concepts/example_selectors)**: Used to select the most relevant examples from a dataset based on a given input. Example selectors are used in few-shot prompting to select examples for a prompt.\\n- **[Async programming](/docs/concepts/async)**: The basics that one should know to use LangChain in an asynchronous context.\\n- **[Callbacks](/docs/concepts/callbacks)**: Callbacks enable the execution of custom auxiliary code in built-in components. Callbacks are used to stream outputs from LLMs in LangChain, trace the intermediate steps of an application, and more.\\n- **[Tracing](/docs/concepts/tracing)**: The process of recording the steps that an application takes to go from input to output. Tracing is essential for debugging and diagnosing issues in complex applications.\\n- **[Evaluation](/docs/concepts/evaluation)**: The process of assessing the performance and effectiveness of AI applications. This involves testing the model\\'s responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose. This process is vital for building reliable applications.\\n- **[Testing](/docs/concepts/testing)**: The process of verifying that a component of an integration or application works as expected. Testing is essential for ensuring that the application behaves correctly and that changes to the codebase do not introduce new bugs.\\n\\n## Glossary\\n\\n- **[AIMessageChunk](/docs/concepts/messages#aimessagechunk)**: A partial response from an AI message. Used when streaming responses from a chat model.\\n- **[AIMessage](/docs/concepts/messages#aimessage)**: Represents a complete response from an AI model.\\n- **[astream_events](/docs/concepts/chat_models#key-methods)**: Stream granular information from [LCEL](/docs/concepts/lcel) chains.\\n- **[BaseTool](/docs/concepts/tools/#tool-interface)**: The base class for all tools in LangChain.\\n- **[batch](/docs/concepts/runnables)**: Use to execute a runnable with batch inputs.\\n- **[bind_tools](/docs/concepts/tool_calling/#tool-binding)**: Allows models to interact with tools.\\n- **[Caching](/docs/concepts/chat_models#caching)**: Storing results to avoid redundant calls to a chat model.\\n- **[Chat models](/docs/concepts/multimodality/#multimodality-in-chat-models)**: Chat models that handle multiple data modalities.\\n- **[Configurable runnables](/docs/concepts/runnables/#configurable-runnables)**: Creating configurable Runnables.\\n- **[Context window](/docs/concepts/chat_models#context-window)**: The maximum size of input a chat model can process.\\n- **[Conversation patterns](/docs/concepts/chat_history#conversation-patterns)**: Common patterns in chat interactions.\\n- **[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)**: LangChain\\'s representation of a document.\\n- **[Embedding models](/docs/concepts/multimodality/#multimodality-in-embedding-models)**: Models that generate vector embeddings for various data types.\\n- **[HumanMessage](/docs/concepts/messages#humanmessage)**: Represents a message from a human user.\\n- **[InjectedState](/docs/concepts/tools#injectedstate)**: A state injected into a tool function.\\n- **[InjectedStore](/docs/concepts/tools#injectedstore)**: A store that can be injected into a tool for data persistence.\\n- **[InjectedToolArg](/docs/concepts/tools#injectedtoolarg)**: Mechanism to inject arguments into tool functions.\\n- **[input and output types](/docs/concepts/runnables#input-and-output-types)**: Types used for input and output in Runnables.\\n- **[Integration packages](/docs/concepts/architecture/#integration-packages)**: Third-party packages that integrate with LangChain.\\n- **[Integration tests](/docs/concepts/testing#integration-tests)**: Tests that verify the correctness of the interaction between components, usually run with access to the underlying API that powers an integration.\\n- **[invoke](/docs/concepts/runnables)**: A standard method to invoke a Runnable.\\n- **[JSON mode](/docs/concepts/structured_outputs#json-mode)**: Returning responses in JSON format.\\n- **[langchain-community](/docs/concepts/architecture#langchain-community)**: Community-driven components for LangChain.\\n- **[langchain-core](/docs/concepts/architecture#langchain-core)**: Core langchain package. Includes base interfaces and in-memory implementations.\\n- **[langchain](/docs/concepts/architecture#langchain)**: A package for higher level components (e.g., some pre-built chains).\\n- **[langgraph](/docs/concepts/architecture#langgraph)**: Powerful orchestration layer for LangChain. Use to build complex pipelines and workflows.\\n- **[langserve](/docs/concepts/architecture#langserve)**: Used to deploy LangChain Runnables as REST endpoints. Uses FastAPI. Works primarily for LangChain Runnables, does not currently integrate with LangGraph.\\n- **[LLMs (legacy)](/docs/concepts/text_llms)**: Older language models that take a string as input and return a string as output.\\n- **[Managing chat history](/docs/concepts/chat_history#managing-chat-history)**: Techniques to maintain and manage the chat history.\\n- **[OpenAI format](/docs/concepts/messages#openai-format)**: OpenAI\\'s message format for chat models.\\n- **[Propagation of RunnableConfig](/docs/concepts/runnables/#propagation-of-runnableconfig)**: Propagating configuration through Runnables. Read if working with python 3.9, 3.10 and async.\\n- **[rate-limiting](/docs/concepts/chat_models#rate-limiting)**: Client side rate limiting for chat models.\\n- **[RemoveMessage](/docs/concepts/messages/#removemessage)**: An abstraction used to remove a message from chat history, used primarily in LangGraph.\\n- **[role](/docs/concepts/messages#role)**: Represents the role (e.g., user, assistant) of a chat message.\\n- **[RunnableConfig](/docs/concepts/runnables/#runnableconfig)**: Use to pass run time information to Runnables (e.g., `run_name`, `run_id`, `tags`, `metadata`, `max_concurrency`, `recursion_limit`, `configurable`).\\n- **[Standard parameters for chat models](/docs/concepts/chat_models#standard-parameters)**: Parameters such as API key, `temperature`, and `max_tokens`.\\n- **[Standard tests](/docs/concepts/testing#standard-tests)**: A defined set of unit and integration tests that all integrations must pass.\\n- **[stream](/docs/concepts/streaming)**: Use to stream output from a Runnable or a graph.\\n- **[Tokenization](/docs/concepts/tokens)**: The process of converting data into tokens and vice versa.\\n- **[Tokens](/docs/concepts/tokens)**: The basic unit that a language model reads, processes, and generates under the hood.\\n- **[Tool artifacts](/docs/concepts/tools#tool-artifacts)**: Add artifacts to the output of a tool that will not be sent to the model, but will be available for downstream processing.\\n- **[Tool binding](/docs/concepts/tool_calling#tool-binding)**: Binding tools to models.\\n- **[@tool](/docs/concepts/tools/#create-tools-using-the-tool-decorator)**: Decorator for creating tools in LangChain.\\n- **[Toolkits](/docs/concepts/tools#toolkits)**: A collection of tools that can be used together.\\n- **[ToolMessage](/docs/concepts/messages#toolmessage)**: Represents a message that contains the results of a tool execution.\\n- **[Unit tests](/docs/concepts/testing#unit-tests)**: Tests that verify the correctness of individual components, run in isolation without access to the Internet.\\n- **[Vector stores](/docs/concepts/vectorstores)**: Datastores specialized for storing and efficiently searching vector embeddings.\\n- **[with_structured_output](/docs/concepts/structured_outputs/#structured-output-method)**: A helper method for chat models that natively support [tool calling](/docs/concepts/tool_calling) to get structured output matching a given schema specified via Pydantic, JSON schema or a function.\\n- **[with_types](/docs/concepts/runnables#with_types)**: Method to overwrite the input and output types of a runnable. Useful when working with complex LCEL chains and deploying with LangServe.\\n': 0.015873015873015872, '---\\nsidebar_position: 0\\nsidebar_class_name: hidden\\n---\\n\\n# How-to guides\\n\\nHere you’ll find answers to “How do I….?” types of questions.\\nThese guides are *goal-oriented* and *concrete*; they\\'re meant to help you complete a specific task.\\nFor conceptual explanations see the [Conceptual guide](/docs/concepts/).\\nFor end-to-end walkthroughs see [Tutorials](/docs/tutorials).\\nFor comprehensive descriptions of every class and function see the [API Reference](https://python.langchain.com/api_reference/).\\n\\n## Installation\\n\\n- [How to: install LangChain packages](/docs/how_to/installation/)\\n- [How to: use LangChain with different Pydantic versions](/docs/how_to/pydantic_compatibility)\\n\\n## Key features\\n\\nThis highlights functionality that is core to using LangChain.\\n\\n- [How to: return structured data from a model](/docs/how_to/structured_output/)\\n- [How to: use a model to call tools](/docs/how_to/tool_calling)\\n- [How to: stream runnables](/docs/how_to/streaming)\\n- [How to: debug your LLM apps](/docs/how_to/debugging/)\\n\\n## Components\\n\\nThese are the core building blocks you can use when building applications.\\n\\n### Chat models\\n\\n[Chat Models](/docs/concepts/chat_models) are newer forms of language models that take messages in and output a message.\\nSee [supported integrations](/docs/integrations/chat/) for details on getting started with chat models from a specific provider.\\n\\n- [How to: do function/tool calling](/docs/how_to/tool_calling)\\n- [How to: get models to return structured output](/docs/how_to/structured_output)\\n- [How to: cache model responses](/docs/how_to/chat_model_caching)\\n- [How to: get log probabilities](/docs/how_to/logprobs)\\n- [How to: create a custom chat model class](/docs/how_to/custom_chat_model)\\n- [How to: stream a response back](/docs/how_to/chat_streaming)\\n- [How to: track token usage](/docs/how_to/chat_token_usage_tracking)\\n- [How to: track response metadata across providers](/docs/how_to/response_metadata)\\n- [How to: use chat model to call tools](/docs/how_to/tool_calling)\\n- [How to: stream tool calls](/docs/how_to/tool_streaming)\\n- [How to: handle rate limits](/docs/how_to/chat_model_rate_limiting)\\n- [How to: few shot prompt tool behavior](/docs/how_to/tools_few_shot)\\n- [How to: bind model-specific formatted tools](/docs/how_to/tools_model_specific)\\n- [How to: force a specific tool call](/docs/how_to/tool_choice)\\n- [How to: work with local models](/docs/how_to/local_llms)\\n- [How to: init any model in one line](/docs/how_to/chat_models_universal_init/)\\n- [How to: pass multimodal data directly to models](/docs/how_to/multimodal_inputs/)\\n\\n### Messages\\n\\n[Messages](/docs/concepts/messages) are the input and output of chat models. They have some `content` and a `role`, which describes the source of the message.\\n\\n- [How to: trim messages](/docs/how_to/trim_messages/)\\n- [How to: filter messages](/docs/how_to/filter_messages/)\\n- [How to: merge consecutive messages of the same type](/docs/how_to/merge_message_runs/)\\n\\n### Prompt templates\\n\\n[Prompt Templates](/docs/concepts/prompt_templates) are responsible for formatting user input into a format that can be passed to a language model.\\n\\n- [How to: use few shot examples](/docs/how_to/few_shot_examples)\\n- [How to: use few shot examples in chat models](/docs/how_to/few_shot_examples_chat/)\\n- [How to: partially format prompt templates](/docs/how_to/prompts_partial)\\n- [How to: compose prompts together](/docs/how_to/prompts_composition)\\n- [How to: use multimodal prompts](/docs/how_to/multimodal_prompts/)\\n\\n### Example selectors\\n\\n[Example Selectors](/docs/concepts/example_selectors) are responsible for selecting the correct few shot examples to pass to the prompt.\\n\\n- [How to: use example selectors](/docs/how_to/example_selectors)\\n- [How to: select examples by length](/docs/how_to/example_selectors_length_based)\\n- [How to: select examples by semantic similarity](/docs/how_to/example_selectors_similarity)\\n- [How to: select examples by semantic ngram overlap](/docs/how_to/example_selectors_ngram)\\n- [How to: select examples by maximal marginal relevance](/docs/how_to/example_selectors_mmr)\\n- [How to: select examples from LangSmith few-shot datasets](/docs/how_to/example_selectors_langsmith/)\\n\\n### LLMs\\n\\nWhat LangChain calls [LLMs](/docs/concepts/text_llms) are older forms of language models that take a string in and output a string.\\n\\n- [How to: cache model responses](/docs/how_to/llm_caching)\\n- [How to: create a custom LLM class](/docs/how_to/custom_llm)\\n- [How to: stream a response back](/docs/how_to/streaming_llm)\\n- [How to: track token usage](/docs/how_to/llm_token_usage_tracking)\\n- [How to: work with local models](/docs/how_to/local_llms)\\n\\n### Output parsers\\n\\n[Output Parsers](/docs/concepts/output_parsers) are responsible for taking the output of an LLM and parsing into more structured format.\\n\\n- [How to: parse text from message objects](/docs/how_to/output_parser_string)\\n- [How to: use output parsers to parse an LLM response into structured format](/docs/how_to/output_parser_structured)\\n- [How to: parse JSON output](/docs/how_to/output_parser_json)\\n- [How to: parse XML output](/docs/how_to/output_parser_xml)\\n- [How to: parse YAML output](/docs/how_to/output_parser_yaml)\\n- [How to: retry when output parsing errors occur](/docs/how_to/output_parser_retry)\\n- [How to: try to fix errors in output parsing](/docs/how_to/output_parser_fixing)\\n- [How to: write a custom output parser class](/docs/how_to/output_parser_custom)\\n\\n### Document loaders\\n\\n[Document Loaders](/docs/concepts/document_loaders) are responsible for loading documents from a variety of sources.\\n\\n- [How to: load PDF files](/docs/how_to/document_loader_pdf)\\n- [How to: load web pages](/docs/how_to/document_loader_web)\\n- [How to: load CSV data](/docs/how_to/document_loader_csv)\\n- [How to: load data from a directory](/docs/how_to/document_loader_directory)\\n- [How to: load HTML data](/docs/how_to/document_loader_html)\\n- [How to: load JSON data](/docs/how_to/document_loader_json)\\n- [How to: load Markdown data](/docs/how_to/document_loader_markdown)\\n- [How to: load Microsoft Office data](/docs/how_to/document_loader_office_file)\\n- [How to: write a custom document loader](/docs/how_to/document_loader_custom)\\n\\n### Text splitters\\n\\n[Text Splitters](/docs/concepts/text_splitters) take a document and split into chunks that can be used for retrieval.\\n\\n- [How to: recursively split text](/docs/how_to/recursive_text_splitter)\\n- [How to: split HTML](/docs/how_to/split_html)\\n- [How to: split by character](/docs/how_to/character_text_splitter)\\n- [How to: split code](/docs/how_to/code_splitter)\\n- [How to: split Markdown by headers](/docs/how_to/markdown_header_metadata_splitter)\\n- [How to: recursively split JSON](/docs/how_to/recursive_json_splitter)\\n- [How to: split text into semantic chunks](/docs/how_to/semantic-chunker)\\n- [How to: split by tokens](/docs/how_to/split_by_token)\\n\\n### Embedding models\\n\\n[Embedding Models](/docs/concepts/embedding_models) take a piece of text and create a numerical representation of it.\\nSee [supported integrations](/docs/integrations/text_embedding/) for details on getting started with embedding models from a specific provider.\\n\\n- [How to: embed text data](/docs/how_to/embed_text)\\n- [How to: cache embedding results](/docs/how_to/caching_embeddings)\\n- [How to: create a custom embeddings class](/docs/how_to/custom_embeddings)\\n\\n### Vector stores\\n\\n[Vector stores](/docs/concepts/vectorstores) are databases that can efficiently store and retrieve embeddings.\\nSee [supported integrations](/docs/integrations/vectorstores/) for details on getting started with vector stores from a specific provider.\\n\\n- [How to: use a vector store to retrieve data](/docs/how_to/vectorstores)\\n\\n### Retrievers\\n\\n[Retrievers](/docs/concepts/retrievers) are responsible for taking a query and returning relevant documents.\\n\\n- [How to: use a vector store to retrieve data](/docs/how_to/vectorstore_retriever)\\n- [How to: generate multiple queries to retrieve data for](/docs/how_to/MultiQueryRetriever)\\n- [How to: use contextual compression to compress the data retrieved](/docs/how_to/contextual_compression)\\n- [How to: write a custom retriever class](/docs/how_to/custom_retriever)\\n- [How to: add similarity scores to retriever results](/docs/how_to/add_scores_retriever)\\n- [How to: combine the results from multiple retrievers](/docs/how_to/ensemble_retriever)\\n- [How to: reorder retrieved results to mitigate the \"lost in the middle\" effect](/docs/how_to/long_context_reorder)\\n- [How to: generate multiple embeddings per document](/docs/how_to/multi_vector)\\n- [How to: retrieve the whole document for a chunk](/docs/how_to/parent_document_retriever)\\n- [How to: generate metadata filters](/docs/how_to/self_query)\\n- [How to: create a time-weighted retriever](/docs/how_to/time_weighted_vectorstore)\\n- [How to: use hybrid vector and keyword retrieval](/docs/how_to/hybrid)\\n\\n### Indexing\\n\\nIndexing is the process of keeping your vectorstore in-sync with the underlying data source.\\n\\n- [How to: reindex data to keep your vectorstore in-sync with the underlying data source](/docs/how_to/indexing)\\n\\n### Tools\\n\\nLangChain [Tools](/docs/concepts/tools) contain a description of the tool (to pass to the language model) as well as the implementation of the function to call. Refer [here](/docs/integrations/tools/) for a list of pre-built tools. \\n\\n- [How to: create tools](/docs/how_to/custom_tools)\\n- [How to: use built-in tools and toolkits](/docs/how_to/tools_builtin)\\n- [How to: use chat models to call tools](/docs/how_to/tool_calling)\\n- [How to: pass tool outputs to chat models](/docs/how_to/tool_results_pass_to_model)\\n- [How to: pass run time values to tools](/docs/how_to/tool_runtime)\\n- [How to: add a human-in-the-loop for tools](/docs/how_to/tools_human)\\n- [How to: handle tool errors](/docs/how_to/tools_error)\\n- [How to: force models to call a tool](/docs/how_to/tool_choice)\\n- [How to: disable parallel tool calling](/docs/how_to/tool_calling_parallel)\\n- [How to: access the `RunnableConfig` from a tool](/docs/how_to/tool_configure)\\n- [How to: stream events from a tool](/docs/how_to/tool_stream_events)\\n- [How to: return artifacts from a tool](/docs/how_to/tool_artifacts/)\\n- [How to: convert Runnables to tools](/docs/how_to/convert_runnable_to_tool)\\n- [How to: add ad-hoc tool calling capability to models](/docs/how_to/tools_prompting)\\n- [How to: pass in runtime secrets](/docs/how_to/runnable_runtime_secrets)\\n\\n### Multimodal\\n\\n- [How to: pass multimodal data directly to models](/docs/how_to/multimodal_inputs/)\\n- [How to: use multimodal prompts](/docs/how_to/multimodal_prompts/)\\n\\n\\n### Agents\\n\\n:::note\\n\\nFor in depth how-to guides for agents, please check out [LangGraph](https://langchain-ai.github.io/langgraph/) documentation.\\n\\n:::\\n\\n- [How to: use legacy LangChain Agents (AgentExecutor)](/docs/how_to/agent_executor)\\n- [How to: migrate from legacy LangChain agents to LangGraph](/docs/how_to/migrate_agent)\\n\\n### Callbacks\\n\\n[Callbacks](/docs/concepts/callbacks) allow you to hook into the various stages of your LLM application\\'s execution.\\n\\n- [How to: pass in callbacks at runtime](/docs/how_to/callbacks_runtime)\\n- [How to: attach callbacks to a module](/docs/how_to/callbacks_attach)\\n- [How to: pass callbacks into a module constructor](/docs/how_to/callbacks_constructor)\\n- [How to: create custom callback handlers](/docs/how_to/custom_callbacks)\\n- [How to: use callbacks in async environments](/docs/how_to/callbacks_async)\\n- [How to: dispatch custom callback events](/docs/how_to/callbacks_custom_events)\\n\\n### Custom\\n\\nAll of LangChain components can easily be extended to support your own versions.\\n\\n- [How to: create a custom chat model class](/docs/how_to/custom_chat_model)\\n- [How to: create a custom LLM class](/docs/how_to/custom_llm)\\n- [How to: create a custom embeddings class](/docs/how_to/custom_embeddings)\\n- [How to: write a custom retriever class](/docs/how_to/custom_retriever)\\n- [How to: write a custom document loader](/docs/how_to/document_loader_custom)\\n- [How to: write a custom output parser class](/docs/how_to/output_parser_custom)\\n- [How to: create custom callback handlers](/docs/how_to/custom_callbacks)\\n- [How to: define a custom tool](/docs/how_to/custom_tools)\\n- [How to: dispatch custom callback events](/docs/how_to/callbacks_custom_events)\\n\\n### Serialization\\n- [How to: save and load LangChain objects](/docs/how_to/serialization)\\n\\n## Use cases\\n\\nThese guides cover use-case specific details.\\n\\n### Q&A with RAG\\n\\nRetrieval Augmented Generation (RAG) is a way to connect LLMs to external sources of data.\\nFor a high-level tutorial on RAG, check out [this guide](/docs/tutorials/rag/).\\n\\n- [How to: add chat history](/docs/how_to/qa_chat_history_how_to/)\\n- [How to: stream](/docs/how_to/qa_streaming/)\\n- [How to: return sources](/docs/how_to/qa_sources/)\\n- [How to: return citations](/docs/how_to/qa_citations/)\\n- [How to: do per-user retrieval](/docs/how_to/qa_per_user/)\\n\\n\\n### Extraction\\n\\nExtraction is when you use LLMs to extract structured information from unstructured text.\\nFor a high level tutorial on extraction, check out [this guide](/docs/tutorials/extraction/).\\n\\n- [How to: use reference examples](/docs/how_to/extraction_examples/)\\n- [How to: handle long text](/docs/how_to/extraction_long_text/)\\n- [How to: do extraction without using function calling](/docs/how_to/extraction_parse)\\n\\n### Chatbots\\n\\nChatbots involve using an LLM to have a conversation.\\nFor a high-level tutorial on building chatbots, check out [this guide](/docs/tutorials/chatbot/).\\n\\n- [How to: manage memory](/docs/how_to/chatbots_memory)\\n- [How to: do retrieval](/docs/how_to/chatbots_retrieval)\\n- [How to: use tools](/docs/how_to/chatbots_tools)\\n- [How to: manage large chat history](/docs/how_to/trim_messages/)\\n\\n### Query analysis\\n\\nQuery Analysis is the task of using an LLM to generate a query to send to a retriever.\\nFor a high-level tutorial on query analysis, check out [this guide](/docs/tutorials/rag/#query-analysis).\\n\\n- [How to: add examples to the prompt](/docs/how_to/query_few_shot)\\n- [How to: handle cases where no queries are generated](/docs/how_to/query_no_queries)\\n- [How to: handle multiple queries](/docs/how_to/query_multiple_queries)\\n- [How to: handle multiple retrievers](/docs/how_to/query_multiple_retrievers)\\n- [How to: construct filters](/docs/how_to/query_constructing_filters)\\n- [How to: deal with high cardinality categorical variables](/docs/how_to/query_high_cardinality)\\n\\n### Q&A over SQL + CSV\\n\\nYou can use LLMs to do question answering over tabular data.\\nFor a high-level tutorial, check out [this guide](/docs/tutorials/sql_qa/).\\n\\n- [How to: use prompting to improve results](/docs/how_to/sql_prompting)\\n- [How to: do query validation](/docs/how_to/sql_query_checking)\\n- [How to: deal with large databases](/docs/how_to/sql_large_db)\\n- [How to: deal with CSV files](/docs/how_to/sql_csv)\\n\\n### Q&A over graph databases\\n\\nYou can use an LLM to do question answering over graph databases.\\nFor a high-level tutorial, check out [this guide](/docs/tutorials/graph/).\\n\\n- [How to: add a semantic layer over the database](/docs/how_to/graph_semantic)\\n- [How to: construct knowledge graphs](/docs/how_to/graph_constructing)\\n\\n### Summarization\\n\\nLLMs can summarize and otherwise distill desired information from text, including\\nlarge volumes of text. For a high-level tutorial, check out [this guide](/docs/tutorials/summarization).\\n\\n- [How to: summarize text in a single LLM call](/docs/how_to/summarize_stuff)\\n- [How to: summarize text through parallelization](/docs/how_to/summarize_map_reduce)\\n- [How to: summarize text through iterative refinement](/docs/how_to/summarize_refine)\\n\\n## LangChain Expression Language (LCEL)\\n\\n:::note Should I use LCEL?\\n\\nLCEL is an orchestration solution. See our\\n[concepts page](/docs/concepts/lcel/#should-i-use-lcel) for recommendations on when to\\nuse LCEL.\\n\\n:::\\n\\n[LangChain Expression Language](/docs/concepts/lcel) is a way to create arbitrary custom chains. It is built on the [Runnable](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html) protocol.\\n\\n[**LCEL cheatsheet**](/docs/how_to/lcel_cheatsheet/): For a quick overview of how to use the main LCEL primitives.\\n\\n[**Migration guide**](/docs/versions/migrating_chains): For migrating legacy chain abstractions to LCEL.\\n\\n- [How to: chain runnables](/docs/how_to/sequence)\\n- [How to: stream runnables](/docs/how_to/streaming)\\n- [How to: invoke runnables in parallel](/docs/how_to/parallel/)\\n- [How to: add default invocation args to runnables](/docs/how_to/binding/)\\n- [How to: turn any function into a runnable](/docs/how_to/functions)\\n- [How to: pass through inputs from one chain step to the next](/docs/how_to/passthrough)\\n- [How to: configure runnable behavior at runtime](/docs/how_to/configure)\\n- [How to: add message history (memory) to a chain](/docs/how_to/message_history)\\n- [How to: route between sub-chains](/docs/how_to/routing)\\n- [How to: create a dynamic (self-constructing) chain](/docs/how_to/dynamic_chain/)\\n- [How to: inspect runnables](/docs/how_to/inspect)\\n- [How to: add fallbacks to a runnable](/docs/how_to/fallbacks)\\n- [How to: pass runtime secrets to a runnable](/docs/how_to/runnable_runtime_secrets)\\n\\n## [LangGraph](https://langchain-ai.github.io/langgraph)\\n\\nLangGraph is an extension of LangChain aimed at\\nbuilding robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.\\n\\nLangGraph documentation is currently hosted on a separate site.\\nYou can peruse [LangGraph how-to guides here](https://langchain-ai.github.io/langgraph/how-tos/).\\n\\n## [LangSmith](https://docs.smith.langchain.com/)\\n\\nLangSmith allows you to closely trace, monitor and evaluate your LLM application.\\nIt seamlessly integrates with LangChain and LangGraph, and you can use it to inspect and debug individual steps of your chains and agents as you build.\\n\\nLangSmith documentation is hosted on a separate site.\\nYou can peruse [LangSmith how-to guides here](https://docs.smith.langchain.com/), but we\\'ll highlight a few sections that are particularly\\nrelevant to LangChain below:\\n\\n### Evaluation\\n<span data-heading-keywords=\"evaluation,evaluate\"></span>\\n\\nEvaluating performance is a vital part of building LLM-powered applications.\\nLangSmith helps with every step of the process from creating a dataset to defining metrics to running evaluators.\\n\\nTo learn more, check out the [LangSmith evaluation how-to guides](https://docs.smith.langchain.com/how_to_guides#evaluation).\\n\\n### Tracing\\n<span data-heading-keywords=\"trace,tracing\"></span>\\n\\nTracing gives you observability inside your chains and agents, and is vital in diagnosing issues.\\n\\n- [How to: trace with LangChain](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain)\\n- [How to: add metadata and tags to traces](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#add-metadata-and-tags-to-traces)\\n\\nYou can see general tracing-related how-tos [in this section of the LangSmith docs](https://docs.smith.langchain.com/how_to_guides/tracing).\\n': 0.015873015873015872}\n",
      "rank:3\n",
      "content_score_mapping:{'# Why LangChain?\\n\\nThe goal of `langchain` the Python package and LangChain the company is to make it as easy as possible for developers to build applications that reason.\\nWhile LangChain originally started as a single open source package, it has evolved into a company and a whole ecosystem.\\nThis page will talk about the LangChain ecosystem as a whole.\\nMost of the components within the LangChain ecosystem can be used by themselves - so if you feel particularly drawn to certain components but not others, that is totally fine! Pick and choose whichever components you like best for your own use case!\\n\\n## Features\\n\\nThere are several primary needs that LangChain aims to address:\\n\\n1. **Standardized component interfaces:** The growing number of [models](/docs/integrations/chat/) and [related components](/docs/integrations/vectorstores/) for AI applications has resulted in a wide variety of different APIs that developers need to learn and use.\\nThis diversity can make it challenging for developers to switch between providers or combine components when building applications.\\nLangChain exposes a standard interface for key components, making it easy to switch between providers.\\n\\n2. **Orchestration:** As applications become more complex, combining multiple components and models, there\\'s [a growing need to efficiently connect these elements into control flows](https://lilianweng.github.io/posts/2023-06-23-agent/) that can [accomplish diverse tasks](https://www.sequoiacap.com/article/generative-ais-act-o1/).\\n[Orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)) is crucial for building such applications.\\n\\n3. **Observability and evaluation:** As applications become more complex, it becomes increasingly difficult to understand what is happening within them.\\nFurthermore, the pace of development can become rate-limited by the [paradox of choice](https://en.wikipedia.org/wiki/Paradox_of_choice).\\nFor example, developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\n[Observability](https://en.wikipedia.org/wiki/Observability) and evaluations can help developers monitor their applications and rapidly answer these types of questions with confidence.\\n\\n\\n## Standardized component interfaces\\n\\nLangChain provides common interfaces for components that are central to many AI applications.\\nAs an example, all [chat models](/docs/concepts/chat_models/) implement the [BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) interface.\\nThis provides a standard way to interact with chat models, supporting important but often provider-specific features like [tool calling](/docs/concepts/tool_calling/) and [structured outputs](/docs/concepts/structured_outputs/).\\n\\n\\n### Example: chat models \\n\\nMany [model providers](/docs/concepts/chat_models/) support [tool calling](/docs/concepts/tool_calling/), a critical feature for many applications (e.g., [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/)), that allows a developer to request model responses that match a particular schema.\\nThe APIs for each provider differ. \\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to bind [tools](/docs/concepts/tools) to a model in order to support [tool calling](/docs/concepts/tool_calling/):\\n\\n```python\\n# Tool creation\\ntools = [my_tool]\\n# Tool binding\\nmodel_with_tools = model.bind_tools(tools)\\n```\\n\\nSimilarly, getting models to produce [structured outputs](/docs/concepts/structured_outputs/) is an extremely common use case. \\nProviders support different approaches for this, including [JSON mode or tool calling](https://platform.openai.com/docs/guides/structured-outputs), with different APIs.\\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to produce structured outputs using the `with_structured_output()` method:\\n\\n```python\\n# Define schema\\nschema = ...\\n# Bind schema to model\\nmodel_with_structure = model.with_structured_output(schema)\\n```\\n\\n### Example: retrievers\\n\\nIn the context of [RAG](/docs/concepts/rag/) and LLM application components, LangChain\\'s [retriever](/docs/concepts/retrievers/) interface provides a standard way to connect to many different types of data services or databases (e.g., [vector stores](/docs/concepts/vectorstores) or databases).\\nThe underlying implementation of the retriever depends on the type of data store or database you are connecting to, but all retrievers implement the [runnable interface](/docs/concepts/runnables/), meaning they can be invoked in a common manner.\\n\\n```python\\ndocuments = my_retriever.invoke(\"What is the meaning of life?\")\\n```\\n\\n## Orchestration \\n\\nWhile standardization for individual components is useful, we\\'ve increasingly seen that developers want to *combine* components into more complex applications. \\nThis motivates the need for [orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)).\\nThere are several common characteristics of LLM applications that this orchestration layer should support:\\n\\n* **Complex control flow:** The application requires complex patterns such as cycles (e.g., a loop that reiterates until a condition is met).\\n* **[Persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/):** The application needs to maintain [short-term and / or long-term memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n* **[Human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/):** The application needs human interaction, e.g., pausing, reviewing, editing, approving certain steps.\\n\\nThe recommended way to orchestrate components for complex applications is [LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/).\\nLangGraph is a library that gives developers a high degree of control by expressing the flow of the application as a set of nodes and edges.\\nLangGraph comes with built-in support for [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), [memory](https://langchain-ai.github.io/langgraph/concepts/memory/), and other features.\\nIt\\'s particularly well suited for building [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/) or [multi-agent](https://langchain-ai.github.io/langgraph/concepts/multi_agent/) applications. \\nImportantly, individual LangChain components can be used as LangGraph nodes, but you can also use LangGraph **without** using LangChain components.\\n\\n:::info[Further reading]\\n\\nHave a look at our free course, [Introduction to LangGraph](https://academy.langchain.com/courses/intro-to-langgraph), to learn more about how to use LangGraph to build complex applications.\\n\\n:::\\n\\n## Observability and evaluation\\n\\nThe pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. \\nDevelopers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\nHigh quality tracing and evaluations can help you rapidly answer these types of questions with confidence.\\n[LangSmith](https://docs.smith.langchain.com/) is our platform that supports observability and evaluation for AI applications.\\nSee our conceptual guides on [evaluations](https://docs.smith.langchain.com/concepts/evaluation) and [tracing](https://docs.smith.langchain.com/concepts/tracing) for more details.\\n\\n:::info[Further reading]\\n\\nSee our video playlist on [LangSmith tracing and evaluations](https://youtube.com/playlist?list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S&feature=shared) for more details.\\n\\n:::\\n\\n## Conclusion\\n\\nLangChain offers standard interfaces for components that are central to many AI applications, which offers a few specific advantages:\\n- **Ease of swapping providers:** It allows you to swap out different component providers without having to change the underlying code.\\n- **Advanced features:** It provides common methods for more advanced features, such as [streaming](/docs/concepts/streaming) and [tool calling](/docs/concepts/tool_calling/).\\n\\n[LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/) makes it possible to orchestrate complex applications (e.g., [agents](/docs/concepts/agents/)) and provide features like including [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), or [memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n\\n[LangSmith](https://docs.smith.langchain.com/) makes it possible to iterate with confidence on your applications, by providing LLM-specific observability and framework for testing and evaluating your application.\\n': 0.05, '---\\nsidebar_position: 0\\nsidebar_class_name: hidden\\n---\\n\\n# Introduction\\n\\n**LangChain** is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:\\n- **Development**: Build your applications using LangChain\\'s open-source [components](/docs/concepts) and [third-party integrations](/docs/integrations/providers/).\\nUse [LangGraph](/docs/concepts/architecture/#langgraph) to build stateful agents with first-class streaming and human-in-the-loop support.\\n- **Productionization**: Use [LangSmith](https://docs.smith.langchain.com/) to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\n- **Deployment**: Turn your LangGraph applications into production-ready APIs and Assistants with [LangGraph Platform](https://langchain-ai.github.io/langgraph/cloud/).\\n\\nimport ThemedImage from \\'@theme/ThemedImage\\';\\nimport useBaseUrl from \\'@docusaurus/useBaseUrl\\';\\n\\n<ThemedImage\\n  alt=\"Diagram outlining the hierarchical organization of the LangChain framework, displaying the interconnected parts across multiple layers.\"\\n  sources={{\\n    light: useBaseUrl(\\'/svg/langchain_stack_112024.svg\\'),\\n    dark: useBaseUrl(\\'/svg/langchain_stack_112024_dark.svg\\'),\\n  }}\\n  style={{ width: \"100%\" }}\\n  title=\"LangChain Framework Overview\"\\n/>\\n\\nLangChain implements a standard interface for large language models and related\\ntechnologies, such as embedding models and vector stores, and integrates with\\nhundreds of providers. See the [integrations](/docs/integrations/providers/) page for\\nmore.\\n\\nimport ChatModelTabs from \"@theme/ChatModelTabs\";\\n\\n<ChatModelTabs/>\\n\\n```python\\nmodel.invoke(\"Hello, world!\")\\n```\\n\\n:::note\\n\\nThese docs focus on the Python LangChain library. [Head here](https://js.langchain.com) for docs on the JavaScript LangChain library.\\n\\n:::\\n\\n## Architecture\\n\\nThe LangChain framework consists of multiple open-source libraries. Read more in the\\n[Architecture](/docs/concepts/architecture/) page.\\n\\n- **`langchain-core`**: Base abstractions for chat models and other components.\\n- **Integration packages** (e.g. `langchain-openai`, `langchain-anthropic`, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\\n- **`langchain`**: Chains, agents, and retrieval strategies that make up an application\\'s cognitive architecture.\\n- **`langchain-community`**: Third-party integrations that are community maintained.\\n- **`langgraph`**: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See [LangGraph documentation](https://langchain-ai.github.io/langgraph/).\\n\\n## Guides\\n\\n### [Tutorials](/docs/tutorials)\\n\\nIf you\\'re looking to build something specific or are more of a hands-on learner, check out our [tutorials section](/docs/tutorials).\\nThis is the best place to get started.\\n\\nThese are the best ones to get started with:\\n\\n- [Build a Simple LLM Application](/docs/tutorials/llm_chain)\\n- [Build a Chatbot](/docs/tutorials/chatbot)\\n- [Build an Agent](/docs/tutorials/agents)\\n- [Introduction to LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/)\\n\\nExplore the full list of LangChain tutorials [here](/docs/tutorials), and check out other [LangGraph tutorials here](https://langchain-ai.github.io/langgraph/tutorials/). To learn more about LangGraph, check out our first LangChain Academy course, *Introduction to LangGraph*, available [here](https://academy.langchain.com/courses/intro-to-langgraph).\\n\\n\\n### [How-to guides](/docs/how_to)\\n\\n[Here](/docs/how_to) you’ll find short answers to “How do I….?” types of questions.\\nThese how-to guides don’t cover topics in depth – you’ll find that material in the [Tutorials](/docs/tutorials) and the [API Reference](https://python.langchain.com/api_reference/).\\nHowever, these guides will help you quickly accomplish common tasks using [chat models](/docs/how_to/#chat-models),\\n[vector stores](/docs/how_to/#vector-stores), and other common LangChain components.\\n\\nCheck out [LangGraph-specific how-tos here](https://langchain-ai.github.io/langgraph/how-tos/).\\n\\n### [Conceptual guide](/docs/concepts)\\n\\nIntroductions to all the key parts of LangChain you’ll need to know! [Here](/docs/concepts) you\\'ll find high level explanations of all LangChain concepts.\\n\\nFor a deeper dive into LangGraph concepts, check out [this page](https://langchain-ai.github.io/langgraph/concepts/).\\n\\n### [Integrations](integrations/providers/index.mdx)\\n\\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\\nIf you\\'re looking to get up and running quickly with [chat models](/docs/integrations/chat/), [vector stores](/docs/integrations/vectorstores/),\\nor other LangChain components from a specific provider, check out our growing list of [integrations](/docs/integrations/providers/).\\n\\n\\n### [API reference](https://python.langchain.com/api_reference/)\\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\\n\\n## Ecosystem\\n\\n### [🦜🛠️ LangSmith](https://docs.smith.langchain.com)\\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\\n\\n### [🦜🕸️ LangGraph](https://langchain-ai.github.io/langgraph)\\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by Linkedin, Uber, Klarna, GitLab, and many more.\\n\\n## Additional resources\\n\\n### [Versions](/docs/versions/v0_3/)\\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\\n\\n### [Security](/docs/security)\\nRead up on [security](/docs/security) best practices to make sure you\\'re developing safely with LangChain.\\n\\n### [Contributing](contributing/index.mdx)\\nCheck out the developer\\'s guide for guidelines on contributing and help getting your dev environment set up.\\n': 0.048915917503966164, '# LangChain Expression Language (LCEL)\\n\\n:::info Prerequisites\\n* [Runnable Interface](/docs/concepts/runnables)\\n:::\\n\\nThe **L**ang**C**hain **E**xpression **L**anguage (LCEL) takes a [declarative](https://en.wikipedia.org/wiki/Declarative_programming) approach to building new [Runnables](/docs/concepts/runnables) from existing Runnables.\\n\\nThis means that you describe what *should* happen, rather than *how* it should happen, allowing LangChain to optimize the run-time execution of the chains.\\n\\nWe often refer to a `Runnable` created using LCEL as a \"chain\". It\\'s important to remember that a \"chain\" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).\\n\\n:::note\\n* The [LCEL cheatsheet](/docs/how_to/lcel_cheatsheet/) shows common patterns that involve the Runnable interface and LCEL expressions.\\n* Please see the following list of [how-to guides](/docs/how_to/#langchain-expression-language-lcel) that cover common tasks with LCEL.\\n* A list of built-in `Runnables` can be found in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html). Many of these Runnables are useful when composing custom \"chains\" in LangChain using LCEL.\\n:::\\n\\n## Benefits of LCEL\\n\\nLangChain optimizes the run-time execution of chains built with LCEL in a number of ways:\\n\\n- **Optimized parallel execution**: Run Runnables in parallel using [RunnableParallel](#runnableparallel) or run multiple inputs through a given chain in parallel using the [Runnable Batch API](/docs/concepts/runnables/#optimized-parallel-execution-batch). Parallel execution can significantly reduce the latency as processing can be done in parallel instead of sequentially.\\n- **Guaranteed Async support**: Any chain built with LCEL can be run asynchronously using the [Runnable Async API](/docs/concepts/runnables/#asynchronous-support). This can be useful when running chains in a server environment where you want to handle large number of requests concurrently.\\n- **Simplify streaming**: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(time elapsed until the first chunk of output from a [chat model](/docs/concepts/chat_models) or [llm](/docs/concepts/text_llms) comes out).\\n\\nOther benefits include:\\n\\n- [**Seamless LangSmith tracing**](https://docs.smith.langchain.com)\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, **all** steps are automatically logged to [LangSmith](https://docs.smith.langchain.com/) for maximum observability and debuggability.\\n- **Standard API**: Because all chains are built using the Runnable interface, they can be used in the same way as any other Runnable.\\n- [**Deployable with LangServe**](/docs/concepts/architecture#langserve): Chains built with LCEL can be deployed using for production use.\\n\\n## Should I use LCEL?\\n\\nLCEL is an [orchestration solution](https://en.wikipedia.org/wiki/Orchestration_(computing)) -- it allows LangChain to handle run-time execution of chains in an optimized way.\\n\\nWhile we have seen users run chains with hundreds of steps in production, we generally recommend using LCEL for simpler orchestration tasks. When the application requires complex state management, branching, cycles or multiple agents, we recommend that users take advantage of [LangGraph](/docs/concepts/architecture#langgraph).\\n\\nIn LangGraph, users define graphs that specify the application\\'s flow. This allows users to keep using LCEL within individual nodes when LCEL is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.\\n\\nHere are some guidelines:\\n\\n* If you are making a single LLM call, you don\\'t need LCEL; instead call the underlying [chat model](/docs/concepts/chat_models) directly.\\n* If you have a simple chain (e.g., prompt + llm + parser, simple retrieval set up etc.), LCEL is a reasonable fit, if you\\'re taking advantage of the LCEL benefits.\\n* If you\\'re building a complex chain (e.g., with branching, cycles, multiple agents, etc.) use [LangGraph](/docs/concepts/architecture#langgraph) instead. Remember that you can always use LCEL within individual nodes in LangGraph.\\n\\n## Composition Primitives\\n\\n`LCEL` chains are built by composing existing `Runnables` together. The two main composition primitives are [RunnableSequence](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableSequence.html#langchain_core.runnables.base.RunnableSequence) and [RunnableParallel](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableParallel.html#langchain_core.runnables.base.RunnableParallel).\\n\\nMany other composition primitives (e.g., [RunnableAssign](\\nhttps://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnableAssign.html#langchain_core.runnables.passthrough.RunnableAssign\\n)) can be thought of as variations of these two primitives.\\n\\n:::note\\nYou can find a list of all composition primitives in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html).\\n:::\\n\\n### RunnableSequence\\n\\n`RunnableSequence` is a composition primitive that allows you \"chain\" multiple runnables sequentially, with the output of one runnable serving as the input to the next.\\n\\n```python\\nfrom langchain_core.runnables import RunnableSequence\\nchain = RunnableSequence([runnable1, runnable2])\\n```\\n\\nInvoking the `chain` with some input:\\n\\n```python\\nfinal_output = chain.invoke(some_input)\\n```\\n\\ncorresponds to the following:\\n\\n```python\\noutput1 = runnable1.invoke(some_input)\\nfinal_output = runnable2.invoke(output1)\\n```\\n\\n:::note\\n`runnable1` and `runnable2` are placeholders for any `Runnable` that you want to chain together.\\n:::\\n\\n### RunnableParallel\\n\\n`RunnableParallel` is a composition primitive that allows you to run multiple runnables concurrently, with the same input provided to each.\\n\\n```python\\nfrom langchain_core.runnables import RunnableParallel\\nchain = RunnableParallel({\\n    \"key1\": runnable1,\\n    \"key2\": runnable2,\\n})\\n```\\n\\nInvoking the `chain` with some input:\\n\\n```python\\nfinal_output = chain.invoke(some_input)\\n```\\n\\nWill yield a `final_output` dictionary with the same keys as the input dictionary, but with the values replaced by the output of the corresponding runnable.\\n\\n```python\\n{\\n    \"key1\": runnable1.invoke(some_input),\\n    \"key2\": runnable2.invoke(some_input),\\n}\\n```\\n\\nRecall, that the runnables are executed in parallel, so while the result is the same as\\ndictionary comprehension shown above, the execution time is much faster.\\n\\n:::note\\n`RunnableParallel`supports both synchronous and asynchronous execution (as all `Runnables` do).\\n\\n* For synchronous execution, `RunnableParallel` uses a [ThreadPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor) to run the runnables concurrently.\\n* For asynchronous execution, `RunnableParallel` uses [asyncio.gather](https://docs.python.org/3/library/asyncio.html#asyncio.gather) to run the runnables concurrently.\\n:::\\n\\n## Composition Syntax\\n\\nThe usage of `RunnableSequence` and `RunnableParallel` is so common that we created a shorthand syntax for using them. This helps\\nto make the code more readable and concise.\\n\\n### The `|` operator\\n\\nWe have [overloaded](https://docs.python.org/3/reference/datamodel.html#special-method-names) the `|` operator to create a `RunnableSequence` from two `Runnables`.\\n\\n```python\\nchain = runnable1 | runnable2\\n```\\n\\nis Equivalent to:\\n\\n```python\\nchain = RunnableSequence([runnable1, runnable2])\\n```\\n\\n### The `.pipe` method\\n\\nIf you have moral qualms with operator overloading, you can use the `.pipe` method instead. This is equivalent to the `|` operator.\\n\\n```python\\nchain = runnable1.pipe(runnable2)\\n```\\n\\n### Coercion\\n\\nLCEL applies automatic type coercion to make it easier to compose chains.\\n\\nIf you do not understand the type coercion, you can always use the `RunnableSequence` and `RunnableParallel` classes directly.\\n\\nThis will make the code more verbose, but it will also make it more explicit.\\n\\n#### Dictionary to RunnableParallel\\n\\nInside an LCEL expression, a dictionary is automatically converted to a `RunnableParallel`.\\n\\nFor example, the following code:\\n\\n```python\\nmapping = {\\n    \"key1\": runnable1,\\n    \"key2\": runnable2,\\n}\\n\\nchain = mapping | runnable3\\n```\\n\\nIt gets automatically converted to the following:\\n\\n```python\\nchain = RunnableSequence([RunnableParallel(mapping), runnable3])\\n```\\n\\n:::caution\\nYou have to be careful because the `mapping` dictionary is not a `RunnableParallel` object, it is just a dictionary. This means that the following code will raise an `AttributeError`:\\n\\n```python\\nmapping.invoke(some_input)\\n```\\n:::\\n\\n#### Function to RunnableLambda\\n\\nInside an LCEL expression, a function is automatically converted to a `RunnableLambda`.\\n\\n```\\ndef some_func(x):\\n    return x\\n\\nchain = some_func | runnable1\\n```\\n\\nIt gets automatically converted to the following:\\n\\n```python\\nchain = RunnableSequence([RunnableLambda(some_func), runnable1])\\n```\\n\\n:::caution\\nYou have to be careful because the lambda function is not a `RunnableLambda` object, it is just a function. This means that the following code will raise an `AttributeError`:\\n\\n```python\\nlambda x: x + 1.invoke(some_input)\\n```\\n:::\\n\\n## Legacy chains\\n\\nLCEL aims to provide consistency around behavior and customization over legacy subclassed chains such as `LLMChain` and\\n`ConversationalRetrievalChain`. Many of these legacy chains hide important details like prompts, and as a wider variety\\nof viable models emerge, customization has become more and more important.\\n\\nIf you are currently using one of these legacy chains, please see [this guide for guidance on how to migrate](/docs/versions/migrating_chains).\\n\\nFor guides on how to do specific tasks with LCEL, check out [the relevant how-to guides](/docs/how_to/#langchain-expression-language-lcel).\\n': 0.048651507139079855, '# Conceptual guide\\n\\nThis guide provides explanations of the key concepts behind the LangChain framework and AI applications more broadly.\\n\\nWe recommend that you go through at least one of the [Tutorials](/docs/tutorials) before diving into the conceptual guide. This will provide practical context that will make it easier to understand the concepts discussed here.\\n\\nThe conceptual guide does not cover step-by-step instructions or specific implementation examples — those are found in the [How-to guides](/docs/how_to/) and [Tutorials](/docs/tutorials). For detailed reference material, please see the [API reference](https://python.langchain.com/api_reference/).\\n\\n## High level\\n\\n- **[Why LangChain?](/docs/concepts/why_langchain)**: Overview of the value that LangChain provides.\\n- **[Architecture](/docs/concepts/architecture)**: How packages are organized in the LangChain ecosystem.\\n\\n## Concepts\\n\\n- **[Chat models](/docs/concepts/chat_models)**: LLMs exposed via a chat API that process sequences of messages as input and output a message.\\n- **[Messages](/docs/concepts/messages)**: The unit of communication in chat models, used to represent model input and output.\\n- **[Chat history](/docs/concepts/chat_history)**: A conversation represented as a sequence of messages, alternating between user messages and model responses.\\n- **[Tools](/docs/concepts/tools)**: A function with an associated schema defining the function\\'s name, description, and the arguments it accepts.\\n- **[Tool calling](/docs/concepts/tool_calling)**: A type of chat model API that accepts tool schemas, along with messages, as input and returns invocations of those tools as part of the output message.\\n- **[Structured output](/docs/concepts/structured_outputs)**: A technique to make a chat model respond in a structured format, such as JSON that matches a given schema.\\n- **[Memory](https://langchain-ai.github.io/langgraph/concepts/memory/)**: Information about a conversation that is persisted so that it can be used in future conversations.\\n- **[Multimodality](/docs/concepts/multimodality)**: The ability to work with data that comes in different forms, such as text, audio, images, and video.\\n- **[Runnable interface](/docs/concepts/runnables)**: The base abstraction that many LangChain components and the LangChain Expression Language are built on.\\n- **[Streaming](/docs/concepts/streaming)**: LangChain streaming APIs for surfacing results as they are generated.\\n- **[LangChain Expression Language (LCEL)](/docs/concepts/lcel)**: A syntax for orchestrating LangChain components. Most useful for simpler applications.\\n- **[Document loaders](/docs/concepts/document_loaders)**: Load a source as a list of documents.\\n- **[Retrieval](/docs/concepts/retrieval)**: Information retrieval systems can retrieve structured or unstructured data from a datasource in response to a query.\\n- **[Text splitters](/docs/concepts/text_splitters)**: Split long text into smaller chunks that can be individually indexed to enable granular retrieval.\\n- **[Embedding models](/docs/concepts/embedding_models)**: Models that represent data such as text or images in a vector space.\\n- **[Vector stores](/docs/concepts/vectorstores)**: Storage of and efficient search over vectors and associated metadata.\\n- **[Retriever](/docs/concepts/retrievers)**: A component that returns relevant documents from a knowledge base in response to a query.\\n- **[Retrieval Augmented Generation (RAG)](/docs/concepts/rag)**: A technique that enhances language models by combining them with external knowledge bases.\\n- **[Agents](/docs/concepts/agents)**: Use a [language model](/docs/concepts/chat_models) to choose a sequence of actions to take. Agents can interact with external resources via [tool](/docs/concepts/tools).\\n- **[Prompt templates](/docs/concepts/prompt_templates)**: Component for factoring out the static parts of a model \"prompt\" (usually a sequence of messages). Useful for serializing, versioning, and reusing these static parts.\\n- **[Output parsers](/docs/concepts/output_parsers)**: Responsible for taking the output of a model and transforming it into a more suitable format for downstream tasks. Output parsers were primarily useful prior to the general availability of [tool calling](/docs/concepts/tool_calling) and [structured outputs](/docs/concepts/structured_outputs).\\n- **[Few-shot prompting](/docs/concepts/few_shot_prompting)**: A technique for improving model performance by providing a few examples of the task to perform in the prompt.\\n- **[Example selectors](/docs/concepts/example_selectors)**: Used to select the most relevant examples from a dataset based on a given input. Example selectors are used in few-shot prompting to select examples for a prompt.\\n- **[Async programming](/docs/concepts/async)**: The basics that one should know to use LangChain in an asynchronous context.\\n- **[Callbacks](/docs/concepts/callbacks)**: Callbacks enable the execution of custom auxiliary code in built-in components. Callbacks are used to stream outputs from LLMs in LangChain, trace the intermediate steps of an application, and more.\\n- **[Tracing](/docs/concepts/tracing)**: The process of recording the steps that an application takes to go from input to output. Tracing is essential for debugging and diagnosing issues in complex applications.\\n- **[Evaluation](/docs/concepts/evaluation)**: The process of assessing the performance and effectiveness of AI applications. This involves testing the model\\'s responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose. This process is vital for building reliable applications.\\n- **[Testing](/docs/concepts/testing)**: The process of verifying that a component of an integration or application works as expected. Testing is essential for ensuring that the application behaves correctly and that changes to the codebase do not introduce new bugs.\\n\\n## Glossary\\n\\n- **[AIMessageChunk](/docs/concepts/messages#aimessagechunk)**: A partial response from an AI message. Used when streaming responses from a chat model.\\n- **[AIMessage](/docs/concepts/messages#aimessage)**: Represents a complete response from an AI model.\\n- **[astream_events](/docs/concepts/chat_models#key-methods)**: Stream granular information from [LCEL](/docs/concepts/lcel) chains.\\n- **[BaseTool](/docs/concepts/tools/#tool-interface)**: The base class for all tools in LangChain.\\n- **[batch](/docs/concepts/runnables)**: Use to execute a runnable with batch inputs.\\n- **[bind_tools](/docs/concepts/tool_calling/#tool-binding)**: Allows models to interact with tools.\\n- **[Caching](/docs/concepts/chat_models#caching)**: Storing results to avoid redundant calls to a chat model.\\n- **[Chat models](/docs/concepts/multimodality/#multimodality-in-chat-models)**: Chat models that handle multiple data modalities.\\n- **[Configurable runnables](/docs/concepts/runnables/#configurable-runnables)**: Creating configurable Runnables.\\n- **[Context window](/docs/concepts/chat_models#context-window)**: The maximum size of input a chat model can process.\\n- **[Conversation patterns](/docs/concepts/chat_history#conversation-patterns)**: Common patterns in chat interactions.\\n- **[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)**: LangChain\\'s representation of a document.\\n- **[Embedding models](/docs/concepts/multimodality/#multimodality-in-embedding-models)**: Models that generate vector embeddings for various data types.\\n- **[HumanMessage](/docs/concepts/messages#humanmessage)**: Represents a message from a human user.\\n- **[InjectedState](/docs/concepts/tools#injectedstate)**: A state injected into a tool function.\\n- **[InjectedStore](/docs/concepts/tools#injectedstore)**: A store that can be injected into a tool for data persistence.\\n- **[InjectedToolArg](/docs/concepts/tools#injectedtoolarg)**: Mechanism to inject arguments into tool functions.\\n- **[input and output types](/docs/concepts/runnables#input-and-output-types)**: Types used for input and output in Runnables.\\n- **[Integration packages](/docs/concepts/architecture/#integration-packages)**: Third-party packages that integrate with LangChain.\\n- **[Integration tests](/docs/concepts/testing#integration-tests)**: Tests that verify the correctness of the interaction between components, usually run with access to the underlying API that powers an integration.\\n- **[invoke](/docs/concepts/runnables)**: A standard method to invoke a Runnable.\\n- **[JSON mode](/docs/concepts/structured_outputs#json-mode)**: Returning responses in JSON format.\\n- **[langchain-community](/docs/concepts/architecture#langchain-community)**: Community-driven components for LangChain.\\n- **[langchain-core](/docs/concepts/architecture#langchain-core)**: Core langchain package. Includes base interfaces and in-memory implementations.\\n- **[langchain](/docs/concepts/architecture#langchain)**: A package for higher level components (e.g., some pre-built chains).\\n- **[langgraph](/docs/concepts/architecture#langgraph)**: Powerful orchestration layer for LangChain. Use to build complex pipelines and workflows.\\n- **[langserve](/docs/concepts/architecture#langserve)**: Used to deploy LangChain Runnables as REST endpoints. Uses FastAPI. Works primarily for LangChain Runnables, does not currently integrate with LangGraph.\\n- **[LLMs (legacy)](/docs/concepts/text_llms)**: Older language models that take a string as input and return a string as output.\\n- **[Managing chat history](/docs/concepts/chat_history#managing-chat-history)**: Techniques to maintain and manage the chat history.\\n- **[OpenAI format](/docs/concepts/messages#openai-format)**: OpenAI\\'s message format for chat models.\\n- **[Propagation of RunnableConfig](/docs/concepts/runnables/#propagation-of-runnableconfig)**: Propagating configuration through Runnables. Read if working with python 3.9, 3.10 and async.\\n- **[rate-limiting](/docs/concepts/chat_models#rate-limiting)**: Client side rate limiting for chat models.\\n- **[RemoveMessage](/docs/concepts/messages/#removemessage)**: An abstraction used to remove a message from chat history, used primarily in LangGraph.\\n- **[role](/docs/concepts/messages#role)**: Represents the role (e.g., user, assistant) of a chat message.\\n- **[RunnableConfig](/docs/concepts/runnables/#runnableconfig)**: Use to pass run time information to Runnables (e.g., `run_name`, `run_id`, `tags`, `metadata`, `max_concurrency`, `recursion_limit`, `configurable`).\\n- **[Standard parameters for chat models](/docs/concepts/chat_models#standard-parameters)**: Parameters such as API key, `temperature`, and `max_tokens`.\\n- **[Standard tests](/docs/concepts/testing#standard-tests)**: A defined set of unit and integration tests that all integrations must pass.\\n- **[stream](/docs/concepts/streaming)**: Use to stream output from a Runnable or a graph.\\n- **[Tokenization](/docs/concepts/tokens)**: The process of converting data into tokens and vice versa.\\n- **[Tokens](/docs/concepts/tokens)**: The basic unit that a language model reads, processes, and generates under the hood.\\n- **[Tool artifacts](/docs/concepts/tools#tool-artifacts)**: Add artifacts to the output of a tool that will not be sent to the model, but will be available for downstream processing.\\n- **[Tool binding](/docs/concepts/tool_calling#tool-binding)**: Binding tools to models.\\n- **[@tool](/docs/concepts/tools/#create-tools-using-the-tool-decorator)**: Decorator for creating tools in LangChain.\\n- **[Toolkits](/docs/concepts/tools#toolkits)**: A collection of tools that can be used together.\\n- **[ToolMessage](/docs/concepts/messages#toolmessage)**: Represents a message that contains the results of a tool execution.\\n- **[Unit tests](/docs/concepts/testing#unit-tests)**: Tests that verify the correctness of individual components, run in isolation without access to the Internet.\\n- **[Vector stores](/docs/concepts/vectorstores)**: Datastores specialized for storing and efficiently searching vector embeddings.\\n- **[with_structured_output](/docs/concepts/structured_outputs/#structured-output-method)**: A helper method for chat models that natively support [tool calling](/docs/concepts/tool_calling) to get structured output matching a given schema specified via Pydantic, JSON schema or a function.\\n- **[with_types](/docs/concepts/runnables#with_types)**: Method to overwrite the input and output types of a runnable. Useful when working with complex LCEL chains and deploying with LangServe.\\n': 0.015873015873015872, '---\\nsidebar_position: 0\\nsidebar_class_name: hidden\\n---\\n\\n# How-to guides\\n\\nHere you’ll find answers to “How do I….?” types of questions.\\nThese guides are *goal-oriented* and *concrete*; they\\'re meant to help you complete a specific task.\\nFor conceptual explanations see the [Conceptual guide](/docs/concepts/).\\nFor end-to-end walkthroughs see [Tutorials](/docs/tutorials).\\nFor comprehensive descriptions of every class and function see the [API Reference](https://python.langchain.com/api_reference/).\\n\\n## Installation\\n\\n- [How to: install LangChain packages](/docs/how_to/installation/)\\n- [How to: use LangChain with different Pydantic versions](/docs/how_to/pydantic_compatibility)\\n\\n## Key features\\n\\nThis highlights functionality that is core to using LangChain.\\n\\n- [How to: return structured data from a model](/docs/how_to/structured_output/)\\n- [How to: use a model to call tools](/docs/how_to/tool_calling)\\n- [How to: stream runnables](/docs/how_to/streaming)\\n- [How to: debug your LLM apps](/docs/how_to/debugging/)\\n\\n## Components\\n\\nThese are the core building blocks you can use when building applications.\\n\\n### Chat models\\n\\n[Chat Models](/docs/concepts/chat_models) are newer forms of language models that take messages in and output a message.\\nSee [supported integrations](/docs/integrations/chat/) for details on getting started with chat models from a specific provider.\\n\\n- [How to: do function/tool calling](/docs/how_to/tool_calling)\\n- [How to: get models to return structured output](/docs/how_to/structured_output)\\n- [How to: cache model responses](/docs/how_to/chat_model_caching)\\n- [How to: get log probabilities](/docs/how_to/logprobs)\\n- [How to: create a custom chat model class](/docs/how_to/custom_chat_model)\\n- [How to: stream a response back](/docs/how_to/chat_streaming)\\n- [How to: track token usage](/docs/how_to/chat_token_usage_tracking)\\n- [How to: track response metadata across providers](/docs/how_to/response_metadata)\\n- [How to: use chat model to call tools](/docs/how_to/tool_calling)\\n- [How to: stream tool calls](/docs/how_to/tool_streaming)\\n- [How to: handle rate limits](/docs/how_to/chat_model_rate_limiting)\\n- [How to: few shot prompt tool behavior](/docs/how_to/tools_few_shot)\\n- [How to: bind model-specific formatted tools](/docs/how_to/tools_model_specific)\\n- [How to: force a specific tool call](/docs/how_to/tool_choice)\\n- [How to: work with local models](/docs/how_to/local_llms)\\n- [How to: init any model in one line](/docs/how_to/chat_models_universal_init/)\\n- [How to: pass multimodal data directly to models](/docs/how_to/multimodal_inputs/)\\n\\n### Messages\\n\\n[Messages](/docs/concepts/messages) are the input and output of chat models. They have some `content` and a `role`, which describes the source of the message.\\n\\n- [How to: trim messages](/docs/how_to/trim_messages/)\\n- [How to: filter messages](/docs/how_to/filter_messages/)\\n- [How to: merge consecutive messages of the same type](/docs/how_to/merge_message_runs/)\\n\\n### Prompt templates\\n\\n[Prompt Templates](/docs/concepts/prompt_templates) are responsible for formatting user input into a format that can be passed to a language model.\\n\\n- [How to: use few shot examples](/docs/how_to/few_shot_examples)\\n- [How to: use few shot examples in chat models](/docs/how_to/few_shot_examples_chat/)\\n- [How to: partially format prompt templates](/docs/how_to/prompts_partial)\\n- [How to: compose prompts together](/docs/how_to/prompts_composition)\\n- [How to: use multimodal prompts](/docs/how_to/multimodal_prompts/)\\n\\n### Example selectors\\n\\n[Example Selectors](/docs/concepts/example_selectors) are responsible for selecting the correct few shot examples to pass to the prompt.\\n\\n- [How to: use example selectors](/docs/how_to/example_selectors)\\n- [How to: select examples by length](/docs/how_to/example_selectors_length_based)\\n- [How to: select examples by semantic similarity](/docs/how_to/example_selectors_similarity)\\n- [How to: select examples by semantic ngram overlap](/docs/how_to/example_selectors_ngram)\\n- [How to: select examples by maximal marginal relevance](/docs/how_to/example_selectors_mmr)\\n- [How to: select examples from LangSmith few-shot datasets](/docs/how_to/example_selectors_langsmith/)\\n\\n### LLMs\\n\\nWhat LangChain calls [LLMs](/docs/concepts/text_llms) are older forms of language models that take a string in and output a string.\\n\\n- [How to: cache model responses](/docs/how_to/llm_caching)\\n- [How to: create a custom LLM class](/docs/how_to/custom_llm)\\n- [How to: stream a response back](/docs/how_to/streaming_llm)\\n- [How to: track token usage](/docs/how_to/llm_token_usage_tracking)\\n- [How to: work with local models](/docs/how_to/local_llms)\\n\\n### Output parsers\\n\\n[Output Parsers](/docs/concepts/output_parsers) are responsible for taking the output of an LLM and parsing into more structured format.\\n\\n- [How to: parse text from message objects](/docs/how_to/output_parser_string)\\n- [How to: use output parsers to parse an LLM response into structured format](/docs/how_to/output_parser_structured)\\n- [How to: parse JSON output](/docs/how_to/output_parser_json)\\n- [How to: parse XML output](/docs/how_to/output_parser_xml)\\n- [How to: parse YAML output](/docs/how_to/output_parser_yaml)\\n- [How to: retry when output parsing errors occur](/docs/how_to/output_parser_retry)\\n- [How to: try to fix errors in output parsing](/docs/how_to/output_parser_fixing)\\n- [How to: write a custom output parser class](/docs/how_to/output_parser_custom)\\n\\n### Document loaders\\n\\n[Document Loaders](/docs/concepts/document_loaders) are responsible for loading documents from a variety of sources.\\n\\n- [How to: load PDF files](/docs/how_to/document_loader_pdf)\\n- [How to: load web pages](/docs/how_to/document_loader_web)\\n- [How to: load CSV data](/docs/how_to/document_loader_csv)\\n- [How to: load data from a directory](/docs/how_to/document_loader_directory)\\n- [How to: load HTML data](/docs/how_to/document_loader_html)\\n- [How to: load JSON data](/docs/how_to/document_loader_json)\\n- [How to: load Markdown data](/docs/how_to/document_loader_markdown)\\n- [How to: load Microsoft Office data](/docs/how_to/document_loader_office_file)\\n- [How to: write a custom document loader](/docs/how_to/document_loader_custom)\\n\\n### Text splitters\\n\\n[Text Splitters](/docs/concepts/text_splitters) take a document and split into chunks that can be used for retrieval.\\n\\n- [How to: recursively split text](/docs/how_to/recursive_text_splitter)\\n- [How to: split HTML](/docs/how_to/split_html)\\n- [How to: split by character](/docs/how_to/character_text_splitter)\\n- [How to: split code](/docs/how_to/code_splitter)\\n- [How to: split Markdown by headers](/docs/how_to/markdown_header_metadata_splitter)\\n- [How to: recursively split JSON](/docs/how_to/recursive_json_splitter)\\n- [How to: split text into semantic chunks](/docs/how_to/semantic-chunker)\\n- [How to: split by tokens](/docs/how_to/split_by_token)\\n\\n### Embedding models\\n\\n[Embedding Models](/docs/concepts/embedding_models) take a piece of text and create a numerical representation of it.\\nSee [supported integrations](/docs/integrations/text_embedding/) for details on getting started with embedding models from a specific provider.\\n\\n- [How to: embed text data](/docs/how_to/embed_text)\\n- [How to: cache embedding results](/docs/how_to/caching_embeddings)\\n- [How to: create a custom embeddings class](/docs/how_to/custom_embeddings)\\n\\n### Vector stores\\n\\n[Vector stores](/docs/concepts/vectorstores) are databases that can efficiently store and retrieve embeddings.\\nSee [supported integrations](/docs/integrations/vectorstores/) for details on getting started with vector stores from a specific provider.\\n\\n- [How to: use a vector store to retrieve data](/docs/how_to/vectorstores)\\n\\n### Retrievers\\n\\n[Retrievers](/docs/concepts/retrievers) are responsible for taking a query and returning relevant documents.\\n\\n- [How to: use a vector store to retrieve data](/docs/how_to/vectorstore_retriever)\\n- [How to: generate multiple queries to retrieve data for](/docs/how_to/MultiQueryRetriever)\\n- [How to: use contextual compression to compress the data retrieved](/docs/how_to/contextual_compression)\\n- [How to: write a custom retriever class](/docs/how_to/custom_retriever)\\n- [How to: add similarity scores to retriever results](/docs/how_to/add_scores_retriever)\\n- [How to: combine the results from multiple retrievers](/docs/how_to/ensemble_retriever)\\n- [How to: reorder retrieved results to mitigate the \"lost in the middle\" effect](/docs/how_to/long_context_reorder)\\n- [How to: generate multiple embeddings per document](/docs/how_to/multi_vector)\\n- [How to: retrieve the whole document for a chunk](/docs/how_to/parent_document_retriever)\\n- [How to: generate metadata filters](/docs/how_to/self_query)\\n- [How to: create a time-weighted retriever](/docs/how_to/time_weighted_vectorstore)\\n- [How to: use hybrid vector and keyword retrieval](/docs/how_to/hybrid)\\n\\n### Indexing\\n\\nIndexing is the process of keeping your vectorstore in-sync with the underlying data source.\\n\\n- [How to: reindex data to keep your vectorstore in-sync with the underlying data source](/docs/how_to/indexing)\\n\\n### Tools\\n\\nLangChain [Tools](/docs/concepts/tools) contain a description of the tool (to pass to the language model) as well as the implementation of the function to call. Refer [here](/docs/integrations/tools/) for a list of pre-built tools. \\n\\n- [How to: create tools](/docs/how_to/custom_tools)\\n- [How to: use built-in tools and toolkits](/docs/how_to/tools_builtin)\\n- [How to: use chat models to call tools](/docs/how_to/tool_calling)\\n- [How to: pass tool outputs to chat models](/docs/how_to/tool_results_pass_to_model)\\n- [How to: pass run time values to tools](/docs/how_to/tool_runtime)\\n- [How to: add a human-in-the-loop for tools](/docs/how_to/tools_human)\\n- [How to: handle tool errors](/docs/how_to/tools_error)\\n- [How to: force models to call a tool](/docs/how_to/tool_choice)\\n- [How to: disable parallel tool calling](/docs/how_to/tool_calling_parallel)\\n- [How to: access the `RunnableConfig` from a tool](/docs/how_to/tool_configure)\\n- [How to: stream events from a tool](/docs/how_to/tool_stream_events)\\n- [How to: return artifacts from a tool](/docs/how_to/tool_artifacts/)\\n- [How to: convert Runnables to tools](/docs/how_to/convert_runnable_to_tool)\\n- [How to: add ad-hoc tool calling capability to models](/docs/how_to/tools_prompting)\\n- [How to: pass in runtime secrets](/docs/how_to/runnable_runtime_secrets)\\n\\n### Multimodal\\n\\n- [How to: pass multimodal data directly to models](/docs/how_to/multimodal_inputs/)\\n- [How to: use multimodal prompts](/docs/how_to/multimodal_prompts/)\\n\\n\\n### Agents\\n\\n:::note\\n\\nFor in depth how-to guides for agents, please check out [LangGraph](https://langchain-ai.github.io/langgraph/) documentation.\\n\\n:::\\n\\n- [How to: use legacy LangChain Agents (AgentExecutor)](/docs/how_to/agent_executor)\\n- [How to: migrate from legacy LangChain agents to LangGraph](/docs/how_to/migrate_agent)\\n\\n### Callbacks\\n\\n[Callbacks](/docs/concepts/callbacks) allow you to hook into the various stages of your LLM application\\'s execution.\\n\\n- [How to: pass in callbacks at runtime](/docs/how_to/callbacks_runtime)\\n- [How to: attach callbacks to a module](/docs/how_to/callbacks_attach)\\n- [How to: pass callbacks into a module constructor](/docs/how_to/callbacks_constructor)\\n- [How to: create custom callback handlers](/docs/how_to/custom_callbacks)\\n- [How to: use callbacks in async environments](/docs/how_to/callbacks_async)\\n- [How to: dispatch custom callback events](/docs/how_to/callbacks_custom_events)\\n\\n### Custom\\n\\nAll of LangChain components can easily be extended to support your own versions.\\n\\n- [How to: create a custom chat model class](/docs/how_to/custom_chat_model)\\n- [How to: create a custom LLM class](/docs/how_to/custom_llm)\\n- [How to: create a custom embeddings class](/docs/how_to/custom_embeddings)\\n- [How to: write a custom retriever class](/docs/how_to/custom_retriever)\\n- [How to: write a custom document loader](/docs/how_to/document_loader_custom)\\n- [How to: write a custom output parser class](/docs/how_to/output_parser_custom)\\n- [How to: create custom callback handlers](/docs/how_to/custom_callbacks)\\n- [How to: define a custom tool](/docs/how_to/custom_tools)\\n- [How to: dispatch custom callback events](/docs/how_to/callbacks_custom_events)\\n\\n### Serialization\\n- [How to: save and load LangChain objects](/docs/how_to/serialization)\\n\\n## Use cases\\n\\nThese guides cover use-case specific details.\\n\\n### Q&A with RAG\\n\\nRetrieval Augmented Generation (RAG) is a way to connect LLMs to external sources of data.\\nFor a high-level tutorial on RAG, check out [this guide](/docs/tutorials/rag/).\\n\\n- [How to: add chat history](/docs/how_to/qa_chat_history_how_to/)\\n- [How to: stream](/docs/how_to/qa_streaming/)\\n- [How to: return sources](/docs/how_to/qa_sources/)\\n- [How to: return citations](/docs/how_to/qa_citations/)\\n- [How to: do per-user retrieval](/docs/how_to/qa_per_user/)\\n\\n\\n### Extraction\\n\\nExtraction is when you use LLMs to extract structured information from unstructured text.\\nFor a high level tutorial on extraction, check out [this guide](/docs/tutorials/extraction/).\\n\\n- [How to: use reference examples](/docs/how_to/extraction_examples/)\\n- [How to: handle long text](/docs/how_to/extraction_long_text/)\\n- [How to: do extraction without using function calling](/docs/how_to/extraction_parse)\\n\\n### Chatbots\\n\\nChatbots involve using an LLM to have a conversation.\\nFor a high-level tutorial on building chatbots, check out [this guide](/docs/tutorials/chatbot/).\\n\\n- [How to: manage memory](/docs/how_to/chatbots_memory)\\n- [How to: do retrieval](/docs/how_to/chatbots_retrieval)\\n- [How to: use tools](/docs/how_to/chatbots_tools)\\n- [How to: manage large chat history](/docs/how_to/trim_messages/)\\n\\n### Query analysis\\n\\nQuery Analysis is the task of using an LLM to generate a query to send to a retriever.\\nFor a high-level tutorial on query analysis, check out [this guide](/docs/tutorials/rag/#query-analysis).\\n\\n- [How to: add examples to the prompt](/docs/how_to/query_few_shot)\\n- [How to: handle cases where no queries are generated](/docs/how_to/query_no_queries)\\n- [How to: handle multiple queries](/docs/how_to/query_multiple_queries)\\n- [How to: handle multiple retrievers](/docs/how_to/query_multiple_retrievers)\\n- [How to: construct filters](/docs/how_to/query_constructing_filters)\\n- [How to: deal with high cardinality categorical variables](/docs/how_to/query_high_cardinality)\\n\\n### Q&A over SQL + CSV\\n\\nYou can use LLMs to do question answering over tabular data.\\nFor a high-level tutorial, check out [this guide](/docs/tutorials/sql_qa/).\\n\\n- [How to: use prompting to improve results](/docs/how_to/sql_prompting)\\n- [How to: do query validation](/docs/how_to/sql_query_checking)\\n- [How to: deal with large databases](/docs/how_to/sql_large_db)\\n- [How to: deal with CSV files](/docs/how_to/sql_csv)\\n\\n### Q&A over graph databases\\n\\nYou can use an LLM to do question answering over graph databases.\\nFor a high-level tutorial, check out [this guide](/docs/tutorials/graph/).\\n\\n- [How to: add a semantic layer over the database](/docs/how_to/graph_semantic)\\n- [How to: construct knowledge graphs](/docs/how_to/graph_constructing)\\n\\n### Summarization\\n\\nLLMs can summarize and otherwise distill desired information from text, including\\nlarge volumes of text. For a high-level tutorial, check out [this guide](/docs/tutorials/summarization).\\n\\n- [How to: summarize text in a single LLM call](/docs/how_to/summarize_stuff)\\n- [How to: summarize text through parallelization](/docs/how_to/summarize_map_reduce)\\n- [How to: summarize text through iterative refinement](/docs/how_to/summarize_refine)\\n\\n## LangChain Expression Language (LCEL)\\n\\n:::note Should I use LCEL?\\n\\nLCEL is an orchestration solution. See our\\n[concepts page](/docs/concepts/lcel/#should-i-use-lcel) for recommendations on when to\\nuse LCEL.\\n\\n:::\\n\\n[LangChain Expression Language](/docs/concepts/lcel) is a way to create arbitrary custom chains. It is built on the [Runnable](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html) protocol.\\n\\n[**LCEL cheatsheet**](/docs/how_to/lcel_cheatsheet/): For a quick overview of how to use the main LCEL primitives.\\n\\n[**Migration guide**](/docs/versions/migrating_chains): For migrating legacy chain abstractions to LCEL.\\n\\n- [How to: chain runnables](/docs/how_to/sequence)\\n- [How to: stream runnables](/docs/how_to/streaming)\\n- [How to: invoke runnables in parallel](/docs/how_to/parallel/)\\n- [How to: add default invocation args to runnables](/docs/how_to/binding/)\\n- [How to: turn any function into a runnable](/docs/how_to/functions)\\n- [How to: pass through inputs from one chain step to the next](/docs/how_to/passthrough)\\n- [How to: configure runnable behavior at runtime](/docs/how_to/configure)\\n- [How to: add message history (memory) to a chain](/docs/how_to/message_history)\\n- [How to: route between sub-chains](/docs/how_to/routing)\\n- [How to: create a dynamic (self-constructing) chain](/docs/how_to/dynamic_chain/)\\n- [How to: inspect runnables](/docs/how_to/inspect)\\n- [How to: add fallbacks to a runnable](/docs/how_to/fallbacks)\\n- [How to: pass runtime secrets to a runnable](/docs/how_to/runnable_runtime_secrets)\\n\\n## [LangGraph](https://langchain-ai.github.io/langgraph)\\n\\nLangGraph is an extension of LangChain aimed at\\nbuilding robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.\\n\\nLangGraph documentation is currently hosted on a separate site.\\nYou can peruse [LangGraph how-to guides here](https://langchain-ai.github.io/langgraph/how-tos/).\\n\\n## [LangSmith](https://docs.smith.langchain.com/)\\n\\nLangSmith allows you to closely trace, monitor and evaluate your LLM application.\\nIt seamlessly integrates with LangChain and LangGraph, and you can use it to inspect and debug individual steps of your chains and agents as you build.\\n\\nLangSmith documentation is hosted on a separate site.\\nYou can peruse [LangSmith how-to guides here](https://docs.smith.langchain.com/), but we\\'ll highlight a few sections that are particularly\\nrelevant to LangChain below:\\n\\n### Evaluation\\n<span data-heading-keywords=\"evaluation,evaluate\"></span>\\n\\nEvaluating performance is a vital part of building LLM-powered applications.\\nLangSmith helps with every step of the process from creating a dataset to defining metrics to running evaluators.\\n\\nTo learn more, check out the [LangSmith evaluation how-to guides](https://docs.smith.langchain.com/how_to_guides#evaluation).\\n\\n### Tracing\\n<span data-heading-keywords=\"trace,tracing\"></span>\\n\\nTracing gives you observability inside your chains and agents, and is vital in diagnosing issues.\\n\\n- [How to: trace with LangChain](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain)\\n- [How to: add metadata and tags to traces](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#add-metadata-and-tags-to-traces)\\n\\nYou can see general tracing-related how-tos [in this section of the LangSmith docs](https://docs.smith.langchain.com/how_to_guides/tracing).\\n': 0.031746031746031744}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LangChainは、大規模言語モデル（LLM）を活用したアプリケーションを開発するためのフレームワークです。LangChainの目的は、開発者が推論を行うアプリケーションをできるだけ簡単に構築できるようにすることです。もともとはオープンソースのパッケージとして始まりましたが、現在は企業とエコシステム全体に進化しています。\\n\\nLangChainの主な特徴には以下が含まれます：\\n\\n1. **標準化されたコンポーネントインターフェース**: 様々なAIアプリケーション向けのモデルや関連コンポーネントの多様性に対処するため、LangChainは主要コンポーネントの標準インターフェースを提供し、プロバイダー間の切り替えを容易にします。\\n\\n2. **オーケストレーション**: 複数のコンポーネントやモデルを組み合わせて複雑なアプリケーションを構築するための効率的な接続を可能にします。LangGraphというライブラリを使用して、アプリケーションのフローをノードとエッジのセットとして表現できます。\\n\\n3. **可観測性と評価**: アプリケーションが複雑になるにつれて、内部で何が起こっているのかを理解することが難しくなります。LangSmithというプラットフォームを使用して、アプリケーションの監視や評価を行い、開発者が自信を持って迅速に質問に答えられるようにします。\\n\\nLangChainは、開発、運用、デプロイの各段階を簡素化し、開発者が自分のユースケースに最適なコンポーネントを選択して使用できるように設計されています。'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_fusion_chain = {\n",
    "    \"question\": RunnablePassthrough(),\n",
    "    \"context\": query_generation_chain | retriever.map() | reciprocal_rank_fusion,\n",
    "} | prompt | model | StrOutputParser()\n",
    "\n",
    "rag_fusion_chain.invoke(\"LangChainの概要を教えて\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-5. 複数の Retriever を使う工夫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import TavilySearchAPIRetriever\n",
    "\n",
    "langchain_document_retriever = retriever.with_config(\n",
    "    {\"run_name\": \"langchain_document_retriever\"}\n",
    ")\n",
    "\n",
    "web_retriever = TavilySearchAPIRetriever(k=3).with_config(\n",
    "    {\"run_name\": \"web_retriever\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class Route(str, Enum):\n",
    "    langchain_document = \"langchain_document\"\n",
    "    web = \"web\"\n",
    "\n",
    "\n",
    "class RouteOutput(BaseModel):\n",
    "    route: Route\n",
    "\n",
    "\n",
    "route_prompt = ChatPromptTemplate.from_template(\"\"\"\\\n",
    "質問に回答するために適切なRetrieverを選択してください。\n",
    "\n",
    "質問: {question}\n",
    "\"\"\")\n",
    "\n",
    "route_chain = (\n",
    "    route_prompt\n",
    "    | model.with_structured_output(RouteOutput)\n",
    "    | (lambda x: x.route)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def routed_retriever(inp: dict[str, Any]) -> list[Document]:\n",
    "    question = inp[\"question\"]\n",
    "    route = inp[\"route\"]\n",
    "\n",
    "    if route == Route.langchain_document:\n",
    "        return langchain_document_retriever.invoke(question)\n",
    "    elif route == Route.web:\n",
    "        return web_retriever.invoke(question)\n",
    "\n",
    "    raise ValueError(f\"Unknown route: {route}\")\n",
    "\n",
    "\n",
    "route_rag_chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"route\": route_chain,\n",
    "    }\n",
    "    | RunnablePassthrough.assign(context=routed_retriever)\n",
    "    | prompt | model | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "route_rag_chain.invoke(\"LangChainの概要を教えて\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "route_rag_chain.invoke(\"東京の今日の天気は？\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
