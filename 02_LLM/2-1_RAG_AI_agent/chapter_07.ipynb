{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 環境設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LANGSMITH_TRACING_V2: None\n",
      "LANGSMITH_ENDPOINT: https://api.smith.langchain.com\n",
      "LANGSMITH_PROJECT: agent-book\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# .env ファイルを読み込む\n",
    "path_env=\"C:\\\\Users\\\\Yuichi Katogi\\\\.env\"\n",
    "load_dotenv(path_env)\n",
    "\n",
    "# 環境変数を取得\n",
    "langsmith_tracing_v2 = os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
    "langsmith_endpoint = os.getenv(\"LANGSMITH_ENDPOINT\")\n",
    "langsmith_api_key = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "langsmith_project = os.getenv(\"LANGSMITH_PROJECT\")\n",
    "\n",
    "# Langsmithがただしく読み込めているか確認。なぜかtracingがNoneだができてる\n",
    "print(f\"LANGSMITH_TRACING_V2: {langsmith_tracing_v2}\")\n",
    "print(f\"LANGSMITH_ENDPOINT: {langsmith_endpoint}\")\n",
    "print(f\"LANGSMITH_PROJECT: {langsmith_project}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Ragasによる合成テストデータの生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install langchain-core==0.2.30 langchain-openai==0.1.21 langchain-community==0.2.12 GitPython==3.1.43 langchain-chroma==0.1.2 chromadb==0.5.3 ragas==0.1.14 nest-asyncio==1.6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 検索対象のドキュメントのロード\n",
    "- Langchainの公式ドキュメントを使用する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import GitLoader\n",
    "\n",
    "\n",
    "def file_filter(file_path: str) -> bool:\n",
    "    return file_path.endswith(\".mdx\")\n",
    "\n",
    "\n",
    "loader = GitLoader(\n",
    "    clone_url=\"https://github.com/langchain-ai/langchain\",\n",
    "    repo_path=\"./langchain\",\n",
    "    branch=\"langchain==0.2.13\",\n",
    "    file_filter=file_filter,\n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ragas による合成テストデータ生成の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for document in documents:\n",
    "    document.metadata[\"filename\"] = document.metadata[\"source\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 4/4 [00:12<00:00,  3.18s/it]         \n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm=ChatOpenAI(model=\"gpt-4o-mini\"),\n",
    "    critic_llm=ChatOpenAI(model=\"gpt-4o-mini\"),\n",
    "    embeddings=OpenAIEmbeddings(),\n",
    ")\n",
    "\n",
    "testset = generator.generate_with_langchain_docs(\n",
    "    documents,\n",
    "    test_size=4,  # 生成するテストデータの数\n",
    "    distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25}, # テストデータの生成に使用するテストセットの分布\n",
    ")\n",
    "\n",
    "# テストセットは単純な質問2/4、推論が必要な質問1/4、回答に複数の情報源が必要な質問1/4となるように設定\n",
    "# gpt-4oだと1分あたりのトークン制限に達してしまった。昔と比べてDocument loaderで参照する情報量が増えている。推論力はやや劣るがgpt-4o-miniを使用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>evolution_type</th>\n",
       "      <th>metadata</th>\n",
       "      <th>episode_done</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the purpose of PromptLayer in the cont...</td>\n",
       "      <td>[# PromptLayer\\n\\n&gt;[PromptLayer](https://docs....</td>\n",
       "      <td>PromptLayer is a platform for prompt engineeri...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'docs\\docs\\integrations\\providers\\...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What features does Telegram Messenger provide ...</td>\n",
       "      <td>[# Telegram\\n\\n&gt;[Telegram Messenger](https://w...</td>\n",
       "      <td>Telegram Messenger provides features such as o...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'docs\\docs\\integrations\\providers\\...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How to set up PromptLayer with LangChain, incl...</td>\n",
       "      <td>[# PromptLayer\\n\\n&gt;[PromptLayer](https://docs....</td>\n",
       "      <td>To set up PromptLayer with LangChain, you need...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>[{'source': 'docs\\docs\\integrations\\providers\\...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does LLM observability boost LangChain sec...</td>\n",
       "      <td>[# PromptLayer\\n\\n&gt;[PromptLayer](https://docs....</td>\n",
       "      <td>The context does not provide specific informat...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': 'docs\\docs\\integrations\\providers\\...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What is the purpose of PromptLayer in the cont...   \n",
       "1  What features does Telegram Messenger provide ...   \n",
       "2  How to set up PromptLayer with LangChain, incl...   \n",
       "3  How does LLM observability boost LangChain sec...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [# PromptLayer\\n\\n>[PromptLayer](https://docs....   \n",
       "1  [# Telegram\\n\\n>[Telegram Messenger](https://w...   \n",
       "2  [# PromptLayer\\n\\n>[PromptLayer](https://docs....   \n",
       "3  [# PromptLayer\\n\\n>[PromptLayer](https://docs....   \n",
       "\n",
       "                                        ground_truth evolution_type  \\\n",
       "0  PromptLayer is a platform for prompt engineeri...         simple   \n",
       "1  Telegram Messenger provides features such as o...         simple   \n",
       "2  To set up PromptLayer with LangChain, you need...      reasoning   \n",
       "3  The context does not provide specific informat...  multi_context   \n",
       "\n",
       "                                            metadata  episode_done  \n",
       "0  [{'source': 'docs\\docs\\integrations\\providers\\...          True  \n",
       "1  [{'source': 'docs\\docs\\integrations\\providers\\...          True  \n",
       "2  [{'source': 'docs\\docs\\integrations\\providers\\...          True  \n",
       "3  [{'source': 'docs\\docs\\integrations\\providers\\...          True  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LangSmith の Dataset の作成\n",
    "- LangSmithでデータセットを管理する「Dataset」というオブジェクトを作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "dataset_name = \"agent-book\"\n",
    "\n",
    "client = Client()\n",
    "\n",
    "if client.has_dataset(dataset_name=dataset_name):\n",
    "    client.delete_dataset(dataset_name=dataset_name)\n",
    "\n",
    "dataset = client.create_dataset(dataset_name=dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 合成テストデータの保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "outputs = []\n",
    "metadatas = []\n",
    "\n",
    "for testset_record in testset.test_data:\n",
    "    inputs.append(\n",
    "        {\n",
    "            \"question\": testset_record.question,\n",
    "        }\n",
    "    )\n",
    "    outputs.append(\n",
    "        {\n",
    "            \"contexts\": testset_record.contexts,\n",
    "            \"ground_truth\": testset_record.ground_truth,\n",
    "        }\n",
    "    )\n",
    "    metadatas.append(\n",
    "        {\n",
    "            \"source\": testset_record.metadata[0][\"source\"],\n",
    "            \"evolution_type\": testset_record.evolution_type,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.create_examples(\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    metadata=metadatas,\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5. LangSmith と Ragas を使ったオフライン評価の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### カスタムEvaluatorの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langsmith.schemas import Example, Run\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.metrics.base import Metric, MetricWithEmbeddings, MetricWithLLM\n",
    "\n",
    "\n",
    "class RagasMetricEvaluator:\n",
    "    def __init__(self, metric: Metric, llm: BaseChatModel, embeddings: Embeddings):\n",
    "        self.metric = metric\n",
    "\n",
    "        # LLMとEmbeddingsをMetricに設定\n",
    "        if isinstance(self.metric, MetricWithLLM):\n",
    "            self.metric.llm = LangchainLLMWrapper(llm)\n",
    "        if isinstance(self.metric, MetricWithEmbeddings):\n",
    "            self.metric.embeddings = LangchainEmbeddingsWrapper(embeddings)\n",
    "\n",
    "    def evaluate(self, run: Run, example: Example) -> dict[str, Any]:\n",
    "        context_strs = [doc.page_content for doc in run.outputs[\"contexts\"]]\n",
    "\n",
    "        # Ragasの評価メトリクスのscoreメソッドでスコアを算出\n",
    "        score = self.metric.score(\n",
    "            {\n",
    "                \"question\": example.inputs[\"question\"],\n",
    "                \"answer\": run.outputs[\"answer\"],\n",
    "                \"contexts\": context_strs,\n",
    "                \"ground_truth\": example.outputs[\"ground_truth\"],\n",
    "            },\n",
    "        )\n",
    "        return {\"key\": self.metric.name, \"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from ragas.metrics import answer_relevancy, context_precision\n",
    "\n",
    "metrics = [context_precision, answer_relevancy]\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "evaluators = [\n",
    "    RagasMetricEvaluator(metric, llm, embeddings).evaluate\n",
    "    for metric in metrics\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 推論の関数の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "db = Chroma.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template('''\\\n",
    "以下の文脈だけを踏まえて質問に回答してください。\n",
    "\n",
    "文脈: \"\"\"\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "質問: {question}\n",
    "''')\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "chain = RunnableParallel(\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"context\": retriever,\n",
    "    }\n",
    ").assign(answer=prompt | model | StrOutputParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inputs: dict[str, Any]) -> dict[str, Any]:\n",
    "    question = inputs[\"question\"]\n",
    "    output = chain.invoke(question)\n",
    "    return {\n",
    "        \"contexts\": output[\"context\"],\n",
    "        \"answer\": output[\"answer\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### オフライン評価の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "evaluate(\n",
    "    predict,\n",
    "    data=\"agent-book\",\n",
    "    evaluators=evaluators,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
